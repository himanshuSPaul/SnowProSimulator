Topic Name: Data Protection
Sub Topic Name: Time Travel
Total Question Count: 210

all questions:

  - Question No: "001"
    question: "What is the maximum Time Travel retention period for a table in Snowflake Enterprise Edition?"
    their_options:
      option A: "1 day"
      option B: "7 days"
      option C: "90 days"
      option D: "45 days"
      option E: "30 days"
    correct Answer: "option C: 90 days"
    explanation: >
      CORRECT - option C: In Snowflake Enterprise Edition (and higher), the maximum DATA_RETENTION_TIME_IN_DAYS 
      is 90 days for permanent tables.
      WRONG - option A: 1 day is the default for Standard Edition and also the maximum for Standard Edition, 
      but Enterprise allows up to 90 days.
      WRONG - option B: 7 days is a common misconception but is NOT the maximum — it is simply a frequently 
      used intermediate value.
      WRONG - option D: 45 days is not a documented limit in Snowflake.
      WRONG - option E: 30 days is not the maximum. Enterprise supports up to 90 days.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, Architect

  - Question No: "002"
    question: "What is the maximum Time Travel retention period in Snowflake Standard Edition?"
    their_options:
      option A: "90 days"
      option B: "14 days"
      option C: "1 day"
      option D: "7 days"
    correct Answer: "option C: 1 day"
    explanation: >
      CORRECT - option C: Standard Edition supports a maximum of 1 day (0 or 1) for Time Travel.
      WRONG - option A: 90 days is only available in Enterprise Edition and higher.
      WRONG - option B: 14 days is not a valid maximum for any Snowflake edition.
      WRONG - option D: 7 days is commonly associated with Enterprise Edition retention periods 
      but is NOT the max for Standard Edition.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "003"
    question: "Which of the following SQL extensions can be used to query historical data using Time Travel? (SELECT TWO)"
    their_options:
      option A: "AT"
      option B: "BEFORE"
      option C: "HISTORY"
      option D: "SNAPSHOT"
      option E: "AS OF"
    correct Answer: "option A: AT, option B: BEFORE"
    explanation: >
      CORRECT - option A (AT): The AT clause queries data as it existed at a specific point in time 
      (timestamp, offset, or statement ID).
      CORRECT - option B (BEFORE): The BEFORE clause queries data just before a specific point in time 
      (typically used with a statement/query ID to undo a DML operation).
      WRONG - option C (HISTORY): Not a valid Snowflake Time Travel SQL keyword.
      WRONG - option D (SNAPSHOT): Not a valid SQL clause in Snowflake Time Travel.
      WRONG - option E (AS OF): This syntax is used in other databases (e.g., DB2 temporal tables) 
      but NOT in Snowflake Time Travel.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, DataAnalyst

  - Question No: "004"
    question: "A developer accidentally drops a table named SALES_DATA. Which command is used to restore it?"
    their_options:
      option A: "RESTORE TABLE SALES_DATA;"
      option B: "UNDROP TABLE SALES_DATA;"
      option C: "RECOVER TABLE SALES_DATA;"
      option D: "FLASHBACK TABLE SALES_DATA;"
    correct Answer: "option B: UNDROP TABLE SALES_DATA;"
    explanation: >
      CORRECT - option B: UNDROP TABLE is Snowflake's command to restore a dropped table from 
      Time Travel. It works within the retention period.
      WRONG - option A: RESTORE TABLE is not valid Snowflake DDL syntax.
      WRONG - option C: RECOVER TABLE is not a Snowflake command.
      WRONG - option D: FLASHBACK TABLE is Oracle's equivalent feature; Snowflake uses UNDROP.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, DataAnalyst, Architect

  - Question No: "005"
    question: "Which parameter controls the Time Travel retention period for a Snowflake object?"
    their_options:
      option A: "TIME_TRAVEL_RETENTION_DAYS"
      option B: "DATA_RETENTION_TIME_IN_DAYS"
      option C: "HISTORY_RETENTION_DAYS"
      option D: "RETENTION_PERIOD_DAYS"
    correct Answer: "option B: DATA_RETENTION_TIME_IN_DAYS"
    explanation: >
      CORRECT - option B: DATA_RETENTION_TIME_IN_DAYS is the exact parameter name used in Snowflake 
      to control how long Time Travel data is retained for tables, schemas, and databases.
      WRONG - option A: TIME_TRAVEL_RETENTION_DAYS is not a valid Snowflake parameter name.
      WRONG - option C: HISTORY_RETENTION_DAYS does not exist in Snowflake.
      WRONG - option D: RETENTION_PERIOD_DAYS is not a valid Snowflake parameter.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, Architect

  - Question No: "006"
    question: "What is the default value of DATA_RETENTION_TIME_IN_DAYS for a newly created table in Snowflake?"
    their_options:
      option A: "0"
      option B: "7"
      option C: "1"
      option D: "90"
    correct Answer: "option C: 1"
    explanation: >
      CORRECT - option C: The default DATA_RETENTION_TIME_IN_DAYS is 1 day for tables when not 
      explicitly set, regardless of edition.
      WRONG - option A: 0 disables Time Travel entirely; it is not the default.
      WRONG - option B: 7 days is not the default; it must be explicitly set.
      WRONG - option D: 90 days is the maximum for Enterprise Edition, not the default.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "007"
    question: "How do you query a table named ORDERS as it existed exactly 1 hour ago using an offset?"
    their_options:
      option A: "SELECT * FROM ORDERS AT (OFFSET => -3600);"
      option B: "SELECT * FROM ORDERS BEFORE (OFFSET => -3600);"
      option C: "SELECT * FROM ORDERS AT (TIMESTAMP => DATEADD(hour, -1, CURRENT_TIMESTAMP()));"
      option D: "SELECT * FROM ORDERS HISTORY (OFFSET => -3600);"
    correct Answer: "option A: SELECT * FROM ORDERS AT (OFFSET => -3600);"
    explanation: >
      CORRECT - option A: The AT clause with OFFSET accepts a negative integer representing seconds 
      relative to the current time. -3600 seconds = -1 hour.
      WRONG - option B: BEFORE with an offset queries data just before that point in time (exclusive), 
      which is subtly different from "as it existed at" that point.
      WRONG - option C: While technically valid syntax using TIMESTAMP, the question specifically asks 
      about using an offset — option A is the direct correct answer for offset-based Time Travel.
      WRONG - option D: HISTORY is not a valid Time Travel clause keyword.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "008"
    question: "A DML statement with query_id '018f-abcd-1234' accidentally deleted rows from a table. Which query correctly restores the deleted rows using Time Travel?"
    their_options:
      option A: "INSERT INTO MY_TABLE SELECT * FROM MY_TABLE BEFORE (STATEMENT => '018f-abcd-1234');"
      option B: "INSERT INTO MY_TABLE SELECT * FROM MY_TABLE AT (STATEMENT => '018f-abcd-1234');"
      option C: "UNDROP TABLE MY_TABLE BEFORE (STATEMENT => '018f-abcd-1234');"
      option D: "RESTORE TABLE MY_TABLE AT (STATEMENT => '018f-abcd-1234');"
    correct Answer: "option A: INSERT INTO MY_TABLE SELECT * FROM MY_TABLE BEFORE (STATEMENT => '018f-abcd-1234');"
    explanation: >
      CORRECT - option A: BEFORE (STATEMENT => '<query_id>') queries data as it was just before 
      that specific DML statement ran, effectively seeing the pre-delete state. INSERT INTO restores 
      the deleted rows.
      WRONG - option B: AT (STATEMENT => '<query_id>') queries the data at the END of that statement 
      execution — meaning the delete has already occurred, so rows are still missing.
      WRONG - option C: UNDROP TABLE restores a dropped table, not rows deleted by a DML statement; 
      also UNDROP doesn't accept a BEFORE clause.
      WRONG - option D: RESTORE TABLE is not valid Snowflake syntax.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "009"
    question: "Which Snowflake objects support Time Travel? (SELECT THREE)"
    their_options:
      option A: "Permanent Tables"
      option B: "Temporary Tables"
      option C: "Transient Tables"
      option D: "External Tables"
      option E: "Stages"
    correct Answer: "option A: Permanent Tables, option B: Temporary Tables, option C: Transient Tables"
    explanation: >
      CORRECT - option A (Permanent Tables): Support Time Travel with up to 90 days retention (Enterprise).
      CORRECT - option B (Temporary Tables): Support Time Travel but only for the session duration; 
      max retention is 1 day.
      CORRECT - option C (Transient Tables): Support Time Travel with a maximum of 1 day retention 
      (not 90 days like permanent tables).
      WRONG - option D (External Tables): External tables reference data in external stages and do NOT 
      support Time Travel.
      WRONG - option E (Stages): Stages are storage locations and do not support Time Travel.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, Architect

  - Question No: "010"
    question: "What is the maximum DATA_RETENTION_TIME_IN_DAYS value allowed for a Transient table in Snowflake?"
    their_options:
      option A: "90 days"
      option B: "7 days"
      option C: "1 day"
      option D: "0 days (Time Travel not supported)"
    correct Answer: "option C: 1 day"
    explanation: >
      CORRECT - option C: Transient tables have a maximum retention of 1 day for Time Travel 
      (can be 0 or 1). They do not support Fail-safe or extended Time Travel.
      WRONG - option A: 90 days is only for permanent tables in Enterprise Edition.
      WRONG - option B: 7 days is not a valid maximum for transient tables.
      WRONG - option D: Transient tables DO support Time Travel (0 or 1 day); they simply don't 
      support Fail-safe.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, Architect

  - Question No: "011"
    question: "A company wants to minimize storage costs while still allowing a 24-hour recovery window for accidental deletions. Which table type should they use?"
    their_options:
      option A: "Permanent table with DATA_RETENTION_TIME_IN_DAYS = 1"
      option B: "Transient table with DATA_RETENTION_TIME_IN_DAYS = 1"
      option C: "Temporary table with DATA_RETENTION_TIME_IN_DAYS = 1"
      option D: "External table with DATA_RETENTION_TIME_IN_DAYS = 1"
    correct Answer: "option B: Transient table with DATA_RETENTION_TIME_IN_DAYS = 1"
    explanation: >
      CORRECT - option B: Transient tables have Time Travel (up to 1 day) but NO Fail-safe period. 
      This means 7 fewer days of Fail-safe storage costs compared to permanent tables, while still 
      providing the 24-hour recovery window via Time Travel.
      WRONG - option A: A permanent table with 1-day retention works but incurs the additional 7-day 
      Fail-safe storage cost, making it more expensive.
      WRONG - option C: Temporary tables only exist for the session duration; they would be lost 
      when the session ends regardless of retention setting.
      WRONG - option D: External tables don't support Time Travel and DATA_RETENTION_TIME_IN_DAYS 
      cannot be set on them.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "012"
    question: "What happens when you set DATA_RETENTION_TIME_IN_DAYS = 0 on a table?"
    their_options:
      option A: "Time Travel is disabled for that table"
      option B: "The table is immediately dropped"
      option C: "Time Travel defaults to the account-level setting"
      option D: "An error is thrown because 0 is not a valid value"
    correct Answer: "option A: Time Travel is disabled for that table"
    explanation: >
      CORRECT - option A: Setting DATA_RETENTION_TIME_IN_DAYS = 0 explicitly disables Time Travel 
      for the object. Historical data is not retained and UNDROP/AT/BEFORE queries will not work.
      WRONG - option B: The table is not dropped; it continues to exist and function normally, 
      just without Time Travel capability.
      WRONG - option C: Setting 0 explicitly overrides any account-level default — it does not 
      fall back to the account setting.
      WRONG - option D: 0 is a perfectly valid value in Snowflake for this parameter.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "013"
    question: "Which AT/BEFORE sub-clause accepts a timestamp as its value?"
    their_options:
      option A: "OFFSET"
      option B: "STATEMENT"
      option C: "TIMESTAMP"
      option D: "QUERY_ID"
    correct Answer: "option C: TIMESTAMP"
    explanation: >
      CORRECT - option C: The TIMESTAMP sub-clause accepts a timestamp value (e.g., 
      AT (TIMESTAMP => '2024-01-15 10:00:00'::TIMESTAMP_TZ)) to query data at a specific moment.
      WRONG - option A: OFFSET accepts an integer value in seconds (negative for past).
      WRONG - option B: STATEMENT accepts a query ID string to reference a specific DML operation.
      WRONG - option D: QUERY_ID is not a valid sub-clause name in Snowflake Time Travel syntax; 
      the correct clause is STATEMENT.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, DataAnalyst

  - Question No: "014"
    question: "A schema named ANALYTICS is dropped. The table REPORTS inside it had DATA_RETENTION_TIME_IN_DAYS = 7. How can you restore the table?"
    their_options:
      option A: "UNDROP TABLE ANALYTICS.REPORTS;"
      option B: "UNDROP SCHEMA ANALYTICS; then UNDROP TABLE ANALYTICS.REPORTS; if needed"
      option C: "Time Travel does not apply after a schema drop"
      option D: "SELECT * FROM ANALYTICS.REPORTS AT (OFFSET => -3600);"
    correct Answer: "option B: UNDROP SCHEMA ANALYTICS; then UNDROP TABLE ANALYTICS.REPORTS; if needed"
    explanation: >
      CORRECT - option B: When you drop a schema, all child objects (tables) are also dropped. 
      You must first UNDROP the schema, which restores the schema and its contents. If the table 
      still needs restoration after, you can then UNDROP it specifically. The schema must be 
      restored first since it's the parent container.
      WRONG - option A: You cannot UNDROP a table inside a dropped schema without first restoring 
      the schema itself. The parent container must exist first.
      WRONG - option C: Time Travel DOES apply after a schema drop — that's the whole point of 
      the feature. The retention period is honored.
      WRONG - option D: You cannot query a table in a dropped schema using AT because the schema 
      no longer exists as an accessible namespace.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "015"
    question: "What additional storage cost does Snowflake charge for Time Travel data?"
    their_options:
      option A: "Time Travel storage is free and not billed"
      option B: "Time Travel data is charged at the same rate as regular storage"
      option C: "Time Travel data is charged at 2x the regular storage rate"
      option D: "Time Travel data is charged at a flat monthly fee"
    correct Answer: "option B: Time Travel data is charged at the same rate as regular storage"
    explanation: >
      CORRECT - option B: Time Travel data (historical versions of changed/deleted data) is stored 
      in Snowflake and billed at the same rate as active storage. There is no premium — it's just 
      regular storage fees for the additional data retained.
      WRONG - option A: Time Travel storage is NOT free; it increases your storage consumption 
      and therefore your bill.
      WRONG - option C: There is no 2x multiplier for Time Travel storage costs.
      WRONG - option D: Snowflake does not charge a flat monthly fee for Time Travel; it's purely 
      consumption-based storage billing.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, Architect

  - Question No: "016"
    question: "A table is created with the following DDL: CREATE TRANSIENT TABLE STG_ORDERS (id INT, amount DECIMAL) DATA_RETENTION_TIME_IN_DAYS = 5; What will happen?"
    their_options:
      option A: "Table is created with 5 days of Time Travel"
      option B: "An error is thrown because transient tables only support 0 or 1 day retention"
      option C: "Table is created with 1 day of Time Travel (5 is silently truncated)"
      option D: "Table is created with 5 days of Time Travel but no Fail-safe"
    correct Answer: "option B: An error is thrown because transient tables only support 0 or 1 day retention"
    explanation: >
      CORRECT - option B: Snowflake enforces that transient tables can only have 
      DATA_RETENTION_TIME_IN_DAYS of 0 or 1. Setting it to 5 will result in an error: 
      "Object does not support the specified retention time."
      WRONG - option A: This would fail; Snowflake does not allow retention > 1 for transient tables.
      WRONG - option C: Snowflake does NOT silently truncate the value; it throws an error.
      WRONG - option D: While transient tables don't have Fail-safe, the DDL itself would fail 
      before the table is created.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "017"
    question: "Which command clones a table as it existed at a specific timestamp using Time Travel?"
    their_options:
      option A: "CREATE TABLE ORDERS_BACKUP CLONE ORDERS AT (TIMESTAMP => '2024-06-01 00:00:00'::TIMESTAMP_TZ);"
      option B: "CREATE TABLE ORDERS_BACKUP AS SELECT * FROM ORDERS AT (TIMESTAMP => '2024-06-01 00:00:00'::TIMESTAMP_TZ);"
      option C: "CLONE TABLE ORDERS INTO ORDERS_BACKUP AT (TIMESTAMP => '2024-06-01 00:00:00'::TIMESTAMP_TZ);"
      option D: "CREATE CLONE ORDERS_BACKUP FROM ORDERS AT (TIMESTAMP => '2024-06-01 00:00:00'::TIMESTAMP_TZ);"
    correct Answer: "option A: CREATE TABLE ORDERS_BACKUP CLONE ORDERS AT (TIMESTAMP => '2024-06-01 00:00:00'::TIMESTAMP_TZ);"
    explanation: >
      CORRECT - option A: Snowflake supports zero-copy cloning combined with Time Travel. The syntax 
      is CREATE TABLE <new_name> CLONE <source> AT (TIMESTAMP => ...). This creates a clone of the 
      table as it appeared at that historical moment without duplicating data storage.
      WRONG - option B: While this creates a table with historical data, it does a full CTAS (Create 
      Table As Select) which copies all data — it's NOT a zero-copy clone.
      WRONG - option C: CLONE TABLE is not valid Snowflake DDL syntax.
      WRONG - option D: CREATE CLONE is not valid Snowflake DDL syntax.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "018"
    question: "Time Travel and Fail-safe together provide how many total days of data protection for a permanent table with 14 days Time Travel retention in Enterprise Edition?"
    their_options:
      option A: "14 days"
      option B: "21 days"
      option C: "7 days"
      option D: "90 days"
    correct Answer: "option B: 21 days"
    explanation: >
      CORRECT - option B: Time Travel provides 14 days of self-service recovery, plus Fail-safe 
      provides an additional 7 days of Snowflake-managed recovery (non-self-service). Total = 14 + 7 = 21 days.
      WRONG - option A: 14 days is only the Time Travel window; Fail-safe adds 7 more days.
      WRONG - option C: 7 days is the Fail-safe period alone, not the total.
      WRONG - option D: 90 days is the maximum Time Travel retention, not relevant to this specific scenario.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, Architect

  - Question No: "019"
    question: "What is the key difference between Time Travel and Fail-safe in Snowflake?"
    their_options:
      option A: "Time Travel is for permanent tables only; Fail-safe is for all table types"
      option B: "Time Travel is self-service (user-controlled); Fail-safe requires Snowflake Support intervention"
      option C: "Time Travel costs extra; Fail-safe is free"
      option D: "Time Travel retains data for 7 days; Fail-safe retains data for 90 days"
    correct Answer: "option B: Time Travel is self-service (user-controlled); Fail-safe requires Snowflake Support intervention"
    explanation: >
      CORRECT - option B: Time Travel is directly accessible by users via AT/BEFORE/UNDROP commands. 
      Fail-safe is a disaster recovery feature managed exclusively by Snowflake; users cannot access 
      Fail-safe data directly and must contact Snowflake Support.
      WRONG - option A: Both features are primarily for permanent tables. Transient and temporary 
      tables have Time Travel (limited) but NO Fail-safe.
      WRONG - option C: Both Time Travel and Fail-safe storage are billed at standard storage rates.
      WRONG - option D: These retention numbers are incorrect; Time Travel is up to 90 days 
      (Enterprise) and Fail-safe is always 7 days for permanent tables.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, Architect

  - Question No: "020"
    question: "A company runs this command: ALTER TABLE SALES SET DATA_RETENTION_TIME_IN_DAYS = 0; What is the immediate effect?"
    their_options:
      option A: "All historical Time Travel data for SALES is immediately purged"
      option B: "Time Travel is disabled going forward; existing historical data remains until it expires"
      option C: "The command fails because you cannot reduce retention to 0 after data exists"
      option D: "SALES table is dropped and recreated"
    correct Answer: "option A: All historical Time Travel data for SALES is immediately purged"
    explanation: >
      CORRECT - option A: When you reduce DATA_RETENTION_TIME_IN_DAYS on a table (especially to 0), 
      Snowflake immediately purges the Time Travel data that falls outside the new retention window. 
      Setting it to 0 immediately removes all historical versions.
      WRONG - option B: This is a common misconception. Snowflake does NOT keep historical data 
      after the retention is reduced — it is immediately purged.
      WRONG - option C: There is no restriction on reducing the retention value; the command succeeds.
      WRONG - option D: The table itself is not affected; only the historical Time Travel data is purged.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "021"
    question: "Which UNDROP commands are valid in Snowflake? (SELECT THREE)"
    their_options:
      option A: "UNDROP TABLE my_table;"
      option B: "UNDROP SCHEMA my_schema;"
      option C: "UNDROP DATABASE my_database;"
      option D: "UNDROP VIEW my_view;"
      option E: "UNDROP COLUMN my_column;"
    correct Answer: "option A: UNDROP TABLE, option B: UNDROP SCHEMA, option C: UNDROP DATABASE"
    explanation: >
      CORRECT - option A (UNDROP TABLE): Valid — restores a dropped table within retention period.
      CORRECT - option B (UNDROP SCHEMA): Valid — restores a dropped schema and its child objects.
      CORRECT - option C (UNDROP DATABASE): Valid — restores a dropped database and all its schemas/tables.
      WRONG - option D (UNDROP VIEW): Views are not subject to Time Travel and cannot be UNDROPped. 
      Dropping a view is permanent (views are metadata only; no data is stored).
      WRONG - option E (UNDROP COLUMN): Dropping a column from a table cannot be undone with UNDROP. 
      You would need to use Time Travel to clone the table before the ALTER and manually recover.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, Architect

  - Question No: "022"
    question: "How do you find the query ID of the most recent DML that modified a table, so you can use BEFORE (STATEMENT => ...) to undo it?"
    their_options:
      option A: "Query INFORMATION_SCHEMA.TABLE_HISTORY"
      option B: "Query INFORMATION_SCHEMA.QUERY_HISTORY or use the Query History in Snowsight"
      option C: "Query SNOWFLAKE.ACCOUNT_USAGE.TIME_TRAVEL_HISTORY"
      option D: "Run DESCRIBE TABLE to see the last DML query ID"
    correct Answer: "option B: Query INFORMATION_SCHEMA.QUERY_HISTORY or use the Query History in Snowsight"
    explanation: >
      CORRECT - option B: INFORMATION_SCHEMA.QUERY_HISTORY (or the equivalent in ACCOUNT_USAGE) 
      stores the query IDs of all executed statements. You can filter by table name and DML 
      statement type to find the relevant query ID. Snowsight UI also shows this in the Query History tab.
      WRONG - option A: INFORMATION_SCHEMA.TABLE_HISTORY does not exist as a standard view; 
      table change tracking uses ACCOUNT_USAGE.TABLE_STORAGE_METRICS or query history.
      WRONG - option C: SNOWFLAKE.ACCOUNT_USAGE.TIME_TRAVEL_HISTORY is not a real view name. 
      The relevant views are QUERY_HISTORY and TABLE_STORAGE_METRICS.
      WRONG - option D: DESCRIBE TABLE shows column definitions and table properties, not DML history.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "023"
    question: "A table has DATA_RETENTION_TIME_IN_DAYS = 30 at the table level, but the schema it belongs to has DATA_RETENTION_TIME_IN_DAYS = 7. What retention period applies to the table?"
    their_options:
      option A: "7 days (schema-level setting takes precedence)"
      option B: "30 days (table-level setting takes precedence)"
      option C: "The average: 18 days"
      option D: "The account-level setting overrides both"
    correct Answer: "option B: 30 days (table-level setting takes precedence)"
    explanation: >
      CORRECT - option B: In Snowflake, the most specific (lowest) object level setting takes 
      precedence. A table-level DATA_RETENTION_TIME_IN_DAYS explicitly set overrides the schema 
      and database level settings.
      WRONG - option A: Schema-level settings apply only when the table does not have its own 
      explicit retention setting.
      WRONG - option C: Snowflake does not average retention settings across levels.
      WRONG - option D: Account-level settings are the lowest priority in the inheritance hierarchy 
      and are overridden by database, schema, and table level settings.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "024"
    question: "Which view can be used to monitor Time Travel storage consumption at the account level?"
    their_options:
      option A: "INFORMATION_SCHEMA.TABLE_STORAGE_METRICS"
      option B: "SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS"
      option C: "INFORMATION_SCHEMA.TIME_TRAVEL_USAGE"
      option D: "SNOWFLAKE.ACCOUNT_USAGE.STORAGE_USAGE"
    correct Answer: "option B: SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS"
    explanation: >
      CORRECT - option B: SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS provides details including 
      TIME_TRAVEL_BYTES column which shows how much storage is used by Time Travel data across all 
      tables in the account. This is the correct view for monitoring Time Travel storage consumption.
      WRONG - option A: INFORMATION_SCHEMA.TABLE_STORAGE_METRICS also exists but is scoped to a 
      single database and may have latency differences. For account-level monitoring, ACCOUNT_USAGE 
      is more appropriate.
      WRONG - option C: INFORMATION_SCHEMA.TIME_TRAVEL_USAGE is not a valid Snowflake view.
      WRONG - option D: SNOWFLAKE.ACCOUNT_USAGE.STORAGE_USAGE shows total storage metrics 
      (DATABASE, STAGE, FAILSAFE bytes) but does not break down Time Travel specifically.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "025"
    question: "What happens to the Time Travel data of child tables when a database is dropped with UNDROP capability?"
    their_options:
      option A: "Child table Time Travel data is immediately deleted"
      option B: "The Time Travel data of child tables is preserved and accessible after UNDROP DATABASE"
      option C: "Child tables must be individually UNDROPped; the database UNDROP only restores the container"
      option D: "Time Travel data expires based on each table's individual retention setting after the database drop"
    correct Answer: "option B: The Time Travel data of child tables is preserved and accessible after UNDROP DATABASE"
    explanation: >
      CORRECT - option B: When you UNDROP DATABASE, it restores the database along with all its 
      schemas and tables to their state at the time of the drop. The Time Travel history of child 
      objects is preserved during the retention period.
      WRONG - option A: Time Travel data is not immediately deleted when the database is dropped — 
      that's the purpose of UNDROP functionality.
      WRONG - option C: UNDROP DATABASE restores the entire hierarchy; you do not need to 
      individually UNDROP each table within the dropped database.
      WRONG - option D: While retention periods still apply after a drop, within those periods 
      UNDROP DATABASE restores all child objects collectively.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect, DataEngineer

  - Question No: "026"
    question: "A business analyst runs: SELECT * FROM CUSTOMERS AT (TIMESTAMP => '2024-03-01 09:00:00'::TIMESTAMP_TZ); but gets an error saying the specified time is too old. What is the most likely cause?"
    their_options:
      option A: "TIMESTAMP_TZ is not a valid type for the AT clause"
      option B: "The requested timestamp is beyond the table's DATA_RETENTION_TIME_IN_DAYS window"
      option C: "The analyst lacks SYSADMIN privileges to run Time Travel queries"
      option D: "Time Travel queries require a running warehouse"
    correct Answer: "option B: The requested timestamp is beyond the table's DATA_RETENTION_TIME_IN_DAYS window"
    explanation: >
      CORRECT - option B: When you request Time Travel data beyond the retention window, Snowflake 
      returns an error indicating the data is no longer available. The historical data has been 
      purged after the retention period expired.
      WRONG - option A: TIMESTAMP_TZ is a valid type for the AT TIMESTAMP clause. Snowflake 
      also accepts TIMESTAMP_NTZ and other timestamp variants.
      WRONG - option C: Time Travel access is controlled by standard object privileges (SELECT on 
      the table), not by SYSADMIN role. Any user with SELECT can use AT/BEFORE.
      WRONG - option D: While DML requires a warehouse, Time Travel SELECT queries do require 
      a running warehouse — but the error described is specifically about data availability, 
      not compute resources.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataAnalyst, DataEngineer

  - Question No: "027"
    question: "Which of the following correctly describes the BEFORE (STATEMENT => ...) clause behavior?"
    their_options:
      option A: "Returns data as it appeared at the END of the specified statement's execution"
      option B: "Returns data as it appeared just BEFORE the specified statement began execution"
      option C: "Returns data from the day before the statement was executed"
      option D: "Undoes the specified statement automatically"
    correct Answer: "option B: Returns data as it appeared just BEFORE the specified statement began execution"
    explanation: >
      CORRECT - option B: BEFORE (STATEMENT => '<query_id>') returns the table data as it was 
      immediately before the specified statement executed. This is the pre-DML state, allowing 
      you to see and recover data that was changed or deleted by that statement.
      WRONG - option A: That describes AT (STATEMENT => ...) which shows data at the END/result 
      of the statement.
      WRONG - option C: "Day before" is incorrect — BEFORE refers to the microsecond before the 
      statement executed, not a calendar day earlier.
      WRONG - option D: BEFORE is a read-only time travel query; it does not automatically undo 
      anything. You must explicitly INSERT/MERGE the historical data to restore it.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "028"
    question: "Can Time Travel be used to query historical data from a CLONE of a table?"
    their_options:
      option A: "Yes, a clone inherits the full historical data of the source table"
      option B: "Yes, but only the history created AFTER the clone was created"
      option C: "No, clones cannot use Time Travel"
      option D: "Only if the clone was created with the TIME_TRAVEL = TRUE option"
    correct Answer: "option B: Yes, but only the history created AFTER the clone was created"
    explanation: >
      CORRECT - option B: A zero-copy clone shares the underlying data with the source at the 
      point of cloning, but the clone's Time Travel history only extends back to when the clone 
      was created. You cannot travel back to before the clone's creation using the clone's Time 
      Travel. To access pre-clone history, you must use the original source table.
      WRONG - option A: A clone does NOT inherit the full historical data of the source for Time 
      Travel purposes. The Time Travel window starts at clone creation.
      WRONG - option C: Clones do support Time Travel — just from their creation point forward.
      WRONG - option D: There is no TIME_TRAVEL = TRUE option in Snowflake's CLONE syntax.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "029"
    question: "A table named INVENTORY is dropped. Another table with the same name INVENTORY is then created. Can you UNDROP the original INVENTORY table?"
    their_options:
      option A: "Yes, UNDROP TABLE INVENTORY will always restore the most recently dropped version"
      option B: "No, once a new table with the same name is created, you cannot UNDROP the old one"
      option C: "Yes, but you must rename the new table first, then run UNDROP TABLE INVENTORY"
      option D: "Yes, Snowflake stores multiple dropped versions and lets you choose which to restore"
    correct Answer: "option C: Yes, but you must rename the new table first, then run UNDROP TABLE INVENTORY"
    explanation: >
      CORRECT - option C: UNDROP TABLE restores the most recently dropped table with that name. 
      However, if a table with the same name already exists, the UNDROP will fail due to a naming 
      conflict. You must first rename or drop the new table, then UNDROP the original.
      WRONG - option A: While UNDROP does restore the most recently dropped version, if a table 
      with the same name already exists, it will fail — it doesn't automatically overwrite.
      WRONG - option B: You CAN UNDROP the old one, you just need to resolve the naming conflict 
      first by renaming/removing the new table.
      WRONG - option D: Snowflake does UNDROP the most recently dropped table, but the issue is 
      the naming conflict, not a version-selection feature.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "030"
    question: "What privilege is required for a user to perform UNDROP TABLE on a table in Snowflake?"
    their_options:
      option A: "OWNERSHIP on the table"
      option B: "CREATE TABLE privilege on the schema containing the dropped table"
      option C: "SYSADMIN role"
      option D: "ACCOUNTADMIN role"
    correct Answer: "option B: CREATE TABLE privilege on the schema containing the dropped table"
    explanation: >
      CORRECT - option B: To UNDROP TABLE, the user needs CREATE TABLE privilege on the schema 
      where the table existed. This makes sense because UNDROP TABLE effectively creates a new 
      object (restores it) in the schema.
      WRONG - option A: OWNERSHIP on a dropped object is not meaningful since the table no longer 
      exists in the active state. The schema-level CREATE privilege is what's needed.
      WRONG - option C: SYSADMIN is not required. Any role with CREATE TABLE on the schema can 
      perform UNDROP.
      WRONG - option D: ACCOUNTADMIN is not required. This would be overly restrictive for a 
      common recovery operation.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "031"
    question: "Which SQL correctly creates a table with 30 days of Time Travel and is only available in Enterprise Edition and higher?"
    their_options:
      option A: "CREATE TABLE ORDERS (id INT) DATA_RETENTION_TIME_IN_DAYS = 30;"
      option B: "CREATE TRANSIENT TABLE ORDERS (id INT) DATA_RETENTION_TIME_IN_DAYS = 30;"
      option C: "CREATE TEMPORARY TABLE ORDERS (id INT) DATA_RETENTION_TIME_IN_DAYS = 30;"
      option D: "CREATE TABLE ORDERS (id INT) TIME_TRAVEL_DAYS = 30;"
    correct Answer: "option A: CREATE TABLE ORDERS (id INT) DATA_RETENTION_TIME_IN_DAYS = 30;"
    explanation: >
      CORRECT - option A: Only permanent tables support DATA_RETENTION_TIME_IN_DAYS greater than 1. 
      This syntax is correct and requires Enterprise Edition or higher for retention > 1 day.
      WRONG - option B: Transient tables only support 0 or 1 day retention. 
      DATA_RETENTION_TIME_IN_DAYS = 30 on a TRANSIENT table will throw an error.
      WRONG - option C: Temporary tables also only support 0 or 1 day retention. Same error as option B.
      WRONG - option D: TIME_TRAVEL_DAYS is not a valid Snowflake parameter name. The correct 
      parameter is DATA_RETENTION_TIME_IN_DAYS.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "032"
    question: "A data engineer wants to track changes to a specific table for CDC (Change Data Capture) purposes using Time Travel. They write: SELECT * FROM ORDERS CHANGES(INFORMATION => DEFAULT) AT (OFFSET => -3600); What does this query return?"
    their_options:
      option A: "All rows in ORDERS table 1 hour ago"
      option B: "Only rows that were inserted in the last hour"
      option C: "The net changes (inserts, updates, deletes) to the ORDERS table in the last hour"
      option D: "CHANGES is not valid Snowflake syntax"
    correct Answer: "option C: The net changes (inserts, updates, deletes) to the ORDERS table in the last hour"
    explanation: >
      CORRECT - option C: The CHANGES clause combined with INFORMATION => DEFAULT returns a changelog 
      showing inserts, updated rows (before/after), and deleted rows that occurred within the specified 
      time range. This is Snowflake's built-in CDC capability using Time Travel.
      WRONG - option A: That would be a plain SELECT * FROM ORDERS AT (OFFSET => -3600) without 
      the CHANGES clause.
      WRONG - option B: CHANGES with DEFAULT captures all DML change types, not just inserts. 
      For inserts only, you would filter by METADATA$ACTION = 'INSERT'.
      WRONG - option D: CHANGES is valid Snowflake syntax for change tracking queries 
      (also related to the CHANGE_TRACKING table property for streams).
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "033"
    question: "How does setting DATA_RETENTION_TIME_IN_DAYS at the database level affect tables within it?"
    their_options:
      option A: "It only affects newly created tables; existing tables are not changed"
      option B: "It applies to all objects in the database that don't have their own explicit setting"
      option C: "It overrides table-level settings for all tables in the database"
      option D: "Database-level retention has no effect on table-level retention"
    correct Answer: "option B: It applies to all objects in the database that don't have their own explicit setting"
    explanation: >
      CORRECT - option B: Snowflake uses parameter inheritance. Database-level DATA_RETENTION_TIME_IN_DAYS 
      applies as a default to schemas and tables within it that don't have their own explicit setting. 
      More specific settings (schema > database > account) override less specific ones.
      WRONG - option A: The database-level setting also applies to existing tables that don't have 
      explicit table-level settings — it's not limited to newly created tables.
      WRONG - option C: Table-level settings always take precedence over database-level settings, 
      not the other way around.
      WRONG - option D: Database-level settings DO affect tables via the inheritance model.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "034"
    question: "What is the OFFSET value in seconds to query data from exactly 12 hours ago using AT?"
    their_options:
      option A: "AT (OFFSET => 43200)"
      option B: "AT (OFFSET => -720)"
      option C: "AT (OFFSET => -43200)"
      option D: "AT (OFFSET => -12)"
    correct Answer: "option C: AT (OFFSET => -43200)"
    explanation: >
      CORRECT - option C: Offsets are in seconds and must be negative to reference past time. 
      12 hours × 60 minutes × 60 seconds = 43,200 seconds. So -43200 is correct.
      WRONG - option A: Positive 43200 would be 12 hours in the FUTURE, which is not valid for 
      Time Travel (you cannot query future data).
      WRONG - option B: -720 seconds = -12 minutes, not 12 hours.
      WRONG - option D: -12 seconds, not 12 hours.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "035"
    question: "Scenario: A production table TRANSACTIONS has 90 days Time Travel enabled. The DBA wants to reduce storage costs. Which approach reduces Time Travel costs WITHOUT disabling Time Travel entirely?"
    their_options:
      option A: "Convert the table to TRANSIENT type"
      option B: "Reduce DATA_RETENTION_TIME_IN_DAYS to a lower value like 7"
      option C: "Move the table to an external stage"
      option D: "Disable the Snowflake warehouse to stop accumulating history"
    correct Answer: "option B: Reduce DATA_RETENTION_TIME_IN_DAYS to a lower value like 7"
    explanation: >
      CORRECT - option B: Reducing the retention period means Snowflake retains fewer historical 
      data versions, directly reducing Time Travel storage consumption. This maintains Time Travel 
      capability while reducing cost.
      WRONG - option A: Converting to TRANSIENT would limit Time Travel to 1 day and eliminate 
      Fail-safe, which is a drastic change that may not be acceptable for a production table. 
      Also, you cannot directly ALTER a permanent table to transient; you'd need to recreate it.
      WRONG - option C: Moving data to an external stage removes it from Snowflake's managed storage 
      entirely but also removes Time Travel capability.
      WRONG - option D: Time Travel storage accumulates based on DML operations, not warehouse 
      activity. Suspending a warehouse does not affect Time Travel data retention.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "036"
    question: "A developer issues: SELECT * FROM MY_TABLE AT (TIMESTAMP => CURRENT_TIMESTAMP()); What does this query return?"
    their_options:
      option A: "An error because AT cannot use CURRENT_TIMESTAMP"
      option B: "The current state of MY_TABLE (same as a regular SELECT)"
      option C: "An empty result set because no historical data exists at the current moment"
      option D: "The table data from 1 second ago"
    correct Answer: "option B: The current state of MY_TABLE (same as a regular SELECT)"
    explanation: >
      CORRECT - option B: Querying AT the current timestamp returns the current data, which is 
      the same as a regular SELECT query. Time Travel at the current moment simply shows the 
      present state of the table.
      WRONG - option A: CURRENT_TIMESTAMP() is a valid expression for the TIMESTAMP sub-clause. 
      No error is thrown.
      WRONG - option C: The query returns current data, not an empty result.
      WRONG - option D: AT (TIMESTAMP => CURRENT_TIMESTAMP()) is exactly the current time, 
      not 1 second ago.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataAnalyst, DataEngineer

  - Question No: "037"
    question: "Which feature of Snowflake allows you to query Time Travel data across a JOIN without creating a separate historical copy?"
    their_options:
      option A: "Zero-copy clone with Time Travel"
      option B: "Inline AT/BEFORE clause on individual tables in the FROM clause"
      option C: "Snowflake Streams with Time Travel"
      option D: "Dynamic Tables with historical references"
    correct Answer: "option B: Inline AT/BEFORE clause on individual tables in the FROM clause"
    explanation: >
      CORRECT - option B: You can apply AT or BEFORE directly to individual tables in a query, 
      including in JOINs. For example:
      SELECT a.*, b.* FROM TABLE_A AT (OFFSET => -3600) a 
      JOIN TABLE_B AT (OFFSET => -3600) b ON a.id = b.id;
      This queries historical versions of both tables in a single query without creating copies.
      WRONG - option A: Zero-copy clones create separate objects; they don't eliminate the need 
      for an additional object.
      WRONG - option C: Streams capture ongoing changes but are forward-looking, not historical.
      WRONG - option D: Dynamic Tables are materialized query results that refresh automatically; 
      they don't support backward historical joins.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "038"
    question: "What happens to Time Travel data when a table is truncated using TRUNCATE TABLE?"
    their_options:
      option A: "Time Travel is disabled and all historical data is purged"
      option B: "The pre-truncation data is preserved in Time Travel for the retention period"
      option C: "TRUNCATE TABLE bypasses Time Travel and cannot be undone"
      option D: "Time Travel only preserves the last 100 rows before truncation"
    correct Answer: "option B: The pre-truncation data is preserved in Time Travel for the retention period"
    explanation: >
      CORRECT - option B: TRUNCATE TABLE is a DML operation in Snowflake (not a DDL that bypasses 
      logging like in some other databases). The data deleted by TRUNCATE is preserved in Time Travel 
      and can be recovered by querying the table BEFORE the truncation statement.
      WRONG - option A: Time Travel is not disabled by TRUNCATE; it continues functioning normally.
      WRONG - option C: This is a common misconception from other databases. In Snowflake, TRUNCATE 
      is recoverable via Time Travel.
      WRONG - option D: There is no 100-row limit; all data is preserved in Time Travel.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "039"
    question: "Which command would you use to see when a table was last modified and its current retention setting?"
    their_options:
      option A: "DESCRIBE TABLE my_table;"
      option B: "SHOW TABLES LIKE 'my_table';"
      option C: "SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'MY_TABLE';"
      option D: "Both B and C provide this information"
    correct Answer: "option D: Both B and C provide this information"
    explanation: >
      CORRECT - option D: SHOW TABLES LIKE 'my_table' displays metadata including the retention 
      time (retention_time column) and timestamps. INFORMATION_SCHEMA.TABLES also contains 
      retention_time and last_altered columns. Both provide the needed information.
      WRONG - option A: DESCRIBE TABLE (DESC TABLE) shows column definitions, data types, and 
      constraints — it does NOT show retention settings or last modification timestamps.
      WRONG - option B: While correct on its own, D is more complete as C is also valid.
      WRONG - option C: While correct on its own, D is more complete as B is also valid.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "040"
    question: "A company wants to perform nightly comparisons between today's data and yesterday's data. Which Time Travel approach is most efficient?"
    their_options:
      option A: "Create a daily snapshot clone of the table each night"
      option B: "Use SELECT * FROM TABLE AT (OFFSET => -86400) in the comparison query"
      option C: "Export yesterday's data to an external stage each night"
      option D: "Use a Snowflake Stream to capture yesterday's changes"
    correct Answer: "option B: Use SELECT * FROM TABLE AT (OFFSET => -86400) in the comparison query"
    explanation: >
      CORRECT - option B: -86400 seconds = -24 hours = yesterday. Using AT with OFFSET in the 
      query is the most efficient approach because it's a zero-copy Time Travel read — no 
      additional storage, no cloning, no export required.
      WRONG - option A: Daily snapshot clones work but incur additional storage overhead as the 
      clones diverge from the source over time.
      WRONG - option C: Exporting to external stages creates additional storage and pipeline 
      complexity without the benefit of Snowflake's native Time Travel.
      WRONG - option D: Streams capture ongoing changes (CDC) rather than enabling point-in-time 
      comparisons. They're complementary tools, but AT (OFFSET) is simpler for this use case.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst, Architect

  - Question No: "041"
    question: "What is the correct syntax for querying a table using a specific query ID to see data BEFORE a DELETE statement ran?"
    their_options:
      option A: "SELECT * FROM ORDERS AT (STATEMENT => '018abc...');"
      option B: "SELECT * FROM ORDERS BEFORE (STATEMENT => '018abc...');"
      option C: "SELECT * FROM ORDERS AT (QUERY_ID => '018abc...');"
      option D: "SELECT * FROM ORDERS BEFORE (QUERY_ID => '018abc...');"
    correct Answer: "option B: SELECT * FROM ORDERS BEFORE (STATEMENT => '018abc...');"
    explanation: >
      CORRECT - option B: BEFORE (STATEMENT => '<query_id>') is the correct syntax. It queries 
      the data as it was just before the specified statement executed, giving you the pre-DELETE state.
      WRONG - option A: AT (STATEMENT => ...) shows data at the END of the statement execution 
      (post-DELETE state), which is not what you want when recovering from a DELETE.
      WRONG - option C: QUERY_ID is not a valid sub-clause name in Snowflake Time Travel syntax. 
      The correct keyword is STATEMENT.
      WRONG - option D: Same issue — QUERY_ID is not valid syntax. Must use STATEMENT.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "042"
    question: "Scenario: A schema-level DATA_RETENTION_TIME_IN_DAYS is set to 14. A new table is created inside with no explicit retention setting. Later, the schema's retention is changed to 3 days. What is the table's retention?"
    their_options:
      option A: "14 days (inherits the setting at table creation time)"
      option B: "3 days (inherits the current schema setting)"
      option C: "1 day (default table-level setting)"
      option D: "The account-level default"
    correct Answer: "option B: 3 days (inherits the current schema setting)"
    explanation: >
      CORRECT - option B: When a table has no explicit DATA_RETENTION_TIME_IN_DAYS set, it 
      dynamically inherits from the parent schema. If the schema changes to 3 days, the table 
      inherits 3 days going forward. This is dynamic inheritance, not snapshot at creation.
      WRONG - option A: Snowflake does NOT lock in the schema retention value at table creation 
      time. The inheritance is dynamic.
      WRONG - option C: 1 day is the Snowflake platform default when no other setting exists, 
      but since the schema explicitly has a retention value, that takes precedence.
      WRONG - option D: Schema-level setting takes precedence over account-level default.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect, DataEngineer

  - Question No: "043"
    question: "Which of the following actions will consume Time Travel storage? (SELECT TWO)"
    their_options:
      option A: "Executing a SELECT query on a table"
      option B: "Executing an UPDATE statement that modifies 1 million rows"
      option C: "Executing a TRUNCATE TABLE statement"
      option D: "Running a COPY INTO command to load data into an empty table"
      option E: "Executing SHOW TABLES"
    correct Answer: "option B: UPDATE statement modifying rows, option C: TRUNCATE TABLE statement"
    explanation: >
      CORRECT - option B (UPDATE): When rows are updated, the previous versions of those rows are 
      retained in Time Travel storage. 1 million updated rows = significant Time Travel storage 
      consumption.
      CORRECT - option C (TRUNCATE TABLE): TRUNCATE removes all rows, and the deleted data is 
      preserved in Time Travel. This can cause a large spike in Time Travel storage (effectively 
      the entire table size is stored in Time Travel).
      WRONG - option A (SELECT): Read-only queries do not modify data and generate zero Time Travel storage.
      WRONG - option D (COPY INTO empty table): Loading into an empty table means there are no 
      previous versions of rows to retain. New inserts into an empty table do not generate 
      significant Time Travel overhead (only the inserted rows would be tracked going forward 
      if later modified).
      WRONG - option E (SHOW TABLES): Metadata command with no data changes; no Time Travel storage.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "044"
    question: "An organization is on Snowflake Business Critical Edition. What is the maximum Time Travel retention period available?"
    their_options:
      option A: "7 days"
      option B: "30 days"
      option C: "90 days"
      option D: "365 days"
    correct Answer: "option C: 90 days"
    explanation: >
      CORRECT - option C: Business Critical Edition, like Enterprise Edition, supports a maximum 
      of 90 days Time Travel retention. The 90-day maximum applies to Enterprise, Business Critical, 
      and Virtual Private Snowflake (VPS) editions.
      WRONG - option A: 7 days is not the maximum for Business Critical; higher editions support 90 days.
      WRONG - option B: 30 days is not the maximum; 90 days is available.
      WRONG - option D: 365 days is not a supported Time Travel retention period in any Snowflake edition.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, Architect

  - Question No: "045"
    question: "A developer writes: CREATE OR REPLACE TABLE ORDERS AS SELECT * FROM ORDERS AT (OFFSET => -86400); What does this accomplish?"
    their_options:
      option A: "Creates a new table with data from 24 hours ago, replacing the current ORDERS table"
      option B: "Creates an error because you cannot SELECT from a table while replacing it"
      option C: "Creates a backup of ORDERS without affecting the original"
      option D: "This syntax is invalid; CREATE OR REPLACE cannot be used with AT"
    correct Answer: "option A: Creates a new table with data from 24 hours ago, replacing the current ORDERS table"
    explanation: >
      CORRECT - option A: CREATE OR REPLACE TABLE with CTAS (Create Table As Select) using 
      AT (OFFSET => -86400) is valid. It replaces the current ORDERS table with a new table 
      containing data as it was 24 hours ago. This effectively "rolls back" the table to 
      yesterday's state (though it's a full data copy, not zero-copy).
      WRONG - option B: Snowflake handles the temporal resolution — it reads the historical 
      version before replacing. No error is thrown in this case.
      WRONG - option C: CREATE OR REPLACE replaces the EXISTING table with the same name; it 
      does not create a separate backup.
      WRONG - option D: CREATE OR REPLACE CTAS works with AT/BEFORE clauses in Snowflake.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "046"
    question: "In terms of Time Travel, what makes Snowflake Temporary tables different from Transient tables?"
    their_options:
      option A: "Temporary tables support 7 days Time Travel; Transient tables support 1 day"
      option B: "Temporary tables only persist for the session duration; Transient tables persist across sessions but both support max 1 day Time Travel"
      option C: "Temporary tables have Fail-safe; Transient tables do not"
      option D: "There is no difference in Time Travel behavior between temporary and transient tables"
    correct Answer: "option B: Temporary tables only persist for the session duration; Transient tables persist across sessions but both support max 1 day Time Travel"
    explanation: >
      CORRECT - option B: Both temporary and transient tables support 0 or 1 day Time Travel 
      maximum, and neither supports Fail-safe. The key difference is lifetime: temporary tables 
      are session-scoped (dropped when session ends), while transient tables persist until 
      explicitly dropped.
      WRONG - option A: Both types are limited to 0 or 1 day Time Travel; there is no 7-day 
      option for temporary tables.
      WRONG - option C: Neither temporary nor transient tables support Fail-safe. This is one 
      of their defining characteristics.
      WRONG - option D: While their Time Travel limits are the same, the session-scoping of 
      temporary tables is a significant difference.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "047"
    question: "Which statement about querying multiple tables with Time Travel in a single query is TRUE?"
    their_options:
      option A: "All tables in a query must use the same AT/BEFORE timestamp"
      option B: "Each table in a query can have its own independent AT/BEFORE clause"
      option C: "Time Travel can only be applied to one table per query"
      option D: "AT/BEFORE clauses are not supported inside subqueries or CTEs"
    correct Answer: "option B: Each table in a query can have its own independent AT/BEFORE clause"
    explanation: >
      CORRECT - option B: Snowflake allows each table reference in a query to have its own 
      independent AT or BEFORE clause. You can join TABLE_A AT one timestamp with TABLE_B AT 
      a different timestamp in the same query.
      WRONG - option A: There is no requirement for a unified timestamp across all tables in a query.
      WRONG - option C: Multiple tables in the same query can each use Time Travel independently.
      WRONG - option D: AT/BEFORE clauses ARE supported in subqueries and CTEs, providing 
      flexible historical analysis.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "048"
    question: "A table is cloned from a source table at a specific historical timestamp: CREATE TABLE ORDERS_CLONE CLONE ORDERS AT (OFFSET => -7200); What is the DATA_RETENTION_TIME_IN_DAYS of ORDERS_CLONE if ORDERS has 14 days retention?"
    their_options:
      option A: "0 days (clones don't inherit retention)"
      option B: "14 days (inherits source table retention)"
      option C: "1 day (default for new tables)"
      option D: "7 days (half of source retention)"
    correct Answer: "option B: 14 days (inherits source table retention)"
    explanation: >
      CORRECT - option B: When a clone is created, it inherits the DATA_RETENTION_TIME_IN_DAYS 
      of the source object unless explicitly overridden in the CREATE CLONE statement. The clone 
      inherits the source's retention setting.
      WRONG - option A: Clones DO inherit the source's retention setting.
      WRONG - option C: 1 day is the Snowflake platform default, but clones inherit from their 
      source, overriding the platform default.
      WRONG - option D: There is no halving logic; clones inherit the exact retention value.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "049"
    question: "What is the purpose of the CHANGES clause in a Time Travel query (e.g., SELECT ... FROM T CHANGES(INFORMATION => DEFAULT) AT (OFFSET => -3600))?"
    their_options:
      option A: "Returns the current schema changes made to the table"
      option B: "Returns a delta of DML changes (inserts, updates, deletes) between two points in time"
      option C: "Returns the Time Travel configuration changes for the table"
      option D: "It is equivalent to using a Snowflake Stream"
    correct Answer: "option B: Returns a delta of DML changes (inserts, updates, deletes) between two points in time"
    explanation: >
      CORRECT - option B: The CHANGES clause queries the change data (CDC) within the Time Travel 
      window. INFORMATION => DEFAULT returns all change types including inserts, deletes, and 
      updates (with before/after images). INFORMATION => APPEND_ONLY returns only inserts.
      WRONG - option A: CHANGES does not return schema/DDL changes; it returns DML data changes.
      WRONG - option C: Time Travel configuration changes are not captured by the CHANGES clause.
      WRONG - option D: While CHANGES provides similar CDC data to Streams, it operates differently. 
      Streams have offsets that advance as data is consumed; CHANGES queries are ad-hoc and the 
      window is specified each time. They are complementary but not equivalent.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "050"
    question: "A table CUSTOMERS has been accidentally updated with wrong values. The DBA finds the query ID of the bad UPDATE as '01abc-def-12345'. Which sequence of SQL statements correctly restores the original data?"
    their_options:
      option A: >
        "TRUNCATE TABLE CUSTOMERS;
        INSERT INTO CUSTOMERS SELECT * FROM CUSTOMERS BEFORE (STATEMENT => '01abc-def-12345');"
      option B: >
        "CREATE OR REPLACE TABLE CUSTOMERS AS
        SELECT * FROM CUSTOMERS BEFORE (STATEMENT => '01abc-def-12345');"
      option C: >
        "UPDATE CUSTOMERS SET col = original_val WHERE...;"
      option D: "Both A and B are valid approaches"
    correct Answer: "option D: Both A and B are valid approaches"
    explanation: >
      CORRECT - option D: Both approaches effectively restore the table to its pre-UPDATE state.
      Option A: TRUNCATE removes current (wrong) data, then INSERT restores from Time Travel — 
      valid but temporarily leaves the table empty between operations.
      Option B: CREATE OR REPLACE with CTAS using BEFORE (STATEMENT) replaces the table atomically 
      with historical data — preferred as it's atomic and cleaner.
      WRONG - option A alone: Valid but not the only correct answer.
      WRONG - option B alone: Valid but not the only correct answer.
      WRONG - option C: A manual UPDATE with specific values would only work if you know exactly 
      which values to restore, which is impractical for a large table — Time Travel is the proper approach.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "051"
    question: "What does the METADATA$ACTION column return when using the CHANGES clause with Time Travel?"
    their_options:
      option A: "The SQL DML command type: SELECT, INSERT, UPDATE, DELETE"
      option B: "INSERT or DELETE (updates appear as a DELETE of old + INSERT of new row)"
      option C: "The username who performed the action"
      option D: "The timestamp when the action occurred"
    correct Answer: "option B: INSERT or DELETE (updates appear as a DELETE of old + INSERT of new row)"
    explanation: >
      CORRECT - option B: When using CHANGES(INFORMATION => DEFAULT), METADATA$ACTION returns 
      either 'INSERT' or 'DELETE'. UPDATE operations are represented as a DELETE of the old row 
      paired with an INSERT of the new row. METADATA$ISUPDATE = TRUE flag identifies these paired 
      rows as originating from an UPDATE.
      WRONG - option A: The METADATA$ACTION column only shows INSERT or DELETE, not the original 
      DML type directly.
      WRONG - option C: The username is not captured in METADATA$ACTION. Query history would 
      need to be checked for that.
      WRONG - option D: The timestamp is in METADATA$ROW_ID or can be derived from the Time 
      Travel window, not in METADATA$ACTION.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "052"
    question: "What is the correct syntax to create a zero-copy clone of a database at a point 48 hours ago?"
    their_options:
      option A: "CREATE DATABASE DB_BACKUP CLONE PROD_DB AT (OFFSET => -172800);"
      option B: "CREATE CLONE DATABASE DB_BACKUP FROM PROD_DB AT (OFFSET => -172800);"
      option C: "CLONE DATABASE PROD_DB AS DB_BACKUP AT (OFFSET => -172800);"
      option D: "CREATE DATABASE DB_BACKUP AS CLONE OF PROD_DB AT (OFFSET => -172800);"
    correct Answer: "option A: CREATE DATABASE DB_BACKUP CLONE PROD_DB AT (OFFSET => -172800);"
    explanation: >
      CORRECT - option A: The Snowflake syntax for cloning is CREATE <object_type> <new_name> 
      CLONE <source_name> [AT/BEFORE clause]. 48 hours = 48 × 3600 = 172,800 seconds, so 
      -172800 is correct for the OFFSET.
      WRONG - option B: CREATE CLONE DATABASE is not valid Snowflake DDL syntax.
      WRONG - option C: CLONE DATABASE is not valid Snowflake DDL syntax.
      WRONG - option D: AS CLONE OF is not valid Snowflake clone syntax; the correct syntax 
      uses CLONE (not AS CLONE OF).
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "053"
    question: "A company stores sensitive PII data and has a regulatory requirement to ensure data is deleted after 30 days. They currently have DATA_RETENTION_TIME_IN_DAYS = 30. Is this configuration compliant?"
    their_options:
      option A: "Yes, data is permanently deleted after 30 days"
      option B: "No, because after 30 days Time Travel expires, the data enters 7-day Fail-safe, so data persists for 37 days total on permanent tables"
      option C: "Yes, because compliance only considers active data, not Time Travel"
      option D: "No, because Snowflake cannot delete data to comply with regulations"
    correct Answer: "option B: No, because after 30 days Time Travel expires, the data enters 7-day Fail-safe, so data persists for 37 days total on permanent tables"
    explanation: >
      CORRECT - option B: Permanent tables have a mandatory 7-day Fail-safe period AFTER Time 
      Travel expires. So with 30 days Time Travel, data actually remains in Snowflake for 37 days 
      total before being permanently purged. This must be considered for compliance.
      WRONG - option A: Data is NOT permanently deleted after exactly 30 days on permanent tables 
      due to the 7-day Fail-safe period.
      WRONG - option C: Regulatory requirements typically include ALL copies of data, including 
      Time Travel and Fail-safe storage.
      WRONG - option D: Snowflake CAN delete data — data is purged after the retention + Fail-safe 
      period. The issue is the timeline, not the capability.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect, DataEngineer

  - Question No: "054"
    question: "Which editions of Snowflake support Time Travel retention periods greater than 1 day? (SELECT TWO)"
    their_options:
      option A: "Standard Edition"
      option B: "Enterprise Edition"
      option C: "Business Critical Edition"
      option D: "Developer Edition"
      option E: "Trial Edition"
    correct Answer: "option B: Enterprise Edition, option C: Business Critical Edition"
    explanation: >
      CORRECT - option B (Enterprise Edition): Supports up to 90 days Time Travel retention.
      CORRECT - option C (Business Critical Edition): Also supports up to 90 days Time Travel retention.
      WRONG - option A (Standard Edition): Limited to a maximum of 1 day Time Travel retention.
      WRONG - option D (Developer Edition): Developer edition is based on Standard Edition 
      capabilities; max 1 day Time Travel.
      WRONG - option E (Trial Edition): Trial accounts are Standard Edition equivalent; 
      max 1 day Time Travel.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, Architect

  - Question No: "055"
    question: "What happens when you UNDROP a table that was dropped more than DATA_RETENTION_TIME_IN_DAYS ago?"
    their_options:
      option A: "The table is restored from Fail-safe automatically"
      option B: "An error is returned stating the object no longer exists in Time Travel"
      option C: "Snowflake asks you to contact support to restore from Fail-safe"
      option D: "The table is restored with no data (empty)"
    correct Answer: "option B: An error is returned stating the object no longer exists in Time Travel"
    explanation: >
      CORRECT - option B: Once the Time Travel retention period expires for a dropped object, 
      UNDROP is no longer possible via self-service. The UNDROP command will return an error 
      because the object's Time Travel data has been purged.
      WRONG - option A: Fail-safe data cannot be accessed directly by users; UNDROP does NOT 
      automatically fall back to Fail-safe.
      WRONG - option C: While contacting support to access Fail-safe is possible, Snowflake 
      does NOT automatically prompt you to do so via UNDROP. The UNDROP simply fails.
      WRONG - option D: UNDROP either fully restores the table or fails entirely; it does not 
      restore an empty shell.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "056"
    question: "A large table is frequently updated and has DATA_RETENTION_TIME_IN_DAYS = 90. A data engineer notices storage costs are very high. Which TWO actions would directly reduce Time Travel storage costs? (SELECT TWO)"
    their_options:
      option A: "Reduce DATA_RETENTION_TIME_IN_DAYS to a smaller value"
      option B: "Pause the Snowflake virtual warehouse"
      option C: "Switch the table from permanent to transient type"
      option D: "Drop and recreate the table weekly"
      option E: "Increase warehouse size"
    correct Answer: "option A: Reduce DATA_RETENTION_TIME_IN_DAYS, option C: Switch to transient type"
    explanation: >
      CORRECT - option A: Reducing retention directly lowers the Time Travel storage window, 
      purging older historical versions and reducing storage consumption.
      CORRECT - option C: Transient tables have a maximum 1-day retention, massively reducing 
      Time Travel storage. Note: this also removes Fail-safe (7 days), further reducing storage 
      costs (though it also reduces protection).
      WRONG - option B: Pausing a warehouse stops compute billing but has NO effect on storage 
      costs. Time Travel data accumulates based on DML, not warehouse activity.
      WRONG - option D: Dropping and recreating purges Time Travel for the old table but creates 
      operational complexity and doesn't address the ongoing accumulation issue.
      WRONG - option E: Increasing warehouse size increases compute costs and does nothing to 
      reduce storage costs.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "057"
    question: "Which column in SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS specifically shows the bytes consumed by Time Travel for each table?"
    their_options:
      option A: "ACTIVE_BYTES"
      option B: "TIME_TRAVEL_BYTES"
      option C: "FAILSAFE_BYTES"
      option D: "RETAINED_FOR_CLONE_BYTES"
    correct Answer: "option B: TIME_TRAVEL_BYTES"
    explanation: >
      CORRECT - option B: The TIME_TRAVEL_BYTES column in TABLE_STORAGE_METRICS shows the number 
      of bytes of historical data retained for Time Travel purposes for each table.
      WRONG - option A: ACTIVE_BYTES represents the current active (live) data in the table, 
      not the Time Travel historical data.
      WRONG - option C: FAILSAFE_BYTES shows the bytes consumed by the Fail-safe period 
      (separate from Time Travel).
      WRONG - option D: RETAINED_FOR_CLONE_BYTES shows bytes retained because a clone references 
      that data (zero-copy clone reference retention), separate from Time Travel.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "058"
    question: "Scenario: A developer accidentally runs DROP SCHEMA PRODUCTION CASCADE. The schema contained 50 tables. What is the best recovery approach?"
    their_options:
      option A: "UNDROP TABLE for each of the 50 tables individually"
      option B: "UNDROP SCHEMA PRODUCTION; — this restores the schema and all 50 tables"
      option C: "Contact Snowflake Support to restore from Fail-safe"
      option D: "Recreate the schema and tables from backup"
    correct Answer: "option B: UNDROP SCHEMA PRODUCTION; — this restores the schema and all 50 tables"
    explanation: >
      CORRECT - option B: UNDROP SCHEMA restores the schema AND all its child objects (tables, views, 
      etc.) that were dropped along with it. You do not need to UNDROP each table individually. 
      This is the most efficient and complete recovery approach.
      WRONG - option A: You cannot UNDROP individual tables that were dropped as part of a CASCADE 
      schema drop without first restoring the schema. Also, 50 individual UNDROPs is unnecessary.
      WRONG - option C: Fail-safe is not necessary here since Time Travel is still available 
      (assuming within retention period). Fail-safe should only be a last resort after Time 
      Travel is exhausted.
      WRONG - option D: Manual recreation is unnecessary when Time Travel provides an automated 
      UNDROP capability for the entire schema.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "059"
    question: "A developer attempts: SELECT * FROM MY_TABLE AT (TIMESTAMP => '2024-01-01') BEFORE (OFFSET => -3600); Is this valid?"
    their_options:
      option A: "Yes, you can chain AT and BEFORE for more precise time travel"
      option B: "No, only one of AT or BEFORE can be specified per table reference"
      option C: "Yes, but only in Enterprise Edition"
      option D: "Yes, but only when using TIMESTAMP with AT and OFFSET with BEFORE"
    correct Answer: "option B: No, only one of AT or BEFORE can be specified per table reference"
    explanation: >
      CORRECT - option B: In Snowflake Time Travel, you can only specify ONE of AT or BEFORE per 
      table reference. Chaining both in the same table reference is a syntax error.
      WRONG - option A: AT and BEFORE cannot be chained; this is invalid syntax and will throw an error.
      WRONG - option C: This restriction applies in all editions; it is not lifted in Enterprise Edition.
      WRONG - option D: The type of sub-clause (TIMESTAMP vs OFFSET) does not change the 
      rule — only one of AT or BEFORE per table reference.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "060"
    question: "Which statements about Time Travel for Streams in Snowflake are TRUE? (SELECT TWO)"
    their_options:
      option A: "Streams use Time Travel internally to track changes relative to their offset"
      option B: "A stream's offset advances automatically as data is consumed from it"
      option C: "Streams can be queried with AT/BEFORE like regular tables"
      option D: "If Time Travel data expires before a stream is consumed, the stream becomes stale"
      option E: "Streams replace the need for Time Travel entirely"
    correct Answer: "option A: Streams use Time Travel internally, option D: Stale stream if Time Travel expires"
    explanation: >
      CORRECT - option A: Snowflake Streams internally leverage Time Travel to track changes. 
      The stream maintains an offset into the Time Travel history of the source table, enabling 
      it to identify changed rows since the last consumption.
      CORRECT - option D: If the Time Travel data that a stream's offset references expires 
      (because it's older than the retention period), the stream becomes STALE and can no longer 
      be queried. This is a critical operational consideration when designing stream-based pipelines 
      — the table must have sufficient retention to prevent stale streams.
      WRONG - option B: A stream's offset only advances when the stream is consumed in a DML 
      transaction (INSERT, UPDATE, MERGE, DELETE using the stream). It does NOT advance automatically.
      WRONG - option C: Streams themselves are not time-travel queryable with AT/BEFORE; they 
      represent the current unconsumed changes.
      WRONG - option E: Streams and Time Travel serve complementary purposes. Streams are 
      forward-looking CDC; Time Travel is backward-looking recovery. Neither replaces the other.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "061"
    question: "Which Snowflake object type does NOT support Time Travel at all?"
    their_options:
      option A: "Permanent Table"
      option B: "Transient Table"
      option C: "External Table"
      option D: "Temporary Table"
    correct Answer: "option C: External Table"
    explanation: >
      CORRECT - option C: External tables reference data stored in external cloud storage (S3, Azure Blob, GCS) 
      and are entirely metadata-based inside Snowflake. Since the underlying data is not managed by Snowflake, 
      Time Travel cannot be applied to external tables.
      WRONG - option A: Permanent tables fully support Time Travel with up to 90 days retention (Enterprise).
      WRONG - option B: Transient tables support Time Travel with 0 or 1 day retention.
      WRONG - option D: Temporary tables support Time Travel for the session duration with 0 or 1 day retention.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, DataAnalyst

  - Question No: "062"
    question: "What is the Fail-safe period for a Transient table in Snowflake?"
    their_options:
      option A: "7 days"
      option B: "1 day"
      option C: "0 days (no Fail-safe)"
      option D: "Same as the DATA_RETENTION_TIME_IN_DAYS setting"
    correct Answer: "option C: 0 days (no Fail-safe)"
    explanation: >
      CORRECT - option C: Transient tables have NO Fail-safe period. This is one of the key trade-offs 
      of using transient tables — reduced storage cost at the expense of the additional 7-day safety net.
      WRONG - option A: 7-day Fail-safe only applies to permanent tables, not transient ones.
      WRONG - option B: 1 day is the maximum Time Travel retention for transient tables, not a Fail-safe period.
      WRONG - option D: Fail-safe is a fixed 7-day period for permanent tables and has no relationship 
      to the DATA_RETENTION_TIME_IN_DAYS parameter.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, Architect

  - Question No: "063"
    question: "A query uses: SELECT * FROM ORDERS AT (OFFSET => 3600); What will this return?"
    their_options:
      option A: "Data from 1 hour ago"
      option B: "Data from 1 hour in the future — which will throw an error"
      option C: "Current data (positive offset is treated as 0)"
      option D: "Data from exactly 1 hour ago"
    correct Answer: "option B: Data from 1 hour in the future — which will throw an error"
    explanation: >
      CORRECT - option B: A positive OFFSET value (e.g., +3600) would reference a time 1 hour in the 
      future, which doesn't exist. Snowflake will throw an error because you cannot query future data. 
      Time Travel only works backwards; OFFSET must be negative or zero.
      WRONG - option A: A negative offset (-3600) would give data from 1 hour ago. Positive does not.
      WRONG - option C: Snowflake does not silently treat invalid positive offsets as 0; it throws an error.
      WRONG - option D: This is the result of a NEGATIVE offset (-3600), not a positive one.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "064"
    question: "How many days of total data protection (Time Travel + Fail-safe) does a Transient table with DATA_RETENTION_TIME_IN_DAYS = 1 provide?"
    their_options:
      option A: "8 days (1 + 7)"
      option B: "1 day"
      option C: "7 days"
      option D: "0 days"
    correct Answer: "option B: 1 day"
    explanation: >
      CORRECT - option B: Transient tables have NO Fail-safe period. Total protection = 1 day Time Travel + 
      0 days Fail-safe = 1 day. This is a significant distinction from permanent tables.
      WRONG - option A: The 7-day Fail-safe only applies to permanent tables. Transient tables have no Fail-safe.
      WRONG - option C: 7 days would only apply if there was a Fail-safe period, which transient tables lack.
      WRONG - option D: 0 days is incorrect; the 1-day Time Travel retention still provides 1 day of protection.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, Architect

  - Question No: "065"
    question: "Which of the following best describes the storage cost impact of Time Travel on a table that has NO updates or deletes (insert-only workload)?"
    their_options:
      option A: "Significant cost increase proportional to the number of inserts"
      option B: "Minimal cost increase because Time Travel only stores changed/deleted data versions"
      option C: "No cost at all — Time Travel is free for insert-only tables"
      option D: "Cost doubles because every inserted row is stored twice"
    correct Answer: "option B: Minimal cost increase because Time Travel only stores changed/deleted data versions"
    explanation: >
      CORRECT - option B: Time Travel storage costs arise from retaining old versions of modified or 
      deleted data. If a table only has inserts (no updates or deletes), the Time Travel overhead is 
      minimal — only the metadata overhead. Newly inserted rows don't create historical versions until 
      they are later modified or deleted.
      WRONG - option A: Inserts alone don't proportionally increase Time Travel storage; it's 
      updates and deletes that create historical versions.
      WRONG - option C: There is still some minor overhead, but it's minimal for insert-only workloads.
      WRONG - option D: Rows are not stored twice on insert. Historical copies are only made when 
      existing rows are changed or removed.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "066"
    question: "What SQL command would you use to alter an existing table to have 30 days of Time Travel retention?"
    their_options:
      option A: "MODIFY TABLE ORDERS SET DATA_RETENTION_TIME_IN_DAYS = 30;"
      option B: "ALTER TABLE ORDERS SET DATA_RETENTION_TIME_IN_DAYS = 30;"
      option C: "UPDATE TABLE ORDERS SET DATA_RETENTION_TIME_IN_DAYS = 30;"
      option D: "ALTER TABLE ORDERS MODIFY DATA_RETENTION_TIME_IN_DAYS = 30;"
    correct Answer: "option B: ALTER TABLE ORDERS SET DATA_RETENTION_TIME_IN_DAYS = 30;"
    explanation: >
      CORRECT - option B: The correct Snowflake DDL syntax to change table parameters is 
      ALTER TABLE <name> SET <parameter> = <value>. This applies to DATA_RETENTION_TIME_IN_DAYS 
      and other table properties.
      WRONG - option A: MODIFY TABLE is not valid Snowflake DDL syntax.
      WRONG - option C: UPDATE TABLE is not valid DDL. UPDATE is a DML command for modifying rows, 
      not table properties.
      WRONG - option D: ALTER TABLE ORDERS MODIFY is not the correct syntax; SET is required.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "067"
    question: "A developer wants to disable Time Travel for a specific schema to reduce costs. Which command achieves this?"
    their_options:
      option A: "DROP TIME TRAVEL ON SCHEMA MY_SCHEMA;"
      option B: "ALTER SCHEMA MY_SCHEMA SET DATA_RETENTION_TIME_IN_DAYS = 0;"
      option C: "ALTER SCHEMA MY_SCHEMA DISABLE TIME_TRAVEL;"
      option D: "SET TIME_TRAVEL = FALSE ON SCHEMA MY_SCHEMA;"
    correct Answer: "option B: ALTER SCHEMA MY_SCHEMA SET DATA_RETENTION_TIME_IN_DAYS = 0;"
    explanation: >
      CORRECT - option B: Setting DATA_RETENTION_TIME_IN_DAYS = 0 at the schema level disables 
      Time Travel for the schema and all tables within it that inherit this setting (those without 
      explicit table-level retention settings).
      WRONG - option A: DROP TIME TRAVEL is not valid Snowflake syntax.
      WRONG - option C: DISABLE TIME_TRAVEL is not a valid Snowflake clause.
      WRONG - option D: There is no SET TIME_TRAVEL = FALSE syntax in Snowflake.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "068"
    question: "What does the AT (STATEMENT => '<query_id>') clause return compared to BEFORE (STATEMENT => '<query_id>')?"
    their_options:
      option A: "AT returns data before the statement; BEFORE returns data after"
      option B: "AT returns data at the end of the statement (post-DML); BEFORE returns data just before (pre-DML)"
      option C: "They are identical — AT and BEFORE with STATEMENT produce the same results"
      option D: "AT only works with SELECT; BEFORE only works with DML statements"
    correct Answer: "option B: AT returns data at the end of the statement (post-DML); BEFORE returns data just before (pre-DML)"
    explanation: >
      CORRECT - option B: AT (STATEMENT => ...) returns the data as it appears AFTER the specified 
      statement executed (inclusive of changes made by that statement). BEFORE (STATEMENT => ...) 
      returns the data as it was just BEFORE that statement ran (exclusive). This distinction is 
      critical for recovery: use BEFORE to see pre-DML state.
      WRONG - option A: This is exactly backwards; AT is post-statement, BEFORE is pre-statement.
      WRONG - option C: They produce different results — AT shows the changed state, BEFORE shows 
      the unchanged state.
      WRONG - option D: Both AT and BEFORE work independently of the query type; they reference 
      any historical query ID regardless of DML type.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "069"
    question: "Can Time Travel be used to query historical data from a Snowflake STAGE?"
    their_options:
      option A: "Yes, all Snowflake objects support Time Travel"
      option B: "No, stages are not tables and do not support Time Travel"
      option C: "Yes, but only for internal stages"
      option D: "Yes, but only for named external stages"
    correct Answer: "option B: No, stages are not tables and do not support Time Travel"
    explanation: >
      CORRECT - option B: Stages (internal and external) are storage locations for loading/unloading 
      data. They are not tables and do not support Time Travel. The data in stages is managed 
      externally (or in the cloud storage layer) and not tracked by Snowflake's historical versioning.
      WRONG - option A: Not all Snowflake objects support Time Travel — stages, views, and external 
      tables are key exceptions.
      WRONG - option C: Internal stages do not support Time Travel regardless of type.
      WRONG - option D: External named stages point to external cloud storage and have no 
      Snowflake-managed historical versioning.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "070"
    question: "A team needs to recover data from a table that was accidentally truncated. The truncation happened 5 hours ago and the table has 7-day retention. Which statement recovers ALL truncated rows?"
    their_options:
      option A: "UNDROP TABLE MY_TABLE;"
      option B: "INSERT INTO MY_TABLE SELECT * FROM MY_TABLE AT (OFFSET => -18000);"
      option C: "INSERT INTO MY_TABLE SELECT * FROM MY_TABLE BEFORE (OFFSET => -18000);"
      option D: "RESTORE TABLE MY_TABLE AT (OFFSET => -18000);"
    correct Answer: "option C: INSERT INTO MY_TABLE SELECT * FROM MY_TABLE BEFORE (OFFSET => -18000);"
    explanation: >
      CORRECT - option C: Since TRUNCATE was issued 5 hours ago (18000 seconds), we need the data 
      BEFORE the truncate ran. Using BEFORE (OFFSET => -18000) gives us the state just before the 
      truncation. We then INSERT this data back into the now-empty table.
      WRONG - option A: UNDROP TABLE is for DROPPED tables, not TRUNCATED ones. A truncated table 
      still exists; only its rows were deleted.
      WRONG - option B: AT (OFFSET => -18000) queries data at a time 5 hours ago, but this may 
      include the state after the truncation if the truncation happened exactly 5 hours ago. 
      BEFORE is more precise for this scenario.
      WRONG - option D: RESTORE TABLE is not valid Snowflake SQL syntax.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "071"
    question: "What is the retention period for Time Travel on a Temporary table that is actively being used in a session?"
    their_options:
      option A: "90 days maximum"
      option B: "7 days maximum"
      option C: "0 or 1 day maximum, and data is purged when the session ends regardless"
      option D: "Indefinite until explicitly dropped"
    correct Answer: "option C: 0 or 1 day maximum, and data is purged when the session ends regardless"
    explanation: >
      CORRECT - option C: Temporary tables exist only for the duration of the session. They support 
      0 or 1 day of Time Travel. However, when the session ends, the temporary table is dropped and 
      all its data (including Time Travel history) is purged — even if within the 1-day window.
      WRONG - option A: 90 days is only for permanent tables in Enterprise Edition.
      WRONG - option B: 7 days is not a valid retention for temporary tables.
      WRONG - option D: Temporary tables are NOT indefinite; they're automatically dropped on session end.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "072"
    question: "Which of the following Time Travel queries using TIMESTAMP syntax is correctly formed?"
    their_options:
      option A: "SELECT * FROM T AT TIMESTAMP = '2024-06-01';"
      option B: "SELECT * FROM T AT (TIMESTAMP = '2024-06-01'::TIMESTAMP_LTZ);"
      option C: "SELECT * FROM T AT (TIMESTAMP => '2024-06-01'::TIMESTAMP_LTZ);"
      option D: "SELECT * FROM T WHERE TIMESTAMP = '2024-06-01';"
    correct Answer: "option C: SELECT * FROM T AT (TIMESTAMP => '2024-06-01'::TIMESTAMP_LTZ);"
    explanation: >
      CORRECT - option C: The correct Snowflake Time Travel syntax uses the fat arrow (=>) named 
      argument syntax inside parentheses: AT (TIMESTAMP => <value>). The timestamp value should 
      be properly cast to a timestamp type.
      WRONG - option A: Missing parentheses and uses = instead of =>; this is invalid syntax.
      WRONG - option B: Uses = instead of => inside the parentheses; this is invalid syntax.
      WRONG - option D: This is a regular WHERE filter, not a Time Travel query. It filters rows 
      where a column named TIMESTAMP has that value — completely different from Time Travel.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "073"
    question: "A Snowflake account is on Standard Edition. A user tries to create a table with DATA_RETENTION_TIME_IN_DAYS = 10. What happens?"
    their_options:
      option A: "The table is created with 10 days retention"
      option B: "An error is thrown because Standard Edition only supports 0 or 1 day"
      option C: "The table is created but retention is silently capped at 1 day"
      option D: "The table is created with 7 days retention (Standard maximum)"
    correct Answer: "option B: An error is thrown because Standard Edition only supports 0 or 1 day"
    explanation: >
      CORRECT - option B: Standard Edition enforces a hard maximum of 1 day for DATA_RETENTION_TIME_IN_DAYS. 
      Attempting to set a value greater than 1 on Standard Edition returns an error stating the 
      retention time exceeds the maximum allowed for the current edition.
      WRONG - option A: Standard Edition does not allow 10 days retention — an error is thrown.
      WRONG - option C: Snowflake does NOT silently cap the value; it explicitly rejects the DDL.
      WRONG - option D: 7 days is not the Standard Edition maximum; 1 day is.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "074"
    question: "Which Snowflake role is needed to alter the DATA_RETENTION_TIME_IN_DAYS parameter at the ACCOUNT level?"
    their_options:
      option A: "SYSADMIN"
      option B: "SECURITYADMIN"
      option C: "ACCOUNTADMIN"
      option D: "Any role with CREATE TABLE privilege"
    correct Answer: "option C: ACCOUNTADMIN"
    explanation: >
      CORRECT - option C: Modifying account-level parameters requires the ACCOUNTADMIN role, as these 
      settings affect the entire Snowflake account. Account-level parameter changes are restricted 
      to the most privileged role.
      WRONG - option A: SYSADMIN can manage databases, warehouses, and objects but cannot alter 
      account-level parameters like DATA_RETENTION_TIME_IN_DAYS at account scope.
      WRONG - option B: SECURITYADMIN manages roles and privileges but does not have authority 
      over account-level operational parameters.
      WRONG - option D: CREATE TABLE privilege is an object-level privilege, not sufficient for 
      account-level parameter changes.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect, DataEngineer

  - Question No: "075"
    question: "What is the INFORMATION_SCHEMA view used to query Time Travel retention settings for tables in the current database?"
    their_options:
      option A: "INFORMATION_SCHEMA.TIME_TRAVEL_SETTINGS"
      option B: "INFORMATION_SCHEMA.TABLES"
      option C: "INFORMATION_SCHEMA.TABLE_PRIVILEGES"
      option D: "INFORMATION_SCHEMA.DATA_RETENTION"
    correct Answer: "option B: INFORMATION_SCHEMA.TABLES"
    explanation: >
      CORRECT - option B: INFORMATION_SCHEMA.TABLES contains a RETENTION_TIME column that shows 
      the DATA_RETENTION_TIME_IN_DAYS setting for each table. You can query it to audit retention 
      settings across tables in the current database.
      WRONG - option A: INFORMATION_SCHEMA.TIME_TRAVEL_SETTINGS does not exist in Snowflake.
      WRONG - option C: TABLE_PRIVILEGES shows access control grants, not retention settings.
      WRONG - option D: INFORMATION_SCHEMA.DATA_RETENTION is not a valid Snowflake view.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "076"
    question: "Scenario: An organization has a table AUDIT_LOG that must retain data for exactly 90 days for compliance and then be immediately purged. They are on Enterprise Edition. What is the ideal configuration?"
    their_options:
      option A: "Permanent table with DATA_RETENTION_TIME_IN_DAYS = 90"
      option B: "Transient table with DATA_RETENTION_TIME_IN_DAYS = 1 and a scheduled job to delete old records"
      option C: "Permanent table with DATA_RETENTION_TIME_IN_DAYS = 83 (so total with 7-day Fail-safe = 90)"
      option D: "External table pointing to data in S3 with a 90-day lifecycle policy"
    correct Answer: "option C: Permanent table with DATA_RETENTION_TIME_IN_DAYS = 83 (so total with 7-day Fail-safe = 90)"
    explanation: >
      CORRECT - option C: For compliance requiring data purged exactly at 90 days, you must account 
      for the 7-day Fail-safe on permanent tables. Time Travel (83 days) + Fail-safe (7 days) = 90 
      days total before data is truly purged. This ensures complete deletion at the 90-day mark.
      WRONG - option A: With 90 days Time Travel + 7 days Fail-safe = 97 total days before purge, 
      which violates the 90-day compliance requirement.
      WRONG - option B: Using a 1-day transient table means data is not retained for 90 days as 
      required. The deletion job adds operational complexity but doesn't solve the retention guarantee.
      WRONG - option D: External tables let Snowflake avoid managing the data, but you'd need 
      to ensure the S3 lifecycle policy handles the 90-day deletion properly. More complex.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect, DataEngineer

  - Question No: "077"
    question: "What does the SHOW TABLES command display regarding Time Travel for each table?"
    their_options:
      option A: "The TIME_TRAVEL_BYTES column showing historical data size"
      option B: "The RETENTION_TIME column showing the DATA_RETENTION_TIME_IN_DAYS value"
      option C: "The LAST_TIME_TRAVEL_QUERY showing the most recent AT/BEFORE query"
      option D: "No Time Travel information is shown in SHOW TABLES"
    correct Answer: "option B: The RETENTION_TIME column showing the DATA_RETENTION_TIME_IN_DAYS value"
    explanation: >
      CORRECT - option B: SHOW TABLES output includes a RETENTION_TIME column that displays the 
      effective DATA_RETENTION_TIME_IN_DAYS for each table, allowing quick auditing of retention 
      settings across a schema.
      WRONG - option A: TIME_TRAVEL_BYTES is found in ACCOUNT_USAGE.TABLE_STORAGE_METRICS, 
      not in SHOW TABLES output.
      WRONG - option C: Snowflake does not track or display the most recent Time Travel query 
      in SHOW TABLES.
      WRONG - option D: SHOW TABLES does include retention time information via the RETENTION_TIME column.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "078"
    question: "What happens to the Fail-safe period when a table is dropped in Snowflake?"
    their_options:
      option A: "Fail-safe starts immediately after the table is dropped and lasts 7 days"
      option B: "Fail-safe only begins after the Time Travel retention period expires for the dropped table"
      option C: "Fail-safe is cancelled when a table is dropped"
      option D: "Fail-safe and Time Travel both start simultaneously when the table is dropped"
    correct Answer: "option B: Fail-safe only begins after the Time Travel retention period expires for the dropped table"
    explanation: >
      CORRECT - option B: When a table is dropped, the Time Travel retention period begins counting. 
      Only AFTER the Time Travel period expires does the data transition to the Fail-safe period 
      (7 days for permanent tables). The total protection window is Time Travel + Fail-safe.
      WRONG - option A: Fail-safe doesn't start immediately on drop; it follows Time Travel expiration.
      WRONG - option C: Fail-safe is not cancelled on drop; it is part of the data lifecycle for 
      permanent tables.
      WRONG - option D: They don't start simultaneously; they are sequential phases of data retention.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, Architect

  - Question No: "079"
    question: "A table ORDERS is dropped and then immediately recreated with the same name. Can the original dropped ORDERS table be recovered?"
    their_options:
      option A: "No, creating a new table with the same name permanently overwrites the dropped table"
      option B: "Yes, UNDROP TABLE ORDERS will restore the original because UNDROP finds the most recently dropped version"
      option C: "Yes, but you must first rename or drop the new ORDERS table, then run UNDROP TABLE ORDERS"
      option D: "Yes, UNDROP TABLE ORDERS BEFORE (OFFSET => -60) will restore the original"
    correct Answer: "option C: Yes, but you must first rename or drop the new ORDERS table, then run UNDROP TABLE ORDERS"
    explanation: >
      CORRECT - option C: UNDROP restores the most recently dropped table with a given name. However, 
      if a live table with the same name already exists, UNDROP fails due to a naming conflict. You 
      must first rename the new ORDERS table (ALTER TABLE ORDERS RENAME TO ORDERS_NEW), then run 
      UNDROP TABLE ORDERS to restore the original.
      WRONG - option A: Creating a new table with the same name does NOT overwrite the dropped table's 
      Time Travel data; the original can still be recovered.
      WRONG - option B: UNDROP doesn't override an existing live table; it would fail with a 
      name conflict error.
      WRONG - option D: UNDROP TABLE does not accept AT/BEFORE clauses; the Time Travel syntax 
      is for querying, not UNDROP operations.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "080"
    question: "Which of the following is TRUE about Time Travel and Snowflake cloning? (SELECT TWO)"
    their_options:
      option A: "You can clone a table at a historical point using AT or BEFORE"
      option B: "Cloning a table at a historical point copies all data (not zero-copy)"
      option C: "A clone created from a historical point shares micro-partitions with the source"
      option D: "Historical clones are only supported for databases, not tables"
      option E: "Clones created from Time Travel points cannot be further cloned"
    correct Answer: "option A: Clone at historical point using AT or BEFORE, option C: Shares micro-partitions with source"
    explanation: >
      CORRECT - option A: Snowflake fully supports CREATE TABLE|SCHEMA|DATABASE <new> CLONE <source> 
      AT/BEFORE (...) to create a zero-copy clone at a historical point in time.
      CORRECT - option C: Zero-copy clones created via Time Travel share the underlying micro-partitions 
      with the source at the cloned point. No data is physically copied; only metadata is created.
      WRONG - option B: Historical clones ARE zero-copy clones — they don't physically copy data.
      WRONG - option D: Historical cloning is supported for tables, schemas, and databases.
      WRONG - option E: Clones can be further cloned. There is no restriction on cloning a clone.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "081"
    question: "What SQL can you use to check whether a specific table currently has Time Travel enabled?"
    their_options:
      option A: "SELECT TIME_TRAVEL_ENABLED FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'MY_TABLE';"
      option B: "SHOW TABLES LIKE 'MY_TABLE'; -- check the RETENTION_TIME column"
      option C: "DESCRIBE TABLE MY_TABLE; -- check the TIME_TRAVEL column"
      option D: "SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.TIME_TRAVEL_STATUS WHERE TABLE_NAME = 'MY_TABLE';"
    correct Answer: "option B: SHOW TABLES LIKE 'MY_TABLE'; -- check the RETENTION_TIME column"
    explanation: >
      CORRECT - option B: SHOW TABLES displays a RETENTION_TIME column. If it shows 0, Time Travel 
      is disabled. If it shows a value >= 1, Time Travel is enabled with that many days of retention.
      WRONG - option A: INFORMATION_SCHEMA.TABLES has a RETENTION_TIME column (not TIME_TRAVEL_ENABLED), 
      but the query syntax shown is slightly off. SHOW TABLES is simpler for this check.
      WRONG - option C: DESCRIBE TABLE shows column-level metadata (data types, nullability, defaults) 
      and does not include a TIME_TRAVEL column.
      WRONG - option D: SNOWFLAKE.ACCOUNT_USAGE.TIME_TRAVEL_STATUS is not a valid view name.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "082"
    question: "A data pipeline inserts 100 million rows per day into a large fact table with 30-day Time Travel retention. The table is never updated or deleted. What is the primary Time Travel storage cost concern?"
    their_options:
      option A: "Very high — each insert creates a Time Travel version of the entire table"
      option B: "Minimal — inserts into new micro-partitions don't create Time Travel overhead; only modified/deleted partitions are stored"
      option C: "Moderate — each row inserted is stored once in active storage and once in Time Travel"
      option D: "None — insert-only tables are exempt from Time Travel storage charges"
    correct Answer: "option B: Minimal — inserts into new micro-partitions don't create Time Travel overhead; only modified/deleted partitions are stored"
    explanation: >
      CORRECT - option B: Snowflake's Time Travel works at the micro-partition level. When new rows 
      are inserted, they create new micro-partitions. These new partitions don't have old versions 
      to retain in Time Travel until they are later modified or deleted. For an insert-only workload, 
      Time Travel overhead is minimal.
      WRONG - option A: Time Travel does not store a full copy of the table on every insert.
      WRONG - option C: Inserts do not double-store rows; only the before-image of changed data 
      is retained in Time Travel.
      WRONG - option D: Insert-only tables are not exempt — they do have some Time Travel storage 
      (micro-partition metadata), but the cost is minimal compared to tables with heavy DML.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "083"
    question: "Which command shows all objects in a database that have been dropped and are still recoverable via Time Travel?"
    their_options:
      option A: "SHOW DROPPED TABLES IN DATABASE MY_DB;"
      option B: "SHOW TABLES HISTORY IN DATABASE MY_DB;"
      option C: "SHOW TABLES HISTORY IN SCHEMA MY_SCHEMA;"
      option D: "Both B and C are valid"
    correct Answer: "option D: Both B and C are valid"
    explanation: >
      CORRECT - option D: SHOW TABLES HISTORY lists all tables including dropped ones that are still 
      within the Time Travel retention window. This can be scoped to a schema (SHOW TABLES HISTORY 
      IN SCHEMA) or implicitly to the current database/schema.
      WRONG - option A: SHOW DROPPED TABLES is not valid Snowflake syntax.
      WRONG - option B: Valid but not the only answer — option C also works.
      WRONG - option C: Valid but not the only answer — option B also works.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "084"
    question: "What happens to a Snowflake Stream's staleness if the underlying table's DATA_RETENTION_TIME_IN_DAYS is set to 0?"
    their_options:
      option A: "The stream works normally because streams don't use Time Travel"
      option B: "The stream immediately becomes stale and cannot be used"
      option C: "The stream is automatically deleted"
      option D: "The stream is paused until Time Travel is re-enabled"
    correct Answer: "option B: The stream immediately becomes stale and cannot be used"
    explanation: >
      CORRECT - option B: Streams rely on Time Travel to maintain their offset into the table's 
      change history. If DATA_RETENTION_TIME_IN_DAYS = 0 is set, Time Travel is disabled and the 
      historical data the stream references is immediately purged. The stream becomes stale 
      (STALE = TRUE) and can no longer be queried.
      WRONG - option A: Streams DO use Time Travel internally to track their offset position.
      WRONG - option C: The stream object itself is not automatically deleted; it just becomes 
      stale/unusable.
      WRONG - option D: Streams don't have a "paused" state relative to Time Travel; they either 
      work (STALE = FALSE) or fail (STALE = TRUE).
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "085"
    question: "What is the syntax to query a view's underlying base table at a historical point in time?"
    their_options:
      option A: "SELECT * FROM MY_VIEW AT (OFFSET => -3600);"
      option B: "Views cannot be queried with Time Travel; you must query the base table directly"
      option C: "SELECT * FROM MY_VIEW HISTORY AT (OFFSET => -3600);"
      option D: "CREATE VIEW MY_VIEW_HIST AS SELECT * FROM BASE_TABLE AT (OFFSET => -3600);"
    correct Answer: "option B: Views cannot be queried with Time Travel; you must query the base table directly"
    explanation: >
      CORRECT - option B: Views themselves don't support Time Travel via the AT/BEFORE clause. 
      To query historical data through a view's logic, you must apply the AT/BEFORE clause to 
      the underlying base table directly, or create a new query/view that references the base 
      table with a Time Travel clause.
      WRONG - option A: Applying AT to a view reference is not supported and will throw an error.
      WRONG - option C: HISTORY AT is not a valid Time Travel syntax in Snowflake.
      WRONG - option D: While this is a workaround (creating a view over a historical base table 
      query), the question asks about querying an existing view with Time Travel, which option B 
      correctly identifies as unsupported.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "086"
    question: "A permanent table has DATA_RETENTION_TIME_IN_DAYS = 14. A row is inserted on Day 1, then updated on Day 5, then the table is queried with AT (OFFSET => -86400) on Day 6. What version of the row is returned?"
    their_options:
      option A: "The original version from Day 1 (before the Day 5 update)"
      option B: "The updated version from Day 5 (after the Day 5 update)"
      option C: "The row is not returned because it was updated within 24 hours"
      option D: "Both versions are returned"
    correct Answer: "option B: The updated version from Day 5 (after the Day 5 update)"
    explanation: >
      CORRECT - option B: AT (OFFSET => -86400) on Day 6 queries the table as it was exactly 24 
      hours before Day 6, which is Day 5. The update happened on Day 5, so the returned state 
      reflects the row AFTER the Day 5 update (since 24 hours before Day 6 is still on Day 5, 
      after the update occurred).
      WRONG - option A: The pre-update state from Day 1 would only be seen if you queried at a 
      point before the Day 5 update (e.g., AT (OFFSET => -172800) on Day 6, which goes back to Day 4).
      WRONG - option C: Updates don't affect Time Travel availability; rows are always accessible 
      within the retention window.
      WRONG - option D: AT returns a single point-in-time snapshot, not multiple versions.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "087"
    question: "Which statement correctly identifies when Time Travel storage costs are HIGHEST for a table?"
    their_options:
      option A: "When the table has many SELECT queries per day"
      option B: "When the table has frequent UPDATE and DELETE operations"
      option C: "When the table has many INSERT operations into empty partitions"
      option D: "When the warehouse size is extra-large"
    correct Answer: "option B: When the table has frequent UPDATE and DELETE operations"
    explanation: >
      CORRECT - option B: Time Travel storage accumulates historical versions of changed and deleted 
      data. Tables with high rates of UPDATE and DELETE operations generate the most Time Travel 
      storage because every changed row's previous version is retained for the retention period.
      WRONG - option A: SELECT queries are read-only and generate no Time Travel storage.
      WRONG - option C: Inserts into empty/new partitions create minimal Time Travel overhead since 
      there are no previous versions to retain.
      WRONG - option D: Warehouse size affects compute costs, not storage costs. Time Travel storage 
      is independent of virtual warehouse size.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "088"
    question: "What is the correct YAML/parameter name to set Time Travel when creating a schema?"
    their_options:
      option A: "DATA_RETENTION_TIME_IN_DAYS"
      option B: "RETENTION_DAYS"
      option C: "TIME_TRAVEL_RETENTION"
      option D: "SCHEMA_RETENTION_PERIOD"
    correct Answer: "option A: DATA_RETENTION_TIME_IN_DAYS"
    explanation: >
      CORRECT - option A: The parameter name DATA_RETENTION_TIME_IN_DAYS is consistent across all 
      Snowflake object levels — it applies to tables, schemas, and databases alike.
      WRONG - option B: RETENTION_DAYS is not a valid Snowflake parameter name.
      WRONG - option C: TIME_TRAVEL_RETENTION is not a valid Snowflake parameter name.
      WRONG - option D: SCHEMA_RETENTION_PERIOD is not a valid Snowflake parameter name.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "089"
    question: "True or False: You can use Time Travel to query a Snowflake SEQUENCE at a historical point."
    their_options:
      option A: "True — sequences store historical values like tables"
      option B: "False — sequences are not tables and do not support Time Travel"
      option C: "True — but only for sequences with AUTO_INCREMENT"
      option D: "True — but only for the last 24 hours"
    correct Answer: "option B: False — sequences are not tables and do not support Time Travel"
    explanation: >
      CORRECT - option B: Sequences are database objects that generate numeric values but are not 
      tables. They do not store row data and therefore do not support Time Travel queries. You 
      cannot use AT/BEFORE on a sequence.
      WRONG - option A: Sequences don't store historical values in the same way tables do; they 
      generate monotonically increasing numbers without retaining historical state.
      WRONG - option C: AUTO_INCREMENT is a table column property, not a sequence type; this 
      distinction doesn't affect Time Travel availability on sequences.
      WRONG - option D: Time Travel doesn't apply to sequences at any window.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer

  - Question No: "090"
    question: "A developer uses SHOW TABLES HISTORY IN SCHEMA PROD. What does the 'dropped_on' column indicate?"
    their_options:
      option A: "The date the table was originally created"
      option B: "The timestamp when the table was dropped (NULL if still active)"
      option C: "The date Time Travel data will expire for the table"
      option D: "The date the table was last queried before being dropped"
    correct Answer: "option B: The timestamp when the table was dropped (NULL if still active)"
    explanation: >
      CORRECT - option B: In the SHOW TABLES HISTORY output, the dropped_on column shows the 
      timestamp when the table was dropped. For active tables that have not been dropped, 
      this column is NULL.
      WRONG - option A: Table creation time is shown in the created_on column.
      WRONG - option C: The Time Travel expiration date is not directly shown in SHOW TABLES HISTORY 
      output; it would need to be calculated from dropped_on + retention_time.
      WRONG - option D: Last query timestamp is not tracked in SHOW TABLES HISTORY output.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "091"
    question: "What is a key benefit of combining Zero-Copy Cloning with Time Travel for testing purposes?"
    their_options:
      option A: "It creates a full physical copy with separate billing for all operations"
      option B: "It allows creating a historical snapshot of production data for testing without duplicating storage"
      option C: "It automatically synchronizes test and production data in real-time"
      option D: "It eliminates the need for a virtual warehouse during testing"
    correct Answer: "option B: It allows creating a historical snapshot of production data for testing without duplicating storage"
    explanation: >
      CORRECT - option B: CREATE TABLE TEST_TABLE CLONE PROD_TABLE AT (OFFSET => ...) creates an 
      instant, zero-copy clone of production data at a historical point. The clone shares 
      micro-partitions with production, so no additional storage is consumed until the test table 
      diverges from production through DML operations.
      WRONG - option A: Zero-copy cloning does NOT create a full physical copy; storage is only 
      charged for data that diverges after clone creation.
      WRONG - option C: Clones are static snapshots at the time of creation; they do not sync with production.
      WRONG - option D: A warehouse is still required to run queries and DML on the cloned table.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "092"
    question: "A table EVENTS has been modified with a large batch update. You want to audit what changed. Which query returns only the DELETED (before-image) rows from the last 2 hours?"
    their_options:
      option A: "SELECT * FROM EVENTS CHANGES(INFORMATION => DEFAULT) AT (OFFSET => -7200) WHERE METADATA$ACTION = 'DELETE';"
      option B: "SELECT * FROM EVENTS AT (OFFSET => -7200) WHERE DELETED = TRUE;"
      option C: "SELECT * FROM EVENTS BEFORE (OFFSET => -7200) MINUS SELECT * FROM EVENTS;"
      option D: "SELECT * FROM EVENTS CHANGES(INFORMATION => APPEND_ONLY) AT (OFFSET => -7200);"
    correct Answer: "option A: SELECT * FROM EVENTS CHANGES(INFORMATION => DEFAULT) AT (OFFSET => -7200) WHERE METADATA$ACTION = 'DELETE';"
    explanation: >
      CORRECT - option A: CHANGES(INFORMATION => DEFAULT) returns all DML changes with a 
      METADATA$ACTION column. Filtering WHERE METADATA$ACTION = 'DELETE' returns only the 
      before-image (deleted/old) rows from the change window.
      WRONG - option B: There is no DELETED column in a standard Snowflake table; this is not 
      a valid approach for change detection.
      WRONG - option C: While a MINUS approach might conceptually work, it's much less efficient 
      than using the CHANGES clause and doesn't use the proper Snowflake Time Travel change tracking.
      WRONG - option D: CHANGES(INFORMATION => APPEND_ONLY) only returns INSERT operations; it 
      does not capture deletes or updates.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "093"
    question: "Can you apply Time Travel to a Materialized View in Snowflake?"
    their_options:
      option A: "Yes, materialized views support AT and BEFORE clauses"
      option B: "No, materialized views do not support Time Travel"
      option C: "Yes, but only if the base table has Time Travel enabled"
      option D: "Yes, but with a maximum retention of 1 day"
    correct Answer: "option B: No, materialized views do not support Time Travel"
    explanation: >
      CORRECT - option B: Materialized views in Snowflake do not support Time Travel. Like regular 
      views, AT/BEFORE clauses cannot be applied to materialized view references. To query historical 
      data, you must use the underlying base tables directly.
      WRONG - option A: Materialized views do not support AT/BEFORE clauses.
      WRONG - option C: Even if the base table has Time Travel, the materialized view itself 
      cannot be queried with AT/BEFORE.
      WRONG - option D: There is no version of Time Travel for materialized views, not even 1 day.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "094"
    question: "How does Snowflake handle Time Travel for a table created using CREATE TABLE AS SELECT (CTAS)?"
    their_options:
      option A: "CTAS tables inherit Time Travel from the source table"
      option B: "CTAS tables have Time Travel enabled from the moment of creation (time of the CTAS execution)"
      option C: "CTAS tables have Time Travel disabled by default"
      option D: "CTAS tables cannot have Time Travel"
    correct Answer: "option B: CTAS tables have Time Travel enabled from the moment of creation (time of the CTAS execution)"
    explanation: >
      CORRECT - option B: A table created via CTAS is a new permanent table and its Time Travel 
      history begins at creation. You cannot use Time Travel on a CTAS table to see states before 
      its creation (that data belongs to the source table's Time Travel history).
      WRONG - option A: CTAS creates an entirely new table; it does not inherit the source table's 
      Time Travel history.
      WRONG - option C: CTAS tables follow the standard retention default (1 day or inherited 
      from schema/account); they are not disabled by default.
      WRONG - option D: CTAS tables fully support Time Travel from their creation point.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer

  - Question No: "095"
    question: "What is the behavior of UNDROP when multiple versions of a dropped table with the same name exist within the retention window?"
    their_options:
      option A: "UNDROP restores all versions simultaneously creating multiple tables"
      option B: "UNDROP restores the MOST RECENTLY dropped version with that name"
      option C: "UNDROP restores the OLDEST dropped version with that name"
      option D: "UNDROP prompts the user to select which version to restore"
    correct Answer: "option B: UNDROP restores the MOST RECENTLY dropped version with that name"
    explanation: >
      CORRECT - option B: When multiple versions of a dropped table share the same name within 
      the Time Travel window, UNDROP TABLE always restores the most recently dropped version. 
      This is important to remember when trying to recover older versions.
      WRONG - option A: UNDROP restores a single version, not multiple simultaneously.
      WRONG - option C: UNDROP uses the most recently dropped version, not the oldest.
      WRONG - option D: Snowflake's UNDROP does not present an interactive selection dialog; 
      it automatically selects the most recently dropped version.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "096"
    question: "A table in a schema has DATA_RETENTION_TIME_IN_DAYS = 7 explicitly set. The account's default is 1 day, the database default is 14 days, and the schema default is 3 days. What is the effective retention for the table?"
    their_options:
      option A: "1 day (account default)"
      option B: "3 days (schema level)"
      option C: "7 days (table level)"
      option D: "14 days (database level)"
    correct Answer: "option C: 7 days (table level)"
    explanation: >
      CORRECT - option C: Snowflake's parameter inheritance follows a hierarchy where more specific 
      settings override less specific ones. The hierarchy from most to least specific is: 
      table > schema > database > account. Since the table has an explicit setting of 7 days, 
      this takes precedence over all parent-level settings.
      WRONG - option A: Account-level setting (1 day) is the lowest priority and is overridden 
      by all other explicit settings.
      WRONG - option B: Schema-level setting (3 days) is overridden by the explicit table-level setting.
      WRONG - option D: Database-level setting (14 days) is overridden by the explicit table-level setting.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, Architect

  - Question No: "097"
    question: "What privilege allows a role to run SELECT * FROM MY_TABLE AT (OFFSET => -3600)?'"
    their_options:
      option A: "SYSADMIN system privilege"
      option B: "SELECT privilege on the table"
      option C: "TIME_TRAVEL privilege on the table"
      option D: "ACCOUNTADMIN role required"
    correct Answer: "option B: SELECT privilege on the table"
    explanation: >
      CORRECT - option B: Time Travel SELECT queries (using AT/BEFORE) require the same privilege 
      as regular SELECT queries on the table. Any role with SELECT privilege on the table can use 
      Time Travel — no special Time Travel-specific privilege exists.
      WRONG - option A: SYSADMIN is a system role, not required for Time Travel reads. Standard 
      SELECT is sufficient.
      WRONG - option C: TIME_TRAVEL privilege does not exist in Snowflake's access control model.
      WRONG - option D: ACCOUNTADMIN is not needed for Time Travel queries; SELECT privilege suffices.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, DataAnalyst

  - Question No: "098"
    question: "Which of the following statements about Time Travel and Fail-safe costs are TRUE? (SELECT TWO)"
    their_options:
      option A: "Both Time Travel and Fail-safe storage are billed at the same rate as active storage"
      option B: "Fail-safe storage is billed at 2x the rate of active storage"
      option C: "Reducing DATA_RETENTION_TIME_IN_DAYS reduces both Time Travel and Fail-safe costs"
      option D: "Reducing DATA_RETENTION_TIME_IN_DAYS reduces Time Travel costs but NOT Fail-safe costs"
      option E: "Time Travel storage is free; only Fail-safe storage is billed"
    correct Answer: "option A: Both billed at same rate as active storage, option D: Reducing retention reduces Time Travel but not Fail-safe"
    explanation: >
      CORRECT - option A: Snowflake bills Time Travel and Fail-safe storage at the same rate per 
      TB as active storage — no premium for historical data storage.
      CORRECT - option D: DATA_RETENTION_TIME_IN_DAYS controls the Time Travel window. Reducing it 
      reduces Time Travel storage. Fail-safe is always 7 days for permanent tables and is NOT 
      controlled by this parameter — you cannot reduce Fail-safe duration through configuration.
      WRONG - option B: There is no 2x premium for Fail-safe storage.
      WRONG - option C: Reducing DATA_RETENTION_TIME_IN_DAYS does NOT reduce Fail-safe costs; 
      Fail-safe is always 7 days for permanent tables.
      WRONG - option E: Both Time Travel and Fail-safe storage are billed; neither is free.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "099"
    question: "A query runs for 45 minutes and during that time a table is updated. The query uses AT (TIMESTAMP => <start_time>). Does the running query see the updates made during its execution?"
    their_options:
      option A: "Yes, Snowflake queries always see the latest committed data"
      option B: "No, AT (TIMESTAMP => <start_time>) fixes the data snapshot at the query's reference point; updates during the query are invisible"
      option C: "Yes, but only if the updates are committed before the query finishes"
      option D: "It depends on the isolation level configured for the session"
    correct Answer: "option B: No, AT (TIMESTAMP => <start_time>) fixes the data snapshot at the query's reference point; updates during the query are invisible"
    explanation: >
      CORRECT - option B: When using AT (TIMESTAMP => ...), the query reads the exact snapshot of 
      data at that specified point in time. Any changes made after that timestamp (including during 
      the query's execution) are invisible to the query. Time Travel creates a fully consistent, 
      immutable point-in-time view.
      WRONG - option A: Snowflake does provide ACID isolation, but AT (TIMESTAMP) specifically 
      pins the read to a historical point, not "latest committed."
      WRONG - option C: The AT clause specifically excludes all changes after the specified timestamp, 
      regardless of commit timing.
      WRONG - option D: Snowflake uses a fixed isolation model (read committed + Time Travel 
      snapshot consistency); isolation level configuration doesn't change AT clause behavior.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "100"
    question: "What is the result of running UNDROP TABLE on a table within its Fail-safe period (after Time Travel has expired)?"
    their_options:
      option A: "The table is restored from Fail-safe automatically"
      option B: "The command fails with an error; Fail-safe data is not accessible via UNDROP"
      option C: "A warning is shown but the table is restored"
      option D: "You must contact Snowflake Support and they run UNDROP on your behalf"
    correct Answer: "option B: The command fails with an error; Fail-safe data is not accessible via UNDROP"
    explanation: >
      CORRECT - option B: UNDROP TABLE uses Time Travel data, not Fail-safe data. Once the Time 
      Travel window has expired, UNDROP cannot access the data. Fail-safe data is managed exclusively 
      by Snowflake and is not accessible through any user-executable command.
      WRONG - option A: UNDROP cannot access Fail-safe data — it's self-service for Time Travel only.
      WRONG - option C: No warning and restoration; the command simply fails.
      WRONG - option D: While Snowflake Support can potentially recover from Fail-safe, they don't 
      "run UNDROP" — they would perform a backend recovery process. Also, option D implies user-initiated 
      UNDROP still works, which is incorrect.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "101"
    question: "You want to create a Time Travel query that references both the current state and a historical state of the same table in a single SQL statement. Which approach works?"
    their_options:
      option A: "SELECT cur.*, hist.* FROM MY_TABLE cur JOIN MY_TABLE AT (OFFSET => -86400) hist ON cur.id = hist.id;"
      option B: "SELECT * FROM MY_TABLE BETWEEN CURRENT_DATE AND DATEADD(day, -1, CURRENT_DATE);"
      option C: "SELECT * FROM MY_TABLE DUAL_TIME_TRAVEL(current, -86400);"
      option D: "This is not possible in a single SQL statement"
    correct Answer: "option A: SELECT cur.*, hist.* FROM MY_TABLE cur JOIN MY_TABLE AT (OFFSET => -86400) hist ON cur.id = hist.id;"
    explanation: >
      CORRECT - option A: Snowflake allows self-joins where one reference uses the current state 
      and another uses a Time Travel state. By aliasing the table references differently, you can 
      compare current vs. historical data in a single query — a powerful auditing capability.
      WRONG - option B: BETWEEN with dates is a WHERE filter, not a Time Travel mechanism. It 
      filters rows based on a column value, not a point-in-time table snapshot.
      WRONG - option C: DUAL_TIME_TRAVEL is not a Snowflake function or syntax.
      WRONG - option D: This is definitely possible — option A demonstrates exactly how.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "102"
    question: "Which Snowflake feature, when combined with Time Travel, provides a complete data versioning and audit history solution?"
    their_options:
      option A: "Snowflake Streams"
      option B: "Access History in ACCOUNT_USAGE"
      option C: "Dynamic Tables"
      option D: "Snowflake Streams + Time Travel together for CDC and point-in-time recovery"
    correct Answer: "option D: Snowflake Streams + Time Travel together for CDC and point-in-time recovery"
    explanation: >
      CORRECT - option D: Streams provide forward-looking CDC (tracking ongoing changes) while 
      Time Travel provides backward-looking point-in-time recovery. Together they offer comprehensive 
      data versioning: Streams for near-real-time change capture, Time Travel for historical auditing 
      and recovery.
      WRONG - option A: Streams alone only provide forward CDC; they cannot recover historical states.
      WRONG - option B: Access History tracks who accessed data (read/write audit), not data 
      versioning or content recovery.
      WRONG - option C: Dynamic Tables are auto-refreshing materialized views for analytics; 
      they don't provide change tracking or historical recovery.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "103"
    question: "A Snowflake table has DATA_RETENTION_TIME_IN_DAYS = 7. On Monday, a row is deleted. On Tuesday, Time Travel is checked and the row is visible. On the following Tuesday (8 days after deletion), an attempt is made to query the deleted row. What is the result?"
    their_options:
      option A: "The row is still visible because Fail-safe extends access to 14 days"
      option B: "The row is NOT visible; Time Travel has expired (7-day window passed)"
      option C: "The row is visible for 7 more days in Fail-safe"
      option D: "The result depends on whether the table has been queried since the deletion"
    correct Answer: "option B: The row is NOT visible; Time Travel has expired (7-day window passed)"
    explanation: >
      CORRECT - option B: The Time Travel window is 7 days. After 7 days, the deleted row's 
      historical data is no longer accessible via AT/BEFORE queries. Fail-safe holds the data 
      for another 7 days but is NOT user-accessible.
      WRONG - option A: Fail-safe does NOT extend user-accessible Time Travel; it's a 
      Snowflake-internal safety net, not accessible via AT/BEFORE queries.
      WRONG - option C: Fail-safe data cannot be queried by users. The data exists in Fail-safe 
      but is only accessible by Snowflake Support for disaster recovery.
      WRONG - option D: Whether the table has been queried is irrelevant; Time Travel expiration 
      is purely time-based.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "104"
    question: "What is the CHANGES(INFORMATION => APPEND_ONLY) modifier used for in Time Travel queries?"
    their_options:
      option A: "Returns only rows that were appended (inserted) and not subsequently updated or deleted"
      option B: "Returns only rows that were added since the last stream consumption"
      option C: "Limits Time Travel to append-only tables"
      option D: "Returns changes including updates treated as append operations"
    correct Answer: "option A: Returns only rows that were appended (inserted) and not subsequently updated or deleted"
    explanation: >
      CORRECT - option A: CHANGES(INFORMATION => APPEND_ONLY) returns only rows with 
      METADATA$ACTION = 'INSERT' that haven't been subsequently modified or deleted. It's useful 
      for identifying net-new rows added to a table in a given period. It's the equivalent of 
      an append-only stream.
      WRONG - option B: That describes how Snowflake Streams work, not the CHANGES clause with 
      APPEND_ONLY. The CHANGES clause doesn't depend on stream consumption state.
      WRONG - option C: APPEND_ONLY is a query modifier, not a table type restriction.
      WRONG - option D: Updates are NOT treated as appends; APPEND_ONLY explicitly excludes 
      rows that have been updated or deleted.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "105"
    question: "Scenario: A table SENSOR_DATA receives 50 million inserts per hour but no updates or deletes. The team wants 24-hour Time Travel with minimal storage overhead. What is the best table configuration?"
    their_options:
      option A: "Permanent table with DATA_RETENTION_TIME_IN_DAYS = 1"
      option B: "Transient table with DATA_RETENTION_TIME_IN_DAYS = 1"
      option C: "Temporary table with DATA_RETENTION_TIME_IN_DAYS = 1"
      option D: "Permanent table with DATA_RETENTION_TIME_IN_DAYS = 90"
    correct Answer: "option B: Transient table with DATA_RETENTION_TIME_IN_DAYS = 1"
    explanation: >
      CORRECT - option B: For an insert-only table, Time Travel overhead is already minimal (no 
      before-images to store). Using a transient table with 1-day retention provides the 24-hour 
      recovery window while eliminating the 7-day Fail-safe storage cost — optimal for a high-volume 
      insert-only workload where Fail-safe's additional protection may not be necessary.
      WRONG - option A: Permanent table with 1-day retention works but incurs Fail-safe storage 
      costs unnecessarily for this high-volume insert-only use case.
      WRONG - option C: A temporary table is session-scoped and would be lost when the session ends, 
      making it inappropriate for a persistent sensor data table.
      WRONG - option D: 90 days of retention on a 50M inserts/hour table would create enormous 
      storage costs and is far more than the 24-hour requirement.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "106"
    question: "Which of the following correctly describes the relationship between Snowflake micro-partitions and Time Travel?"
    their_options:
      option A: "Time Travel copies entire micro-partitions when any row is changed"
      option B: "Time Travel tracks row-level changes within micro-partitions using delta encoding"
      option C: "When a micro-partition is modified, the old version is retained in Time Travel; new data writes to new partitions"
      option D: "Time Travel creates a separate shadow database for each historical version"
    correct Answer: "option C: When a micro-partition is modified, the old version is retained in Time Travel; new data writes to new partitions"
    explanation: >
      CORRECT - option C: Snowflake's columnar micro-partition architecture is immutable. When data 
      is updated or deleted, Snowflake does NOT modify existing micro-partitions. Instead, it creates 
      new micro-partitions with the changed data and retains the old micro-partitions for Time Travel. 
      This immutable architecture naturally enables efficient Time Travel.
      WRONG - option A: Snowflake doesn't copy entire micro-partitions on change; old partitions 
      are simply retained in their original state (no copy needed).
      WRONG - option B: Snowflake uses immutable micro-partitions, not delta encoding within 
      micro-partitions.
      WRONG - option D: Time Travel doesn't create shadow databases; it leverages the existing 
      immutable micro-partition storage model.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect, DataEngineer

  - Question No: "107"
    question: "Can the BEFORE clause in Time Travel accept a TIMESTAMP value?"
    their_options:
      option A: "No, BEFORE only accepts STATEMENT (query ID)"
      option B: "Yes, BEFORE accepts TIMESTAMP, OFFSET, and STATEMENT just like AT"
      option C: "Yes, but only TIMESTAMP and OFFSET; STATEMENT is only for AT"
      option D: "No, BEFORE only accepts OFFSET values"
    correct Answer: "option B: Yes, BEFORE accepts TIMESTAMP, OFFSET, and STATEMENT just like AT"
    explanation: >
      CORRECT - option B: The BEFORE clause supports all three sub-clauses: 
      BEFORE (TIMESTAMP => ...), BEFORE (OFFSET => ...), and BEFORE (STATEMENT => ...). 
      The functionality is symmetric with AT, just exclusive rather than inclusive.
      WRONG - option A: BEFORE is not limited to STATEMENT; it supports all three sub-clause types.
      WRONG - option C: STATEMENT is valid with both AT and BEFORE.
      WRONG - option D: BEFORE is not limited to OFFSET; it supports all three sub-clause types.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "108"
    question: "What is the correct way to drop a table and completely bypass Time Travel (immediate permanent deletion)?"
    their_options:
      option A: "DROP TABLE MY_TABLE PURGE;"
      option B: "DROP TABLE MY_TABLE CASCADE;"
      option C: "First ALTER TABLE SET DATA_RETENTION_TIME_IN_DAYS = 0, then DROP TABLE MY_TABLE;"
      option D: "DROP TABLE MY_TABLE IMMEDIATELY;"
    correct Answer: "option C: First ALTER TABLE SET DATA_RETENTION_TIME_IN_DAYS = 0, then DROP TABLE MY_TABLE;"
    explanation: >
      CORRECT - option C: To bypass Time Travel on drop, set DATA_RETENTION_TIME_IN_DAYS = 0 
      first (which purges existing Time Travel data), then DROP the table. With retention = 0, 
      there is no Time Travel window, so the drop is effectively permanent.
      WRONG - option A: DROP TABLE MY_TABLE PURGE is not valid Snowflake syntax. Some databases 
      support PURGE (like Oracle), but Snowflake does not.
      WRONG - option B: DROP TABLE CASCADE drops the table and dependent objects (views, etc.), 
      but the table still enters the Time Travel retention period.
      WRONG - option D: DROP TABLE IMMEDIATELY is not valid Snowflake syntax.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "109"
    question: "Which SQL retrieves a list of all queries that ran against a table in the last 24 hours to help identify a bad DML operation?"
    their_options:
      option A: "SELECT * FROM INFORMATION_SCHEMA.TABLE_HISTORY WHERE TABLE_NAME = 'MY_TABLE';"
      option B: "SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY WHERE QUERY_TEXT ILIKE '%MY_TABLE%' AND START_TIME >= DATEADD(hour, -24, CURRENT_TIMESTAMP());"
      option C: "SHOW QUERY HISTORY FOR TABLE MY_TABLE LAST 24 HOURS;"
      option D: "SELECT * FROM INFORMATION_SCHEMA.QUERY_HISTORY() WHERE TABLE_NAME = 'MY_TABLE';"
    correct Answer: "option B: SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY WHERE QUERY_TEXT ILIKE '%MY_TABLE%' AND START_TIME >= DATEADD(hour, -24, CURRENT_TIMESTAMP());"
    explanation: >
      CORRECT - option B: SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY (or INFORMATION_SCHEMA.QUERY_HISTORY()) 
      stores query execution details. Filtering on QUERY_TEXT for the table name and time range 
      allows identification of recent DML operations on the table.
      WRONG - option A: INFORMATION_SCHEMA.TABLE_HISTORY is not a standard Snowflake view.
      WRONG - option C: SHOW QUERY HISTORY FOR TABLE is not valid Snowflake syntax.
      WRONG - option D: INFORMATION_SCHEMA.QUERY_HISTORY() is a table function but doesn't support 
      TABLE_NAME as a direct parameter filter in this way.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "110"
    question: "A developer needs to compare a table's data between two different historical points (e.g., yesterday vs. last week). Which Time Travel query achieves this?"
    their_options:
      option A: >
        "SELECT * FROM MY_TABLE AT (OFFSET => -86400)
        MINUS
        SELECT * FROM MY_TABLE AT (OFFSET => -604800);"
      option B: "SELECT * FROM MY_TABLE BETWEEN (OFFSET => -86400) AND (OFFSET => -604800);"
      option C: "SELECT * FROM MY_TABLE AT (OFFSET => -86400) COMPARE TO MY_TABLE AT (OFFSET => -604800);"
      option D: "SELECT * FROM MY_TABLE AT (OFFSET => -86400) DIFF AT (OFFSET => -604800);"
    correct Answer: "option A: SELECT * FROM MY_TABLE AT (OFFSET => -86400) MINUS SELECT * FROM MY_TABLE AT (OFFSET => -604800);"
    explanation: >
      CORRECT - option A: You can use set operations like MINUS (or EXCEPT), UNION, and INTERSECT 
      on Time Travel queries. This approach compares yesterday's state vs. last week's state 
      by computing rows present yesterday but not last week. -86400 = 1 day ago, -604800 = 7 days ago.
      WRONG - option B: BETWEEN is not a valid Time Travel syntax for comparing two historical points.
      WRONG - option C: COMPARE TO is not valid Snowflake SQL syntax.
      WRONG - option D: DIFF is not a valid Snowflake SQL clause.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "111"
    question: "What is the TIME_TRAVEL_BYTES column in SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS?"
    their_options:
      option A: "The total storage used by the active table data"
      option B: "The storage consumed by historical versions retained for Time Travel"
      option C: "The storage reserved but not yet used for Time Travel"
      option D: "The maximum Time Travel storage allowed for the table"
    correct Answer: "option B: The storage consumed by historical versions retained for Time Travel"
    explanation: >
      CORRECT - option B: TIME_TRAVEL_BYTES represents the actual bytes of historical (before-image) 
      data retained for Time Travel purposes. This is the billable Time Travel storage overhead 
      beyond the table's active storage (ACTIVE_BYTES).
      WRONG - option A: ACTIVE_BYTES tracks the current live table data size, not Time Travel.
      WRONG - option C: Time Travel storage is not "reserved" — it grows dynamically based on 
      DML operations and the retention window.
      WRONG - option D: There is no maximum Time Travel storage cap in Snowflake; it grows 
      proportionally with data changes and the retention period.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "112"
    question: "Scenario: A team accidentally runs DELETE FROM ORDERS WHERE 1=1 (deletes all rows). They discover this 3 days later. The table has 7-day retention. How should they recover?"
    their_options:
      option A: "UNDROP TABLE ORDERS;"
      option B: "INSERT INTO ORDERS SELECT * FROM ORDERS BEFORE (STATEMENT => '<delete_query_id>');"
      option C: "INSERT INTO ORDERS SELECT * FROM ORDERS AT (OFFSET => -259200);"
      option D: "Both B and C are valid approaches"
    correct Answer: "option D: Both B and C are valid approaches"
    explanation: >
      CORRECT - option D: Both approaches work:
      Option B uses the exact query ID of the DELETE statement and BEFORE to see the pre-delete 
      state — the most precise approach.
      Option C uses an offset of 3 days (259200 seconds) to see the data from 3 days ago, which 
      is before the delete occurred — also valid if the query ID is not known.
      WRONG - option A: The table was NOT dropped; rows were deleted. UNDROP TABLE only works for 
      dropped objects, not for tables where rows were deleted via DML.
      WRONG - option B alone: Valid but not the only answer.
      WRONG - option C alone: Valid but not the only answer.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "113"
    question: "How does Time Travel handle NULL values in columns? Are rows with NULL values included in Time Travel queries?"
    their_options:
      option A: "Rows with NULL values are excluded from Time Travel to save storage"
      option B: "Rows with NULL values are treated exactly like any other row — fully included in Time Travel"
      option C: "NULL values are replaced with default values in Time Travel queries"
      option D: "Time Travel cannot restore NULL values; they appear as empty strings"
    correct Answer: "option B: Rows with NULL values are treated exactly like any other row — fully included in Time Travel"
    explanation: >
      CORRECT - option B: Time Travel stores complete row data at the micro-partition level, 
      including NULL values. NULL values are preserved exactly as they were when the row was 
      originally stored. There is no special handling of NULLs in Time Travel.
      WRONG - option A: No rows are excluded from Time Travel based on NULL values.
      WRONG - option C: NULL values are preserved as NULLs in Time Travel; they are not replaced.
      WRONG - option D: Time Travel fully preserves NULL values; they are not converted to empty strings.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "114"
    question: "Which of the following is the CORRECT understanding of 'Time Travel data is stored in Snowflake cloud storage'?"
    their_options:
      option A: "Time Travel data is stored in a separate dedicated cold storage tier"
      option B: "Time Travel data is stored in the same cloud storage layer as active data but retained beyond normal DML lifecycle"
      option C: "Time Travel data is stored in the virtual warehouse's local SSD cache"
      option D: "Time Travel data is stored in a separate Snowflake-managed S3 bucket per account"
    correct Answer: "option B: Time Travel data is stored in the same cloud storage layer as active data but retained beyond normal DML lifecycle"
    explanation: >
      CORRECT - option B: Snowflake stores all data — active, Time Travel, and Fail-safe — in the 
      same cloud storage layer (S3, Azure Blob, or GCS depending on the Snowflake deployment). 
      Time Travel data is simply micro-partitions that are retained instead of being purged after DML.
      WRONG - option A: There is no separate cold storage tier for Time Travel in Snowflake. 
      Snowflake manages storage internally.
      WRONG - option C: Virtual warehouse SSD/memory cache is for query performance (hot caching), 
      not persistent storage of Time Travel data.
      WRONG - option D: Snowflake doesn't provision separate S3 buckets per account for Time Travel. 
      All storage is within Snowflake's managed storage layer.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect, DataEngineer

  - Question No: "115"
    question: "When would you choose AT (TIMESTAMP => ...) over AT (OFFSET => ...) for a Time Travel query?"
    their_options:
      option A: "When you need higher precision — TIMESTAMP is more precise than OFFSET"
      option B: "When you know a specific date/time of interest rather than a relative duration"
      option C: "When querying tables that were created more than 7 days ago"
      option D: "TIMESTAMP and OFFSET are interchangeable — choose either"
    correct Answer: "option B: When you know a specific date/time of interest rather than a relative duration"
    explanation: >
      CORRECT - option B: Use TIMESTAMP when you have a specific business-meaningful point in time 
      (e.g., 'end of quarter', 'before the batch job on 2024-03-15 at 02:00 UTC'). Use OFFSET 
      when referencing relative time (e.g., '1 hour ago', 'yesterday'). Both achieve the same 
      goal but TIMESTAMP is more explicit for known absolute times.
      WRONG - option A: Both provide microsecond precision for time-based queries; neither is 
      inherently more precise.
      WRONG - option C: Table age does not affect the choice between TIMESTAMP and OFFSET.
      WRONG - option D: While functionally both reference a historical point in time, TIMESTAMP 
      is for absolute times and OFFSET is for relative — they are conceptually different choices.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "116"
    question: "What is the impact on Time Travel if an account is suspended or a virtual warehouse is suspended?"
    their_options:
      option A: "Time Travel data is paused and retention timer stops"
      option B: "Time Travel data is purged immediately when resources are suspended"
      option C: "Time Travel continues accumulating and the retention timer runs independently of compute state"
      option D: "Time Travel is only active when a warehouse is running"
    correct Answer: "option C: Time Travel continues accumulating and the retention timer runs independently of compute state"
    explanation: >
      CORRECT - option C: Time Travel is a storage-layer feature and operates completely independently 
      of compute (virtual warehouses). The retention timer counts real calendar time regardless of 
      whether warehouses or the account is suspended. Storage costs continue to accrue during suspension.
      WRONG - option A: The Time Travel retention timer never pauses; it counts continuous wall-clock time.
      WRONG - option B: Time Travel data is not purged during suspension; it expires naturally 
      at the end of the retention period.
      WRONG - option D: Time Travel operates at the storage layer, not the compute layer. No 
      warehouse is needed for Time Travel data to be retained.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "117"
    question: "A company is considering using Snowflake's Time Travel for their backup strategy, replacing traditional external backups. What is a critical limitation they should consider?"
    their_options:
      option A: "Time Travel cannot be used with Enterprise Edition"
      option B: "Time Travel data is stored in the same cloud account as production data — a region-wide outage could affect both"
      option C: "Time Travel cannot restore data deleted more than 1 hour ago"
      option D: "Time Travel does not support databases larger than 1 TB"
    correct Answer: "option B: Time Travel data is stored in the same cloud account as production data — a region-wide outage could affect both"
    explanation: >
      CORRECT - option B: Time Travel data resides in the same Snowflake storage infrastructure as 
      active data. A major disaster affecting the Snowflake deployment region (cloud outage, account 
      compromise, or accidental account deletion) could impact both active and Time Travel data. 
      Critical compliance requirements may still necessitate external backups.
      WRONG - option A: Time Travel is fully available and highly functional in Enterprise Edition.
      WRONG - option C: Time Travel can restore data deleted within the entire retention window 
      (up to 90 days with Enterprise Edition).
      WRONG - option D: There is no database size limit for Time Travel capability.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect

  - Question No: "118"
    question: "Which parameter at the ACCOUNT level sets the default Time Travel retention for all new tables that don't have explicit settings?"
    their_options:
      option A: "DEFAULT_DATA_RETENTION_TIME"
      option B: "DATA_RETENTION_TIME_IN_DAYS"
      option C: "ACCOUNT_RETENTION_DAYS"
      option D: "DEFAULT_TIME_TRAVEL_DAYS"
    correct Answer: "option B: DATA_RETENTION_TIME_IN_DAYS"
    explanation: >
      CORRECT - option B: The same parameter name DATA_RETENTION_TIME_IN_DAYS is used at all 
      levels — account, database, schema, and table. When set at the account level 
      (ALTER ACCOUNT SET DATA_RETENTION_TIME_IN_DAYS = N), it becomes the default for all 
      new objects that don't have explicit settings.
      WRONG - option A: DEFAULT_DATA_RETENTION_TIME is not a valid Snowflake parameter name.
      WRONG - option C: ACCOUNT_RETENTION_DAYS is not a valid Snowflake parameter name.
      WRONG - option D: DEFAULT_TIME_TRAVEL_DAYS is not a valid Snowflake parameter name.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, Architect

  - Question No: "119"
    question: "Which of the following Time Travel queries would retrieve data from a point in time EXACTLY at the start of a specific DML statement's execution?"
    their_options:
      option A: "SELECT * FROM T BEFORE (STATEMENT => '<query_id>');"
      option B: "SELECT * FROM T AT (STATEMENT => '<query_id>');"
      option C: "SELECT * FROM T AT (TIMESTAMP => <statement_start_time>);"
      option D: "Both A and C achieve the same result"
    correct Answer: "option A: SELECT * FROM T BEFORE (STATEMENT => '<query_id>');"
    explanation: >
      CORRECT - option A: BEFORE (STATEMENT => '<query_id>') retrieves data just BEFORE the 
      statement began, which is semantically equivalent to the start of that statement's execution 
      — the pre-DML state.
      WRONG - option B: AT (STATEMENT => '<query_id>') returns data at the END of the statement 
      execution — i.e., AFTER the DML changes have been applied.
      WRONG - option C: AT (TIMESTAMP => <statement_start_time>) could approximate the pre-DML 
      state if the timestamp is before the statement, but it's less precise than BEFORE (STATEMENT).
      WRONG - option D: They don't achieve the same result — option A uses BEFORE with query_id, 
      option C uses AT with a timestamp, which has different precision.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "120"
    question: "A table TRANSACTIONS was last updated 2 hours ago. The table has DATA_RETENTION_TIME_IN_DAYS = 1. A developer wants to use AT (OFFSET => -7200). Is this valid?"
    their_options:
      option A: "Yes — 7200 seconds = 2 hours, well within the 1-day (86400-second) retention window"
      option B: "No — offset queries are only valid for values greater than 86400 seconds"
      option C: "No — AT (OFFSET) can only be used with Enterprise Edition"
      option D: "Yes, but only if no new DML has occurred since the 2-hour mark"
    correct Answer: "option A: Yes — 7200 seconds = 2 hours, well within the 1-day (86400-second) retention window"
    explanation: >
      CORRECT - option A: 7200 seconds = 2 hours, and the retention window is 1 day (86400 seconds). 
      Since 2 hours is well within the 24-hour retention period, this Time Travel query is valid.
      WRONG - option B: There is no minimum offset requirement. Any negative value within the 
      retention window is valid.
      WRONG - option C: AT (OFFSET) is available in all Snowflake editions, not just Enterprise.
      WRONG - option D: DML activity after the 2-hour mark doesn't invalidate a Time Travel query; 
      the AT clause reads the state at that specific point regardless of subsequent changes.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "121"
    question: "What is the maximum DATA_RETENTION_TIME_IN_DAYS value for a Temporary table?"
    their_options:
      option A: "90 days"
      option B: "7 days"
      option C: "1 day"
      option D: "Unlimited (session-scoped)"
    correct Answer: "option C: 1 day"
    explanation: >
      CORRECT - option C: Like transient tables, temporary tables support a maximum of 1 day 
      (0 or 1) for Time Travel retention. They also have no Fail-safe period.
      WRONG - option A: 90 days is only for permanent tables in Enterprise Edition.
      WRONG - option B: 7 days is not supported for temporary tables.
      WRONG - option D: The session scope refers to the table's lifetime, not its Time Travel 
      retention. Retention is still constrained to 0 or 1 day.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "122"
    question: "When cloning a schema with Time Travel, which of the following is TRUE?"
    their_options:
      option A: "CREATE SCHEMA NEW_SCHEMA CLONE OLD_SCHEMA AT (OFFSET => -3600); — this is valid syntax"
      option B: "Schema cloning with Time Travel requires SYSADMIN role"
      option C: "Schema cloning with AT/BEFORE is not supported; only table-level cloning supports Time Travel"
      option D: "Schema clones do not inherit table retention settings"
    correct Answer: "option A: CREATE SCHEMA NEW_SCHEMA CLONE OLD_SCHEMA AT (OFFSET => -3600); — this is valid syntax"
    explanation: >
      CORRECT - option A: Time Travel-based cloning is supported at the schema level. 
      CREATE SCHEMA <new> CLONE <source> AT/BEFORE (...) creates a zero-copy clone of the entire 
      schema (including all its tables) at the specified historical point.
      WRONG - option B: SYSADMIN is not a specific requirement; CREATE SCHEMA privilege on the 
      target database is what's needed.
      WRONG - option C: Time Travel cloning is supported at table, schema, and database levels.
      WRONG - option D: When cloning a schema, tables within the clone inherit the same retention 
      settings as the source tables (unless overridden).
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "123"
    question: "A table CUSTOMER_ORDERS is renamed using ALTER TABLE CUSTOMER_ORDERS RENAME TO CUST_ORDERS. Does Time Travel history transfer to the new name?"
    their_options:
      option A: "No — renaming creates a new table and drops the old one, losing Time Travel history"
      option B: "Yes — renaming is a metadata operation; all Time Travel history is preserved under the new name"
      option C: "Yes — but only the last 24 hours of history transfers"
      option D: "No — you must use CREATE TABLE CUST_ORDERS CLONE CUSTOMER_ORDERS to preserve history"
    correct Answer: "option B: Yes — renaming is a metadata operation; all Time Travel history is preserved under the new name"
    explanation: >
      CORRECT - option B: ALTER TABLE RENAME is a pure metadata operation that changes the table's 
      name reference in the metadata layer. The underlying micro-partitions (including Time Travel 
      history) are untouched and remain fully accessible under the new name.
      WRONG - option A: Renaming does NOT drop and recreate the table; it's a lightweight metadata 
      change with no effect on data or Time Travel.
      WRONG - option C: All history within the retention window is preserved, not just 24 hours.
      WRONG - option D: Cloning would create a NEW table without the original's full Time Travel 
      history — and is unnecessary since a simple rename preserves everything.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer

  - Question No: "124"
    question: "Which query correctly retrieves the query ID of the last DML operation on a table ORDERS from INFORMATION_SCHEMA?"
    their_options:
      option A: "SELECT QUERY_ID FROM INFORMATION_SCHEMA.QUERY_HISTORY() WHERE QUERY_TEXT ILIKE '%ORDERS%' ORDER BY START_TIME DESC LIMIT 1;"
      option B: "SELECT LAST_DML_QUERY_ID FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'ORDERS';"
      option C: "SHOW TABLES LIKE 'ORDERS'; -- check the LAST_DML_QUERY column"
      option D: "SELECT QUERY_ID FROM SNOWFLAKE.ACCOUNT_USAGE.DML_HISTORY WHERE TABLE_NAME = 'ORDERS';"
    correct Answer: "option A: SELECT QUERY_ID FROM INFORMATION_SCHEMA.QUERY_HISTORY() WHERE QUERY_TEXT ILIKE '%ORDERS%' ORDER BY START_TIME DESC LIMIT 1;"
    explanation: >
      CORRECT - option A: INFORMATION_SCHEMA.QUERY_HISTORY() is a table function that returns 
      recent query history. Filtering on table name in the query text and ordering by start time 
      descending to get the most recent query is the correct approach.
      WRONG - option B: INFORMATION_SCHEMA.TABLES does not have a LAST_DML_QUERY_ID column; 
      it shows table metadata like retention_time and last_altered but not DML query IDs.
      WRONG - option C: SHOW TABLES does not include a LAST_DML_QUERY column in its output.
      WRONG - option D: SNOWFLAKE.ACCOUNT_USAGE.DML_HISTORY is not a real view in Snowflake; 
      query history is accessed via QUERY_HISTORY.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "125"
    question: "What ACCOUNT_USAGE view would you query to monitor Time Travel storage costs across your entire Snowflake account historically (with up to 365 days of history)?"
    their_options:
      option A: "SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS"
      option B: "SNOWFLAKE.ACCOUNT_USAGE.STORAGE_USAGE"
      option C: "SNOWFLAKE.ACCOUNT_USAGE.METERING_HISTORY"
      option D: "SNOWFLAKE.ACCOUNT_USAGE.TIME_TRAVEL_HISTORY"
    correct Answer: "option A: SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS"
    explanation: >
      CORRECT - option A: TABLE_STORAGE_METRICS in ACCOUNT_USAGE contains the TIME_TRAVEL_BYTES 
      column per table and retains data for up to 365 days. This is the primary view for 
      monitoring and auditing Time Travel storage consumption at the table level.
      WRONG - option B: STORAGE_USAGE provides account-level aggregate storage (DATABASE_BYTES, 
      STAGE_BYTES, FAILSAFE_BYTES) but does NOT break down Time Travel storage specifically.
      WRONG - option C: METERING_HISTORY tracks compute credit consumption, not storage costs.
      WRONG - option D: SNOWFLAKE.ACCOUNT_USAGE.TIME_TRAVEL_HISTORY does not exist as a Snowflake view.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "126"
    question: "A developer wants to see the exact SQL used to delete data from a table in the last 7 days. They have the query ID. What is the easiest way to retrieve the full SQL text?"
    their_options:
      option A: "SELECT QUERY_TEXT FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY WHERE QUERY_ID = '<id>';"
      option B: "SELECT SQL_TEXT FROM INFORMATION_SCHEMA.TIME_TRAVEL_HISTORY WHERE QUERY_ID = '<id>';"
      option C: "SHOW QUERY '<id>';"
      option D: "DESCRIBE QUERY '<id>';"
    correct Answer: "option A: SELECT QUERY_TEXT FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY WHERE QUERY_ID = '<id>';"
    explanation: >
      CORRECT - option A: SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY contains a QUERY_TEXT column 
      with the full SQL of every executed statement. Filtering by QUERY_ID retrieves the exact 
      SQL text of any historical query.
      WRONG - option B: INFORMATION_SCHEMA.TIME_TRAVEL_HISTORY does not exist in Snowflake.
      WRONG - option C: SHOW QUERY is not valid Snowflake syntax.
      WRONG - option D: DESCRIBE QUERY is not valid Snowflake syntax for retrieving SQL text.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "127"
    question: "Scenario: A data engineer writes a Snowflake task that runs every hour and uses CHANGES(INFORMATION => APPEND_ONLY). If the task fails for 2 days (retention = 1 day), what happens when the task resumes?"
    their_options:
      option A: "The task catches up on 2 days of missed changes automatically"
      option B: "If the underlying table's stream or time travel data has expired, the CHANGES query fails for data older than 1 day"
      option C: "The CHANGES clause automatically extends retention when tasks fail"
      option D: "The task simply processes the current state of the table"
    correct Answer: "option B: If the underlying table's stream or time travel data has expired, the CHANGES query fails for data older than 1 day"
    explanation: >
      CORRECT - option B: CHANGES(INFORMATION => ...) relies on the Time Travel retention window. 
      If the table has 1-day retention and the task fails for 2 days, the Time Travel data for the 
      oldest changes has expired. The query will either fail or return incomplete results for the 
      period beyond the retention window.
      WRONG - option A: CHANGES does not automatically catch up across expired Time Travel data.
      WRONG - option C: Time Travel retention does not automatically extend due to task failures; 
      this is a critical pipeline design consideration.
      WRONG - option D: CHANGES is not equivalent to a full table scan of current state; it 
      specifically queries the change delta.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "128"
    question: "Which of the following statements about UNDROP DATABASE is TRUE?"
    their_options:
      option A: "UNDROP DATABASE restores only the database container but not its schemas or tables"
      option B: "UNDROP DATABASE restores the database AND all its schemas and tables to the state at the time of the drop"
      option C: "UNDROP DATABASE requires ACCOUNTADMIN role"
      option D: "UNDROP DATABASE only works if the database was dropped within the last 24 hours"
    correct Answer: "option B: UNDROP DATABASE restores the database AND all its schemas and tables to the state at the time of the drop"
    explanation: >
      CORRECT - option B: UNDROP DATABASE performs a cascading restore of the entire database 
      hierarchy — the database, all its schemas, and all tables within those schemas — restoring 
      them to their state at the time of the DROP DATABASE command.
      WRONG - option A: UNDROP DATABASE is not limited to just the container; it restores the 
      entire hierarchy.
      WRONG - option C: UNDROP DATABASE requires CREATE DATABASE privilege on the account (or 
      SYSADMIN with that privilege), not specifically ACCOUNTADMIN.
      WRONG - option D: UNDROP DATABASE works within the DATA_RETENTION_TIME_IN_DAYS window, 
      which can be up to 90 days in Enterprise Edition, not just 24 hours.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "129"
    question: "A data analyst queries a table using Time Travel: SELECT count(*) FROM SALES AT (OFFSET => -86400). The table was created 10 hours ago. What happens?"
    their_options:
      option A: "Returns 0 because the table had no data 24 hours ago"
      option B: "Returns an error because the table didn't exist 24 hours ago"
      option C: "Returns the current row count"
      option D: "Returns NULL"
    correct Answer: "option B: Returns an error because the table didn't exist 24 hours ago"
    explanation: >
      CORRECT - option B: You cannot query a table at a point in time before it was created. 
      The table was created 10 hours ago, so querying with AT (OFFSET => -86400) (24 hours ago) 
      requests a historical state that predates the table's existence — Snowflake returns an error.
      WRONG - option A: Returning 0 would imply the table existed but was empty; in reality, 
      it didn't exist.
      WRONG - option C: AT (OFFSET => -86400) explicitly queries 24 hours ago, not the current state.
      WRONG - option D: NULL is not returned for Time Travel queries on non-existent historical states.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "130"
    question: "What does the METADATA$ISUPDATE column indicate in a CHANGES clause query?"
    their_options:
      option A: "TRUE if the row was inserted as part of an UPDATE operation (new value side)"
      option B: "TRUE if the row is the result of a MERGE statement"
      option C: "TRUE if the row has been updated since the table was created"
      option D: "TRUE for any row that has a non-NULL value"
    correct Answer: "option A: TRUE if the row was inserted as part of an UPDATE operation (new value side)"
    explanation: >
      CORRECT - option A: In CHANGES(INFORMATION => DEFAULT), UPDATE operations appear as a 
      DELETE (old value) and INSERT (new value) pair. METADATA$ISUPDATE = TRUE on the INSERT 
      row indicates it is the new value from an UPDATE, not a pure insert.
      WRONG - option B: While MERGE can produce updates, METADATA$ISUPDATE is specifically tied 
      to the paired DELETE/INSERT representation of updates, not the statement type.
      WRONG - option C: METADATA$ISUPDATE applies specifically to rows in CHANGES output, not 
      to the table's general update history.
      WRONG - option D: METADATA$ISUPDATE is a boolean tied to UPDATE semantics, not NULL status.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "131"
    question: "Which statement about Time Travel and Snowflake's tri-secret security (customer-managed keys) is TRUE?"
    their_options:
      option A: "Time Travel data is not protected by customer-managed encryption keys"
      option B: "Time Travel data is encrypted with the same keys as active data, including customer-managed keys in Business Critical Edition"
      option C: "Time Travel requires separate key management from active data"
      option D: "Customer-managed keys are only available for active data, not Time Travel"
    correct Answer: "option B: Time Travel data is encrypted with the same keys as active data, including customer-managed keys in Business Critical Edition"
    explanation: >
      CORRECT - option B: Snowflake's encryption applies uniformly to all data in the cloud storage 
      layer, including Time Travel and Fail-safe data. In Business Critical Edition with Tri-Secret 
      Secure (customer-managed keys via AWS KMS, Azure Key Vault, etc.), Time Travel data is also 
      protected by the same customer-controlled encryption.
      WRONG - option A: Time Travel data is fully encrypted, including with customer-managed keys.
      WRONG - option C: No separate key management is needed; the same encryption scheme covers 
      all data tiers.
      WRONG - option D: Customer-managed keys protect all data in Snowflake's storage layer, 
      including Time Travel.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect

  - Question No: "132"
    question: "How can you verify the effective retention period of a table that inherits its setting from the schema level?"
    their_options:
      option A: "The table always shows 0 if it inherits from schema level"
      option B: "SHOW TABLES LIKE '<table_name>' — the RETENTION_TIME column shows the effective (resolved) value"
      option C: "DESCRIBE TABLE shows the inherited retention period"
      option D: "You must check the schema's setting separately; tables don't show inherited values"
    correct Answer: "option B: SHOW TABLES LIKE '<table_name>' — the RETENTION_TIME column shows the effective (resolved) value"
    explanation: >
      CORRECT - option B: SHOW TABLES displays the RETENTION_TIME column with the EFFECTIVE/RESOLVED 
      value for the table. Even if the table inherits its setting from the schema, the RETENTION_TIME 
      column shows the actual resolved value (not just a reference to the schema).
      WRONG - option A: A table doesn't show 0 when it inherits; it shows the actual effective 
      inherited value.
      WRONG - option C: DESCRIBE TABLE shows column definitions, not table parameter settings.
      WRONG - option D: SHOW TABLES does resolve and display the effective value, including inherited ones.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "133"
    question: "A developer wants to use Time Travel to audit ALL changes made to a table in the past 6 hours without knowing specific query IDs. What is the most appropriate approach?"
    their_options:
      option A: "SELECT * FROM MY_TABLE AT (OFFSET => -21600);"
      option B: >
        "SELECT * FROM MY_TABLE CHANGES(INFORMATION => DEFAULT)
        AT (OFFSET => -21600);"
      option C: "SHOW TABLE CHANGES FOR MY_TABLE LAST 6 HOURS;"
      option D: "SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.DML_HISTORY WHERE TABLE = 'MY_TABLE';"
    correct Answer: "option B: SELECT * FROM MY_TABLE CHANGES(INFORMATION => DEFAULT) AT (OFFSET => -21600);"
    explanation: >
      CORRECT - option B: CHANGES(INFORMATION => DEFAULT) AT (OFFSET => -21600) returns all DML 
      changes (inserts, updates, deletes) that occurred in the last 6 hours (21600 seconds). This 
      is the built-in CDC audit capability of Snowflake Time Travel.
      WRONG - option A: SELECT * FROM MY_TABLE AT (OFFSET => -21600) returns a point-in-time 
      snapshot of the table 6 hours ago, not the changes that occurred over the 6-hour period.
      WRONG - option C: SHOW TABLE CHANGES is not valid Snowflake syntax.
      WRONG - option D: SNOWFLAKE.ACCOUNT_USAGE.DML_HISTORY is not a real Snowflake view.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "134"
    question: "A Snowflake table ORDERS is moved to a different schema using ALTER TABLE ORDERS RENAME TO NEW_SCHEMA.ORDERS. What happens to its Time Travel history?"
    their_options:
      option A: "Time Travel history is lost during the rename/move operation"
      option B: "Time Travel history is preserved; it's a metadata operation"
      option C: "Only the last 24 hours of history transfers to the new schema location"
      option D: "The table gets a new DATA_RETENTION_TIME_IN_DAYS from the new schema"
    correct Answer: "option B: Time Travel history is preserved; it's a metadata operation"
    explanation: >
      CORRECT - option B: Moving a table to another schema via RENAME is a metadata-only operation. 
      The underlying micro-partitions (including all Time Travel history) are untouched. All 
      historical data remains fully accessible under the new location.
      WRONG - option A: Moving a table via RENAME does not lose Time Travel history; the 
      underlying storage is unchanged.
      WRONG - option C: All history within the retention window is preserved, not just 24 hours.
      WRONG - option D: While the table may eventually inherit the new schema's retention setting 
      (if the table has no explicit table-level setting), this does not immediately purge existing 
      history.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer

  - Question No: "135"
    question: "In Snowflake, what is the difference between DATA_RETENTION_TIME_IN_DAYS = 0 and dropping a table?"
    their_options:
      option A: "There is no difference — both result in loss of all historical data"
      option B: "Setting retention = 0 disables Time Travel but keeps the table active; dropping removes the table but allows UNDROP within the retention window"
      option C: "Setting retention = 0 drops the table automatically after 0 days"
      option D: "Dropping with retention > 0 allows UNDROP; retention = 0 then drop means UNDROP is unavailable"
    correct Answer: "option D: Dropping with retention > 0 allows UNDROP; retention = 0 then drop means UNDROP is unavailable"
    explanation: >
      CORRECT - option D: If a table has DATA_RETENTION_TIME_IN_DAYS > 0 when dropped, it enters 
      the Time Travel window and can be UNDROPped. If retention = 0, there is no Time Travel 
      window, so dropping the table means it cannot be recovered via UNDROP. The combination of 
      setting retention to 0 and then dropping is effectively permanent deletion.
      WRONG - option A: Setting retention = 0 keeps the table active (just disables Time Travel); 
      these are very different operations.
      WRONG - option B: Partially correct — retention = 0 does disable Time Travel and keep the 
      table, but the second part about UNDROP within retention window is where option D is more 
      precise.
      WRONG - option C: DATA_RETENTION_TIME_IN_DAYS = 0 does NOT auto-drop the table; it simply 
      disables Time Travel history.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "136"
    question: "What is the correct SQL to clone a database to its state from 12 hours ago using Time Travel?"
    their_options:
      option A: "CREATE DATABASE PROD_BACKUP AS CLONE PROD AT (OFFSET => -43200);"
      option B: "CREATE DATABASE PROD_BACKUP CLONE PROD AT (OFFSET => -43200);"
      option C: "CLONE DATABASE PROD TO PROD_BACKUP AT (OFFSET => -43200);"
      option D: "CREATE DATABASE PROD_BACKUP FROM PROD AT (OFFSET => -43200);"
    correct Answer: "option B: CREATE DATABASE PROD_BACKUP CLONE PROD AT (OFFSET => -43200);"
    explanation: >
      CORRECT - option B: The Snowflake syntax for cloning is CREATE <type> <new_name> CLONE 
      <source_name> [AT/BEFORE clause]. For database-level cloning, this is 
      CREATE DATABASE <new> CLONE <source> AT (OFFSET => ...). -43200 = -12 hours in seconds.
      WRONG - option A: AS CLONE is not the correct syntax; the keyword CLONE comes without AS.
      WRONG - option C: CLONE DATABASE is not valid Snowflake DDL syntax.
      WRONG - option D: FROM is not the correct keyword for Snowflake clone syntax; CLONE is.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "137"
    question: "A table CUSTOMER_DATA contains sensitive information. A privacy officer wants to ensure that when data is deleted under GDPR right-to-erasure requests, it cannot be recovered via Time Travel. What should the process be?"
    their_options:
      option A: "Run DELETE and the data is immediately purged from Time Travel"
      option B: "Run DELETE, then set DATA_RETENTION_TIME_IN_DAYS = 0 to purge Time Travel history"
      option C: "Run DELETE and contact Snowflake Support to purge Time Travel data"
      option D: "GDPR erasure is impossible in Snowflake due to Time Travel"
    correct Answer: "option B: Run DELETE, then set DATA_RETENTION_TIME_IN_DAYS = 0 to purge Time Travel history"
    explanation: >
      CORRECT - option B: Deleting rows removes them from the active table, but Time Travel retains 
      them. To truly purge deleted data from Time Travel, you must reduce DATA_RETENTION_TIME_IN_DAYS 
      to 0, which immediately purges all historical Time Travel data. Note: Fail-safe data 
      (7 days for permanent tables) still exists and would require Snowflake Support for complete 
      erasure — or using transient tables (no Fail-safe) for truly sensitive data.
      WRONG - option A: DELETE alone does not immediately purge Time Travel data; the historical 
      copy is retained for the full retention period.
      WRONG - option C: While Snowflake Support can help with Fail-safe data, you don't need 
      support for the Time Travel portion — setting retention to 0 handles it.
      WRONG - option D: GDPR erasure IS achievable in Snowflake through proper configuration 
      (retention = 0 + transient tables to avoid Fail-safe).
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect, DataEngineer

  - Question No: "138"
    question: "What is the result of querying a Time Travel snapshot with a LIMIT clause?"
    their_options:
      option A: "LIMIT is not supported with AT/BEFORE Time Travel queries"
      option B: "LIMIT works normally — it limits the rows returned from the historical snapshot"
      option C: "LIMIT is only supported for BEFORE queries, not AT queries"
      option D: "LIMIT returns rows in Time Travel chronological order"
    correct Answer: "option B: LIMIT works normally — it limits the rows returned from the historical snapshot"
    explanation: >
      CORRECT - option B: All standard SQL clauses including LIMIT, WHERE, ORDER BY, GROUP BY, 
      JOIN, and aggregate functions work normally with Time Travel AT/BEFORE queries. The Time 
      Travel clause simply specifies which historical snapshot to read; all subsequent SQL 
      operations apply normally to that snapshot.
      WRONG - option A: LIMIT is fully supported with Time Travel queries.
      WRONG - option C: Both AT and BEFORE support all SQL clauses including LIMIT.
      WRONG - option D: LIMIT doesn't imply chronological ordering; you would need ORDER BY 
      to define row ordering.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataAnalyst, DataEngineer

  - Question No: "139"
    question: "Scenario: A company has a critical ETL job that failed silently and loaded incorrect data into a table FACT_SALES over 3 days. How should they recover using Time Travel? (Enterprise Edition, 30-day retention)"
    their_options:
      option A: "Run UNDROP TABLE FACT_SALES to restore the pre-ETL state"
      option B: >
        "1. Identify the timestamp when the bad ETL first ran.
        2. Clone FACT_SALES at that timestamp: CREATE TABLE FACT_SALES_CLEAN CLONE FACT_SALES AT (TIMESTAMP => <pre_ETL_time>).
        3. Validate the clone.
        4. Swap: ALTER TABLE FACT_SALES SWAP WITH FACT_SALES_CLEAN."
      option C: "Run DELETE FROM FACT_SALES WHERE load_date >= <ETL_start_date> and reload from source"
      option D: "Time Travel cannot help recover from silent data corruption over multiple days"
    correct Answer: "option B"
    explanation: >
      CORRECT - option B: This is the production-safe recovery pattern using Time Travel:
      1) Identify the corruption start time using Query History.
      2) Clone the table at a point before corruption — zero-copy, instant, no storage duplication.
      3) Validate the clone has correct data.
      4) SWAP the tables atomically so FACT_SALES contains clean data with zero downtime.
      This avoids deleting and reloading data and is fast even for large tables.
      WRONG - option A: UNDROP TABLE restores a DROPPED table — FACT_SALES still exists but 
      has incorrect data. UNDROP is for dropped objects, not data corruption within a live table.
      WRONG - option C: DELETE + reload from source works but requires access to the source 
      system, assumes the source hasn't changed, and may take much longer than the clone approach.
      WRONG - option D: Time Travel is precisely designed for this type of recovery scenario.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "140"
    question: "What is the TABLE SWAP operation used in conjunction with Time Travel recovery?"
    their_options:
      option A: "ALTER TABLE A SWAP WITH B — atomically exchanges the contents and metadata of two tables"
      option B: "ALTER TABLE A SWAP WITH B — copies data from B into A"
      option C: "TABLE SWAP clones table A into table B"
      option D: "TABLE SWAP is not a valid Snowflake operation"
    correct Answer: "option A: ALTER TABLE A SWAP WITH B — atomically exchanges the contents and metadata of two tables"
    explanation: >
      CORRECT - option A: ALTER TABLE <A> SWAP WITH <B> is a Snowflake DDL command that atomically 
      swaps the definitions and contents of two tables. It's used in the Time Travel recovery pattern: 
      clone to a clean state → validate → SWAP to make the clean clone the production table while 
      retaining the corrupt data for investigation.
      WRONG - option B: SWAP is not a one-directional copy; it's a two-way atomic exchange.
      WRONG - option C: SWAP does not clone; cloning is CREATE TABLE ... CLONE ...
      WRONG - option D: TABLE SWAP (ALTER TABLE ... SWAP WITH ...) is a valid, commonly used 
      Snowflake operation.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "141"
    question: "When you run SHOW TABLES HISTORY, which of the following tables are included in the output? (SELECT TWO)"
    their_options:
      option A: "Currently active tables in the schema"
      option B: "Tables that have been dropped within the Time Travel retention window"
      option C: "Tables in Fail-safe but beyond Time Travel"
      option D: "External tables"
      option E: "Tables from all schemas in the database"
    correct Answer: "option A: Currently active tables, option B: Dropped tables within retention window"
    explanation: >
      CORRECT - option A: SHOW TABLES HISTORY includes all active (non-dropped) tables, which 
      appear with a NULL dropped_on timestamp.
      CORRECT - option B: SHOW TABLES HISTORY also includes tables that have been dropped within 
      the Time Travel retention window, showing their dropped_on timestamp.
      WRONG - option C: Tables in Fail-safe (beyond Time Travel) are not included in SHOW TABLES 
      HISTORY because they are no longer user-accessible.
      WRONG - option D: External tables are listed in SHOW EXTERNAL TABLES, not necessarily in 
      SHOW TABLES HISTORY in the same way.
      WRONG - option E: SHOW TABLES HISTORY is scoped to the current schema (or specified schema), 
      not all schemas in the database.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "142"
    question: "A Snowflake table has a DATA_RETENTION_TIME_IN_DAYS = 30. After 25 days, an ALTER TABLE command reduces the retention to 3 days. What happens to the historical data from days 4-25?"
    their_options:
      option A: "The historical data from days 4-25 remains accessible for the original 30-day window"
      option B: "The historical data from days 4-25 is immediately purged because it's outside the new 3-day window"
      option C: "The historical data from days 4-25 moves to Fail-safe"
      option D: "The historical data from days 4-25 remains for 7 more days as a grace period"
    correct Answer: "option B: The historical data from days 4-25 is immediately purged because it's outside the new 3-day window"
    explanation: >
      CORRECT - option B: When DATA_RETENTION_TIME_IN_DAYS is reduced, Snowflake IMMEDIATELY purges 
      Time Travel data that falls outside the new (shorter) retention window. Reducing from 30 to 3 
      days means only the last 3 days of history are retained; days 4-25 are immediately removed.
      WRONG - option A: The original 30-day window does not grandfather in existing history when 
      retention is reduced; purging is immediate.
      WRONG - option C: Data doesn't move from Time Travel to Fail-safe via an ALTER TABLE command; 
      it's simply purged.
      WRONG - option D: There is no grace period — the purge is immediate upon reducing retention.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "143"
    question: "What is the purpose of the INFORMATION_SCHEMA.QUERY_HISTORY() table function in the context of Time Travel?"
    their_options:
      option A: "It directly returns Time Travel queries executed against a table"
      option B: "It allows you to find query IDs of DML statements to use with BEFORE (STATEMENT => ...) for recovery"
      option C: "It shows the before and after states of all DML operations"
      option D: "It is used to measure Time Travel query performance"
    correct Answer: "option B: It allows you to find query IDs of DML statements to use with BEFORE (STATEMENT => ...) for recovery"
    explanation: >
      CORRECT - option B: INFORMATION_SCHEMA.QUERY_HISTORY() (or ACCOUNT_USAGE.QUERY_HISTORY) 
      records all query executions including their QUERY_ID, QUERY_TEXT, and timestamps. This 
      allows you to identify the specific DML statement (by query ID) that caused data issues, 
      which you can then use with BEFORE (STATEMENT => '<query_id>') to recover the pre-DML state.
      WRONG - option A: It logs ALL queries, not just Time Travel queries specifically.
      WRONG - option C: QUERY_HISTORY shows query metadata but not the actual data before/after 
      states — that's what Time Travel AT/BEFORE queries are for.
      WRONG - option D: While query history includes execution time, its primary purpose for Time 
      Travel is identifying DML statement IDs for recovery, not performance monitoring.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "144"
    question: "An organization wants all new tables in their ANALYTICS database to default to 14 days of Time Travel. What is the most efficient way to configure this?"
    their_options:
      option A: "Set DATA_RETENTION_TIME_IN_DAYS = 14 on each table as it is created"
      option B: "ALTER DATABASE ANALYTICS SET DATA_RETENTION_TIME_IN_DAYS = 14;"
      option C: "ALTER ACCOUNT SET DATA_RETENTION_TIME_IN_DAYS = 14;"
      option D: "Create a template CREATE TABLE script with DATA_RETENTION_TIME_IN_DAYS = 14"
    correct Answer: "option B: ALTER DATABASE ANALYTICS SET DATA_RETENTION_TIME_IN_DAYS = 14;"
    explanation: >
      CORRECT - option B: Setting DATA_RETENTION_TIME_IN_DAYS at the database level establishes 
      the default for all new tables within that database (for those without explicit table-level 
      settings). This is more efficient than configuring each table individually.
      WRONG - option A: Setting on each table individually works but is error-prone and requires 
      discipline; a database-level default is more manageable.
      WRONG - option C: Setting at the account level affects all databases and tables in the account, 
      which may not be desired. A database-scoped setting is more targeted.
      WRONG - option D: Template scripts are brittle and require compliance; a database parameter 
      is enforced automatically.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "145"
    question: "Which of the following best describes why Snowflake's micro-partition architecture naturally supports Time Travel efficiently?"
    their_options:
      option A: "Micro-partitions are mutable, so old versions are kept alongside new ones"
      option B: "Micro-partitions are immutable; DML creates new partitions while retaining old ones, enabling Time Travel with no additional write cost"
      option C: "Micro-partitions use copy-on-write, creating full copies for each version"
      option D: "Time Travel uses a separate log file alongside micro-partitions"
    correct Answer: "option B: Micro-partitions are immutable; DML creates new partitions while retaining old ones, enabling Time Travel with no additional write cost"
    explanation: >
      CORRECT - option B: Snowflake's core storage architecture uses immutable micro-partitions. 
      When data changes, new micro-partitions are written while old ones are retained. This 
      immutability is the architectural foundation for Time Travel — old micro-partitions naturally 
      represent historical states without requiring any special logging or copy mechanism.
      WRONG - option A: Micro-partitions are immutable, not mutable. Immutability is the key.
      WRONG - option C: While the effect is similar to copy-on-write, Snowflake's micro-partitions 
      are not modified at all — they're immutable, not copied.
      WRONG - option D: Time Travel doesn't use a separate log file; it leverages the immutable 
      micro-partition storage directly.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect

  - Question No: "146"
    question: "A Snowflake Stream on table T has offset pointing to a transaction from 5 days ago. The table T has DATA_RETENTION_TIME_IN_DAYS = 3. What is the status of the stream?"
    their_options:
      option A: "The stream is active because streams have their own 14-day retention"
      option B: "The stream is STALE because the offset references Time Travel data that has expired"
      option C: "The stream auto-resets to the current state when Time Travel expires"
      option D: "The stream becomes read-only but not stale"
    correct Answer: "option B: The stream is STALE because the offset references Time Travel data that has expired"
    explanation: >
      CORRECT - option B: A stream's offset must reference data within the underlying table's Time 
      Travel retention window. Since the stream's offset points 5 days back but the table only 
      retains 3 days, the referenced historical data no longer exists. The stream becomes STALE 
      (STALE = TRUE) and querying it returns an error.
      WRONG - option A: Streams don't have independent retention separate from the table's Time Travel.
      WRONG - option C: Stale streams do NOT auto-reset; they must be explicitly addressed 
      (typically by recreating the stream).
      WRONG - option D: Stale is the correct state; there's no read-only intermediate state.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "147"
    question: "What SQL would you use to check if a Snowflake Stream is stale?"
    their_options:
      option A: "DESCRIBE STREAM MY_STREAM; — check the STALE column"
      option B: "SHOW STREAMS LIKE 'MY_STREAM'; — check the stale column"
      option C: "SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.STREAM_STALE_STATUS WHERE STREAM_NAME = 'MY_STREAM';"
      option D: "Both A and B are valid"
    correct Answer: "option D: Both A and B are valid"
    explanation: >
      CORRECT - option D: Both SHOW STREAMS and DESCRIBE STREAM display the STALE column 
      which shows TRUE or FALSE indicating whether the stream's Time Travel offset has expired.
      SHOW STREAMS LIKE 'MY_STREAM' lists streams with their stale status.
      DESCRIBE STREAM MY_STREAM shows detailed properties including stale status.
      WRONG - option A alone: Valid but not the only answer.
      WRONG - option B alone: Valid but not the only answer.
      WRONG - option C: SNOWFLAKE.ACCOUNT_USAGE.STREAM_STALE_STATUS is not a real Snowflake view.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer

  - Question No: "148"
    question: "Which of the following is a valid use case for TIME TRAVEL with BEFORE (STATEMENT) rather than BEFORE (OFFSET)? (SELECT TWO)"
    their_options:
      option A: "You know the exact query ID of the bad DML statement"
      option B: "You need the most precise pre-DML state (microsecond before a specific statement)"
      option C: "You need to query data from exactly 48 hours ago"
      option D: "You need to query the state at a specific business date like end-of-quarter"
      option E: "You don't know when the bad DML ran but know it was 'around noon'"
    correct Answer: "option A: Know the exact query ID, option B: Most precise pre-DML state"
    explanation: >
      CORRECT - option A: When you have the query ID, BEFORE (STATEMENT => '<query_id>') is the most 
      direct and precise way to access the pre-DML state of that specific operation.
      CORRECT - option B: BEFORE (STATEMENT) provides microsecond precision relative to a specific 
      DML statement, which is more precise than time-based offsets for pinpointing pre-DML state.
      WRONG - option C: Querying exactly 48 hours ago is better served by OFFSET or TIMESTAMP, 
      not STATEMENT (no specific DML statement is referenced).
      WRONG - option D: End-of-quarter is an absolute timestamp, best served by AT (TIMESTAMP => ...).
      WRONG - option E: Without a specific query ID, STATEMENT cannot be used; you'd use 
      OFFSET or TIMESTAMP to approximate "around noon."
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "149"
    question: "A data engineer runs: SELECT * FROM T1 AT (OFFSET => -3600) t1 JOIN T2 AT (OFFSET => -7200) t2 ON t1.id = t2.id; What does this query do?"
    their_options:
      option A: "This syntax is invalid — all tables in a join must use the same Time Travel point"
      option B: "Joins T1 as it was 1 hour ago with T2 as it was 2 hours ago"
      option C: "Joins the current T1 with T2 as it was 2 hours ago (AT on first table is ignored)"
      option D: "Uses the average of the two offsets: queries both tables at 1.5 hours ago"
    correct Answer: "option B: Joins T1 as it was 1 hour ago with T2 as it was 2 hours ago"
    explanation: >
      CORRECT - option B: Snowflake supports independent AT/BEFORE clauses on each table in a 
      JOIN. T1 AT (OFFSET => -3600) queries T1 at 1 hour ago, T2 AT (OFFSET => -7200) queries 
      T2 at 2 hours ago. These are completely independent historical snapshots joined together.
      WRONG - option A: This is valid Snowflake syntax. Different AT/BEFORE clauses per table 
      are fully supported.
      WRONG - option C: Both AT clauses apply to their respective tables; neither is ignored.
      WRONG - option D: Offsets are applied independently to each table; there is no averaging.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "150"
    question: "What is the RETAINED_FOR_CLONE_BYTES column in TABLE_STORAGE_METRICS?"
    their_options:
      option A: "Bytes of data retained in Time Travel specifically for cloned tables"
      option B: "Bytes of micro-partitions retained because a zero-copy clone still references them"
      option C: "Bytes of storage used by all clones derived from a source table"
      option D: "Bytes reserved in advance for future cloning operations"
    correct Answer: "option B: Bytes of micro-partitions retained because a zero-copy clone still references them"
    explanation: >
      CORRECT - option B: When a zero-copy clone is created, it shares micro-partitions with the 
      source. Even if the source table's active data no longer references those partitions (due to 
      updates/deletes on the source), they must be retained as long as the clone references them. 
      RETAINED_FOR_CLONE_BYTES captures this storage overhead.
      WRONG - option A: RETAINED_FOR_CLONE_BYTES is distinct from TIME_TRAVEL_BYTES; it's 
      specifically about clone reference retention, not Time Travel history.
      WRONG - option C: It measures the retained partitions for all clones referencing the source, 
      but from the SOURCE table's perspective (not from the clone's perspective).
      WRONG - option D: Snowflake doesn't reserve storage in advance; billing is purely consumption-based.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect, DataEngineer

  - Question No: "151"
    question: "Which two objects can be directly cloned using Time Travel in a single command? (SELECT TWO)"
    their_options:
      option A: "TABLE"
      option B: "VIEW"
      option C: "DATABASE"
      option D: "STAGE"
      option E: "PIPE"
    correct Answer: "option A: TABLE, option C: DATABASE"
    explanation: >
      CORRECT - option A (TABLE): CREATE TABLE <new> CLONE <source> AT (...) is fully supported 
      for Time Travel-based cloning.
      CORRECT - option C (DATABASE): CREATE DATABASE <new> CLONE <source> AT (...) creates a 
      zero-copy clone of the entire database at a historical point.
      WRONG - option B (VIEW): Views are metadata objects and don't have data to clone with 
      Time Travel. You can clone the underlying base tables.
      WRONG - option D (STAGE): Stages store files for loading/unloading and don't support 
      Time Travel or cloning with AT/BEFORE.
      WRONG - option E (PIPE): Pipes are used for Snowpipe continuous loading and cannot be 
      cloned with Time Travel.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "152"
    question: "What is the exact error type a user encounters when they attempt an AT (OFFSET) query that exceeds the table's retention window?"
    their_options:
      option A: "SQL compilation error: table not found"
      option B: "Time Travel data is not available for the requested time"
      option C: "Insufficient privileges to query historical data"
      option D: "Virtual warehouse timeout error"
    correct Answer: "option B: Time Travel data is not available for the requested time"
    explanation: >
      CORRECT - option B: When a Time Travel query references a point beyond the retention window, 
      Snowflake returns an error indicating the data is not available for the requested time 
      (the exact phrasing may be: 'History data is not available. Please contact Snowflake Support' 
      or 'Time travel data is not available beyond the retention period').
      WRONG - option A: 'Table not found' would indicate the table itself doesn't exist, not 
      a retention window issue.
      WRONG - option C: Insufficient privileges is an access control error, not a retention error.
      WRONG - option D: A warehouse timeout relates to compute resources, not Time Travel availability.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "153"
    question: "Can you use Time Travel on Snowflake objects created in other cloud regions?"
    their_options:
      option A: "No, Time Travel only works within the same cloud region"
      option B: "Yes, Time Travel works the same across all cloud platforms and regions in Snowflake"
      option C: "Yes, but with a maximum of 1-day retention for cross-region objects"
      option D: "Time Travel requires dedicated cross-region replication to be enabled"
    correct Answer: "option B: Yes, Time Travel works the same across all cloud platforms and regions in Snowflake"
    explanation: >
      CORRECT - option B: Time Travel is a core Snowflake feature available in all regions and 
      cloud platforms (AWS, Azure, GCP). It operates identically regardless of cloud provider 
      or region. The retention limits are determined by edition, not geography.
      WRONG - option A: There is no regional restriction on Time Travel.
      WRONG - option C: Cross-region does not limit retention to 1 day; edition type determines limits.
      WRONG - option D: Time Travel does not depend on replication configuration; they are 
      independent features.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, Architect

  - Question No: "154"
    question: "A Snowflake table is partitioned by date (using clustering). Does Time Travel work differently for clustered tables?"
    their_options:
      option A: "Yes, Time Travel only retains the most recently clustered micro-partitions"
      option B: "No, Time Travel works the same for clustered and non-clustered tables; the automatic clustering re-writes micro-partitions which are then retained in Time Travel"
      option C: "Clustered tables have 50% less Time Travel storage overhead"
      option D: "Time Travel cannot be used with clustering keys enabled"
    correct Answer: "option B: No, Time Travel works the same for clustered and non-clustered tables; the automatic clustering re-writes micro-partitions which are then retained in Time Travel"
    explanation: >
      CORRECT - option B: Time Travel operates at the micro-partition level regardless of clustering. 
      However, it's important to note that automatic clustering generates DML-like operations 
      (reclustering) that create new micro-partitions and retain old ones in Time Travel. This 
      means heavily clustered tables can accumulate significant Time Travel storage.
      WRONG - option A: All relevant micro-partitions are retained, not just recently clustered ones.
      WRONG - option C: Clustering does not reduce Time Travel storage overhead; in fact, 
      it can increase it due to reclustering DML.
      WRONG - option D: Time Travel is fully compatible with clustering keys.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "155"
    question: "What happens to Time Travel data when a Snowflake table's ownership is transferred from one role to another?"
    their_options:
      option A: "Time Travel data is purged when ownership changes"
      option B: "Time Travel data is preserved; ownership transfer is a metadata operation"
      option C: "Only the new owner can access Time Travel history after the transfer"
      option D: "Time Travel resets to 1 day after an ownership transfer"
    correct Answer: "option B: Time Travel data is preserved; ownership transfer is a metadata operation"
    explanation: >
      CORRECT - option B: Changing a table's ownership (GRANT OWNERSHIP ... TO ROLE ...) is 
      a metadata operation that updates the access control but does not affect the underlying 
      data storage or Time Travel history.
      WRONG - option A: Time Travel data is not purged on ownership changes.
      WRONG - option C: Access to Time Travel data is governed by the SELECT privilege, not 
      exclusively by ownership. Any role with SELECT can use Time Travel.
      WRONG - option D: DATA_RETENTION_TIME_IN_DAYS is not reset by ownership changes.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer

  - Question No: "156"
    question: "You need to create a report showing how a table's row count changed every day for the past week. Which Time Travel approach enables this most efficiently?"
    their_options:
      option A: >
        "Run 7 separate queries:
        SELECT COUNT(*) FROM T AT (OFFSET => -86400);
        SELECT COUNT(*) FROM T AT (OFFSET => -172800); etc."
      option B: >
        "SELECT 1 AS day, COUNT(*) FROM T AT (OFFSET => -86400)
        UNION ALL SELECT 2, COUNT(*) FROM T AT (OFFSET => -172800)
        UNION ALL SELECT 3, COUNT(*) FROM T AT (OFFSET => -259200)
        UNION ALL SELECT 4, COUNT(*) FROM T AT (OFFSET => -345600)
        UNION ALL SELECT 5, COUNT(*) FROM T AT (OFFSET => -432000)
        UNION ALL SELECT 6, COUNT(*) FROM T AT (OFFSET => -518400)
        UNION ALL SELECT 7, COUNT(*) FROM T AT (OFFSET => -604800);"
      option C: "SELECT DATE, COUNT(*) FROM T GROUP BY DATE ORDER BY DATE;"
      option D: "SELECT * FROM T CHANGES(INFORMATION => DEFAULT) AT (OFFSET => -604800) GROUP BY date;"
    correct Answer: "option B"
    explanation: >
      CORRECT - option B: Using UNION ALL with 7 Time Travel AT queries at different daily offsets 
      in a single SQL statement is the most efficient approach. This returns the daily row counts 
      in one query execution, which can be used directly for the report.
      WRONG - option A: Running 7 separate queries works but is less efficient than a single 
      UNION ALL statement; it requires 7 roundtrips vs. 1.
      WRONG - option C: This queries current data grouped by a date column, which is fundamentally 
      different from Time Travel historical snapshots.
      WRONG - option D: The CHANGES clause returns change delta rows, not point-in-time snapshots 
      for counting; this doesn't give daily row counts accurately.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "157"
    question: "What does FAILSAFE_BYTES represent in SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS?"
    their_options:
      option A: "Bytes in Time Travel that are about to expire and move to Fail-safe"
      option B: "Bytes of data in the 7-day Fail-safe period after Time Travel has expired"
      option C: "Bytes reserved for disaster recovery by Snowflake"
      option D: "Bytes of data protected by both Time Travel and Fail-safe"
    correct Answer: "option B: Bytes of data in the 7-day Fail-safe period after Time Travel has expired"
    explanation: >
      CORRECT - option B: FAILSAFE_BYTES represents the storage consumed by data that has passed 
      through the Time Travel window and is now in the Fail-safe period (the 7-day backend-only 
      recovery window for permanent tables). This data is not user-accessible.
      WRONG - option A: Data in Time Travel appears in TIME_TRAVEL_BYTES; it doesn't "move" 
      to Fail-safe in the metrics — FAILSAFE_BYTES specifically tracks the Fail-safe tier.
      WRONG - option C: FAILSAFE_BYTES is actual consumed storage, not reserved space.
      WRONG - option D: Data in Time Travel is tracked in TIME_TRAVEL_BYTES; Fail-safe data 
      is tracked separately in FAILSAFE_BYTES.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "158"
    question: "Scenario: A company processes financial data and cannot afford any Time Travel data being accessible to unauthorized users. How should they approach Time Travel security?"
    their_options:
      option A: "Disable Time Travel entirely by setting DATA_RETENTION_TIME_IN_DAYS = 0 globally"
      option B: "Grant SELECT privilege only to authorized roles — Time Travel queries respect the same access controls as regular queries"
      option C: "Enable Time Travel encryption with a separate encryption key"
      option D: "Move all financial tables to external storage where Time Travel is unavailable"
    correct Answer: "option B: Grant SELECT privilege only to authorized roles — Time Travel queries respect the same access controls as regular queries"
    explanation: >
      CORRECT - option B: Time Travel queries (AT/BEFORE) require the same SELECT privilege on 
      the table as regular queries. If access to the table is restricted via RBAC (Role-Based 
      Access Control), those same restrictions apply to Time Travel. Unauthorized users cannot 
      access historical data any more than they can access current data.
      WRONG - option A: Disabling Time Travel entirely sacrifices recovery capability, which is 
      too drastic when proper RBAC is the simpler, correct solution.
      WRONG - option C: There is no separate Time Travel-specific encryption; Time Travel data 
      uses the same encryption as active data.
      WRONG - option D: External tables don't support Time Travel, but moving data to external 
      storage introduces other access control challenges and loses Snowflake's RBAC benefits.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect, DataEngineer

  - Question No: "159"
    question: "What is the minimum Snowflake edition required to set DATA_RETENTION_TIME_IN_DAYS = 5 for a table?"
    their_options:
      option A: "Standard Edition"
      option B: "Enterprise Edition"
      option C: "Business Critical Edition"
      option D: "Any edition — all support retention up to 90 days"
    correct Answer: "option B: Enterprise Edition"
    explanation: >
      CORRECT - option B: Standard Edition only supports 0 or 1 day Time Travel retention. Any 
      retention value greater than 1 (including 5) requires Enterprise Edition or higher.
      WRONG - option A: Standard Edition cannot support DATA_RETENTION_TIME_IN_DAYS > 1; 
      setting 5 on Standard Edition throws an error.
      WRONG - option C: Business Critical is sufficient but not the minimum; Enterprise Edition 
      is the minimum required.
      WRONG - option D: Standard Edition cannot support values greater than 1 day.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, Architect

  - Question No: "160"
    question: "What is the impact on Snowflake query performance when using Time Travel AT/BEFORE queries?"
    their_options:
      option A: "Time Travel queries are always significantly slower than regular queries"
      option B: "Time Travel queries have similar performance to regular queries for recent timestamps; performance may degrade for very old timestamps due to micro-partition pruning differences"
      option C: "Time Travel queries bypass the query optimizer and always do full table scans"
      option D: "Time Travel queries are faster because they read compressed historical data"
    correct Answer: "option B: Time Travel queries have similar performance to regular queries for recent timestamps; performance may degrade for very old timestamps due to micro-partition pruning differences"
    explanation: >
      CORRECT - option B: For recent timestamps, Time Travel queries leverage the same micro-partition 
      metadata and pruning as regular queries. For much older timestamps, the partition pruning 
      metadata may be less optimal as historical micro-partition statistics differ from current 
      ones, potentially leading to less efficient pruning.
      WRONG - option A: Time Travel queries are not inherently always slower; recent Time Travel 
      queries perform similarly to regular queries.
      WRONG - option C: Time Travel queries still use the query optimizer and benefit from 
      partition pruning based on the historical state's metadata.
      WRONG - option D: Historical data is stored at the same compression level as active data; 
      no speed advantage from compression.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "161"
    question: "How would you use Time Travel to restore a specific row (not all rows) that was accidentally deleted from a table?"
    their_options:
      option A: >
        "INSERT INTO MY_TABLE
        SELECT * FROM MY_TABLE BEFORE (STATEMENT => '<delete_query_id>')
        WHERE id = 12345;"
      option B: "RESTORE ROW 12345 FROM MY_TABLE AT (OFFSET => -3600);"
      option C: "UNDROP ROW FROM MY_TABLE WHERE id = 12345;"
      option D: "UPDATE MY_TABLE SET deleted = FALSE WHERE id = 12345;"
    correct Answer: "option A"
    explanation: >
      CORRECT - option A: To restore a specific row, use INSERT INTO ... SELECT with a WHERE 
      clause filtering on the row's key, referencing the table in its pre-delete state using 
      BEFORE (STATEMENT => ...). This inserts only the specific deleted row back.
      WRONG - option B: RESTORE ROW is not valid Snowflake syntax.
      WRONG - option C: UNDROP ROW is not valid Snowflake syntax; UNDROP only works for 
      table-level objects (TABLE, SCHEMA, DATABASE).
      WRONG - option D: A soft-delete column (deleted = FALSE) would only work if the table 
      uses a soft-delete pattern; for hard deletes, Time Travel is required.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "162"
    question: "What is the key architectural reason that Snowflake can offer Time Travel for free (no performance overhead during DML) compared to traditional databases?"
    their_options:
      option A: "Snowflake uses a special background process that copies data asynchronously"
      option B: "Snowflake's immutable micro-partition architecture means DML writes new partitions without modifying existing ones; old partitions are simply retained"
      option C: "Snowflake charges extra for Time Travel to fund dedicated versioning infrastructure"
      option D: "Snowflake stores only deltas (differences) between versions to minimize overhead"
    correct Answer: "option B: Snowflake's immutable micro-partition architecture means DML writes new partitions without modifying existing ones; old partitions are simply retained"
    explanation: >
      CORRECT - option B: Because micro-partitions are immutable, a DML operation simply marks 
      old partitions for eventual deletion and writes new ones. For Time Travel, the "deletion" 
      step is delayed by the retention period. No extra versioning writes are needed — Time Travel 
      is an architectural side effect of the immutable storage model, not an additional feature layer.
      WRONG - option A: There is no background copy process; old micro-partitions are just retained 
      in their natural state.
      WRONG - option C: Time Travel is included in Snowflake pricing; there's no separate Time 
      Travel fee (only the standard storage cost for retained data).
      WRONG - option D: Snowflake stores complete micro-partitions, not deltas. The immutable 
      storage model does mean only changed partitions are retained, but they are full partitions.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect

  - Question No: "163"
    question: "A table has had 3 schema changes (ALTER TABLE ADD/DROP COLUMN) over the past week. Can Time Travel queries access data from before the schema changes?"
    their_options:
      option A: "Yes, Time Travel stores schema evolution history and returns data in the historical schema"
      option B: "No, schema changes are DDL and invalidate Time Travel history"
      option C: "Yes, but column additions return NULL for pre-existing rows and dropped column data is accessible"
      option D: "Only if the DATA_RETENTION_TIME_IN_DAYS was set before the schema changes"
    correct Answer: "option A: Yes, Time Travel stores schema evolution history and returns data in the historical schema"
    explanation: >
      CORRECT - option A: Time Travel can query data at historical points even if the schema has 
      changed since. Snowflake tracks schema evolution alongside data evolution. A query at a 
      historical timestamp will return data according to the schema that was in effect at that time.
      WRONG - option B: DDL changes (ALTER TABLE) do NOT invalidate Time Travel history; 
      they are tracked as part of the object's history.
      WRONG - option C: This partially describes how Time Travel handles schema changes, but 
      option A is the most accurate and complete answer.
      WRONG - option D: DATA_RETENTION_TIME_IN_DAYS being set at a specific time doesn't affect 
      schema change compatibility with Time Travel.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "164"
    question: "What is the SHOW SCHEMAS HISTORY command used for?"
    their_options:
      option A: "Shows the historical DDL changes made to all schemas"
      option B: "Lists all schemas including those dropped within the Time Travel retention window"
      option C: "Shows the query history for all schemas"
      option D: "SHOW SCHEMAS HISTORY is not valid Snowflake syntax"
    correct Answer: "option B: Lists all schemas including those dropped within the Time Travel retention window"
    explanation: >
      CORRECT - option B: Similar to SHOW TABLES HISTORY, SHOW SCHEMAS HISTORY lists all schemas 
      in the current database including those that have been dropped but are still within the 
      Time Travel retention window (available for UNDROP).
      WRONG - option A: It doesn't show DDL change history; it shows object existence history 
      (active and recently dropped schemas).
      WRONG - option C: Schema-level query history is in ACCOUNT_USAGE.QUERY_HISTORY, not 
      in SHOW SCHEMAS HISTORY.
      WRONG - option D: SHOW SCHEMAS HISTORY IS valid Snowflake syntax.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer

  - Question No: "165"
    question: "Scenario: A business user says 'I deleted some rows 2 hours ago but I need the data back. The table has 1-day retention.' The DBA finds the query ID of the delete. What is the optimal step-by-step recovery?"
    their_options:
      option A: >
        "1. UNDROP TABLE <table_name>;
        2. Verify data is restored."
      option B: >
        "1. CREATE TABLE <table_name>_BACKUP CLONE <table_name> BEFORE (STATEMENT => '<delete_query_id>');
        2. Validate the backup has the deleted rows.
        3. INSERT INTO <table_name> SELECT * FROM <table_name>_BACKUP WHERE id IN (<deleted_ids>);
        4. DROP TABLE <table_name>_BACKUP;"
      option C: >
        "1. INSERT INTO <table_name> SELECT * FROM <table_name> BEFORE (STATEMENT => '<delete_query_id>') WHERE id IN (<deleted_ids>);"
      option D: "Both B and C are valid approaches, with C being simpler"
    correct Answer: "option D: Both B and C are valid approaches, with C being simpler"
    explanation: >
      CORRECT - option D: Both approaches work correctly:
      Option C (simpler): Directly inserts the deleted rows back using BEFORE (STATEMENT) 
      with a WHERE filter — single SQL statement, no intermediate objects needed. Best for 
      simple recovery.
      Option B (safer): Creates a validation clone first, verifies it has the right data, 
      then inserts — adds a validation step to prevent double-recovery mistakes. Better for 
      critical production scenarios.
      WRONG - option A: UNDROP is for DROPPED tables, not for rows deleted by DML.
      WRONG - option B alone: Valid but not the only correct approach.
      WRONG - option C alone: Valid and simpler but not the only correct approach.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "166"
    question: "In Snowflake, can you combine Time Travel with Snowflake Secure Views?"
    their_options:
      option A: "Yes, Secure Views support AT/BEFORE clauses"
      option B: "No, Secure Views block Time Travel access"
      option C: "Time Travel on Secure Views requires ACCOUNTADMIN role"
      option D: "You can use Time Travel on the underlying base tables through standard views or queries; Secure Views themselves don't support AT/BEFORE"
    correct Answer: "option D: You can use Time Travel on the underlying base tables through standard views or queries; Secure Views themselves don't support AT/BEFORE"
    explanation: >
      CORRECT - option D: Like regular views, Secure Views don't support AT/BEFORE clauses on 
      the view reference itself. However, you can apply Time Travel to the underlying base tables 
      directly (if you have access) or create a query that applies AT/BEFORE to base table references.
      WRONG - option A: Neither regular nor Secure Views support AT/BEFORE on the view reference.
      WRONG - option B: Secure Views don't specifically "block" Time Travel; they simply don't 
      support AT/BEFORE clauses (same as regular views).
      WRONG - option C: ACCOUNTADMIN is not a relevant privilege for Time Travel access.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "167"
    question: "A developer writes: SELECT * FROM ORDERS BEFORE (TIMESTAMP => CURRENT_TIMESTAMP()); What does this return?"
    their_options:
      option A: "An error because BEFORE cannot be used with CURRENT_TIMESTAMP()"
      option B: "The state of ORDERS just before the current moment (effectively current data)"
      option C: "An empty result set"
      option D: "Data from 1 second ago"
    correct Answer: "option B: The state of ORDERS just before the current moment (effectively current data)"
    explanation: >
      CORRECT - option B: BEFORE (TIMESTAMP => CURRENT_TIMESTAMP()) queries the state just 
      before the current timestamp — which is effectively microseconds before now, returning 
      the current data. This is a valid but rarely useful query since it returns essentially 
      the same result as a regular SELECT.
      WRONG - option A: CURRENT_TIMESTAMP() is a valid expression for BEFORE TIMESTAMP clause.
      WRONG - option C: The query returns current data, not an empty set.
      WRONG - option D: BEFORE at CURRENT_TIMESTAMP is microseconds before now, not 1 second.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataAnalyst, DataEngineer

  - Question No: "168"
    question: "When a permanent table is dropped in Snowflake, through what sequence of phases does its data pass before complete deletion?"
    their_options:
      option A: "Active → Deleted"
      option B: "Active → Time Travel → Fail-safe → Permanent Deletion"
      option C: "Active → Fail-safe → Time Travel → Permanent Deletion"
      option D: "Active → Time Travel → Permanent Deletion"
    correct Answer: "option B: Active → Time Travel → Fail-safe → Permanent Deletion"
    explanation: >
      CORRECT - option B: The data lifecycle for a dropped permanent table is:
      1. Active: Data is live and queryable.
      2. Time Travel: After DROP, data enters the Time Travel window (user-accessible via AT/BEFORE/UNDROP).
      3. Fail-safe: After Time Travel expires, data enters 7-day Fail-safe (Snowflake-only access).
      4. Permanent Deletion: After Fail-safe, data is permanently purged.
      WRONG - option A: Only two phases; missing the Time Travel and Fail-safe periods.
      WRONG - option C: Fail-safe comes AFTER Time Travel, not before.
      WRONG - option D: Only two recovery phases shown; Fail-safe is missing.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer, Architect

  - Question No: "169"
    question: "A table WAREHOUSE_INVENTORY has DATA_RETENTION_TIME_IN_DAYS = 14. How many days of Fail-safe does this table have?"
    their_options:
      option A: "14 days (same as Time Travel retention)"
      option B: "7 days (fixed for all permanent tables)"
      option C: "0 days (Fail-safe is only for tables with < 7 days retention)"
      option D: "21 days (14 + 7)"
    correct Answer: "option B: 7 days (fixed for all permanent tables)"
    explanation: >
      CORRECT - option B: Fail-safe is always 7 days for permanent tables regardless of the 
      DATA_RETENTION_TIME_IN_DAYS setting. It's a fixed, non-configurable period.
      WRONG - option A: Fail-safe does not change based on the Time Travel retention setting.
      WRONG - option C: Fail-safe is available for ALL permanent tables regardless of their 
      retention setting.
      WRONG - option D: 21 days would be the total protection (14 TT + 7 FS), not the Fail-safe 
      period alone.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "170"
    question: "True or False: Time Travel queries count against a Snowflake virtual warehouse's credit consumption."
    their_options:
      option A: "False — Time Travel queries are free and don't consume credits"
      option B: "True — Time Travel queries run on virtual warehouses and consume credits like any other query"
      option C: "True — Time Travel queries consume 2x credits due to the additional historical data scan"
      option D: "False — Time Travel data is served from a dedicated serverless cache"
    correct Answer: "option B: True — Time Travel queries run on virtual warehouses and consume credits like any other query"
    explanation: >
      CORRECT - option B: Time Travel SELECT queries (using AT/BEFORE) are executed on virtual 
      warehouses just like any regular query. They consume credits based on the warehouse size 
      and query duration. There is no special billing for Time Travel compute.
      WRONG - option A: Time Travel queries DO consume warehouse credits; only storage is 
      distinctly billed. Compute is the same.
      WRONG - option C: There is no 2x credit multiplier for Time Travel queries.
      WRONG - option D: Time Travel data is in cloud storage, not a dedicated serverless cache. 
      Queries require an active virtual warehouse.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "171"
    question: "When using CHANGES(INFORMATION => DEFAULT) in a Time Travel query, what does the METADATA$ROW_ID column contain?"
    their_options:
      option A: "The primary key of the changed row"
      option B: "A unique, opaque internal identifier for the row that is stable across DML operations"
      option C: "The timestamp when the row was last changed"
      option D: "The query ID of the DML statement that changed the row"
    correct Answer: "option B: A unique, opaque internal identifier for the row that is stable across DML operations"
    explanation: >
      CORRECT - option B: METADATA$ROW_ID is Snowflake's internal unique identifier for each row. 
      It's immutable once assigned and can be used to correlate the DELETE (before-image) and 
      INSERT (after-image) rows of an UPDATE operation in CHANGES output.
      WRONG - option A: METADATA$ROW_ID is an internal Snowflake ID, not the user-defined primary key.
      WRONG - option C: Row timestamps are not provided by METADATA$ROW_ID; you would need to 
      derive timestamps from the Time Travel window or query history.
      WRONG - option D: METADATA$ROW_ID is a row identifier, not a query identifier.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "172"
    question: "What is the recommended practice for tables containing very large amounts of historical data that need to minimize Time Travel storage costs while maintaining a meaningful recovery window?"
    their_options:
      option A: "Use DATA_RETENTION_TIME_IN_DAYS = 90 to maximize recovery options"
      option B: "Use DATA_RETENTION_TIME_IN_DAYS = 0 to eliminate costs"
      option C: "Set DATA_RETENTION_TIME_IN_DAYS to a value matching business recovery requirements (e.g., 3-7 days) instead of defaulting to the maximum"
      option D: "Convert to External Tables to avoid Time Travel storage entirely"
    correct Answer: "option C: Set DATA_RETENTION_TIME_IN_DAYS to a value matching business recovery requirements (e.g., 3-7 days) instead of defaulting to the maximum"
    explanation: >
      CORRECT - option C: The best practice is to set retention based on actual business recovery 
      requirements, not defaulting to the maximum. A table with heavy DML that only needs 3 days 
      of recovery capability should use DATA_RETENTION_TIME_IN_DAYS = 3 to avoid unnecessary 
      storage costs from retaining 90 days of history.
      WRONG - option A: 90 days of retention for all tables generates massive storage costs and 
      should only be used when 90-day recovery is genuinely required.
      WRONG - option B: Setting to 0 eliminates Time Travel entirely, removing recovery capability.
      WRONG - option D: External tables don't support Time Travel, which removes recovery capabilities.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "173"
    question: "What is the time precision of the OFFSET parameter in Snowflake Time Travel?"
    their_options:
      option A: "Milliseconds"
      option B: "Seconds"
      option C: "Minutes"
      option D: "Hours"
    correct Answer: "option B: Seconds"
    explanation: >
      CORRECT - option B: The OFFSET parameter in Time Travel accepts values in SECONDS 
      (as an integer). For example, AT (OFFSET => -3600) means 3600 seconds = 1 hour ago.
      WRONG - option A: OFFSET is in seconds, not milliseconds. For millisecond precision, 
      use the TIMESTAMP sub-clause with a precise timestamp value.
      WRONG - option C: OFFSET is in seconds, not minutes. -60 seconds = 1 minute ago.
      WRONG - option D: OFFSET is in seconds, not hours. -3600 seconds = 1 hour ago.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Core, DataEngineer

  - Question No: "174"
    question: "A company uses dbt (data build tool) with Snowflake. How does dbt's 'full refresh' mode interact with Time Travel?"
    their_options:
      option A: "dbt full refresh completely bypasses Time Travel"
      option B: "dbt full refresh typically runs DROP TABLE + CREATE TABLE AS SELECT, which drops the old table (entering Time Travel) and creates a new one (starting fresh Time Travel history)"
      option C: "dbt full refresh extends Time Travel to include the pre-refresh history"
      option D: "dbt full refresh disables Time Travel for the model"
    correct Answer: "option B: dbt full refresh typically runs DROP TABLE + CREATE TABLE AS SELECT, which drops the old table (entering Time Travel) and creates a new one (starting fresh Time Travel history)"
    explanation: >
      CORRECT - option B: dbt's full refresh strategy runs DROP TABLE (or DROP IF EXISTS) followed 
      by CREATE TABLE AS SELECT. The DROP puts the old table into Time Travel (within retention window). 
      The new table starts its own Time Travel history from creation. The old and new tables are 
      separate objects with independent histories.
      WRONG - option A: dbt full refresh doesn't bypass Time Travel; it creates a new table 
      and the old one enters Time Travel as dropped.
      WRONG - option C: dbt full refresh doesn't merge Time Travel histories; they're separate.
      WRONG - option D: dbt operations follow standard Snowflake DDL/DML rules; they don't 
      disable Time Travel.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer

  - Question No: "175"
    question: "Which of the following correctly explains the APPEND_ONLY vs DEFAULT options in CHANGES clause? (SELECT TWO)"
    their_options:
      option A: "DEFAULT captures inserts, updates (as delete+insert pairs), and deletes"
      option B: "APPEND_ONLY captures only net-new inserts (not updated or subsequently deleted rows)"
      option C: "APPEND_ONLY is used for append-only tables that don't support updates"
      option D: "DEFAULT returns duplicates for updated rows"
      option E: "APPEND_ONLY returns all rows with METADATA$ACTION = 'DELETE'"
    correct Answer: "option A: DEFAULT captures all DML types, option B: APPEND_ONLY captures net-new inserts only"
    explanation: >
      CORRECT - option A: CHANGES(INFORMATION => DEFAULT) returns all DML changes: inserts 
      (ACTION=INSERT), delete-side of updates (ACTION=DELETE, ISUPDATE=TRUE), insert-side of 
      updates (ACTION=INSERT, ISUPDATE=TRUE), and pure deletes (ACTION=DELETE, ISUPDATE=FALSE).
      CORRECT - option B: CHANGES(INFORMATION => APPEND_ONLY) returns only rows where 
      ACTION=INSERT AND the row hasn't been subsequently modified or deleted — truly net-new rows.
      WRONG - option C: APPEND_ONLY is a query modifier, not a table type restriction. 
      Any table can use APPEND_ONLY CHANGES.
      WRONG - option D: DEFAULT returns paired DELETE/INSERT for updates (not duplicates); 
      they're distinguishable via METADATA$ISUPDATE.
      WRONG - option E: APPEND_ONLY returns INSERT actions for net-new rows, not DELETE actions.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "176"
    question: "A developer accidentally creates a table with the wrong schema. They drop it immediately and try to create it with the correct schema. But then they realize the original data that was in the wrong-schema table is needed. What should they do?"
    their_options:
      option A: "The data is permanently lost since the table was dropped and recreated"
      option B: "UNDROP TABLE <table_name>; to restore the wrong-schema table, extract needed data, then drop again"
      option C: "SELECT * FROM <table_name> AT (OFFSET => -60) to query the dropped table's data"
      option D: "The new table automatically inherited the data from the dropped table"
    correct Answer: "option B: UNDROP TABLE <table_name>; to restore the wrong-schema table, extract needed data, then drop again"
    explanation: >
      CORRECT - option B: UNDROP TABLE restores the dropped (wrong-schema) table. However, a 
      table with the same name now exists (the new one). So the process is: 
      1. Rename the new table temporarily.
      2. UNDROP the original (wrong-schema) table.
      3. Extract needed data.
      4. Drop the restored table.
      5. Rename the new table back.
      WRONG - option A: Data is NOT lost; Time Travel preserves the dropped table's data within 
      the retention window.
      WRONG - option C: AT (OFFSET => ...) queries require the table to be accessible (either 
      active or within Time Travel, but referenced as a table object). Since the table is dropped 
      and a new one with the same name exists, you'd need UNDROP to access the original. 
      The query would target the NEW table's history, not the dropped one.
      WRONG - option D: New tables do not inherit data from previously dropped tables.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer

  - Question No: "177"
    question: "What Snowflake feature uses Time Travel internally and requires the source table to have adequate retention to prevent operational issues?"
    their_options:
      option A: "Materialized Views"
      option B: "Snowflake Streams"
      option C: "Automatic Clustering"
      option D: "Snowflake Tasks"
    correct Answer: "option B: Snowflake Streams"
    explanation: >
      CORRECT - option B: Snowflake Streams use Time Travel internally to maintain their offset 
      (the pointer to where the stream has consumed up to). If the table's Time Travel retention 
      expires before the stream is consumed, the stream becomes STALE. Therefore, tables backing 
      streams need sufficient DATA_RETENTION_TIME_IN_DAYS to prevent stale streams.
      WRONG - option A: Materialized Views don't use Time Travel internally for their refresh mechanism.
      WRONG - option C: Automatic Clustering rewrites micro-partitions and benefits from the 
      immutable storage model, but doesn't "use" Time Travel the way Streams do.
      WRONG - option D: Snowflake Tasks are scheduled queries and don't have a Time Travel 
      dependency.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "178"
    question: "What is the effect of setting DATA_RETENTION_TIME_IN_DAYS = 0 at the account level?"
    their_options:
      option A: "Time Travel is disabled for all objects across the entire account"
      option B: "This becomes the default for all new objects; existing objects retain their current settings"
      option C: "All existing Time Travel data across the account is immediately purged"
      option D: "Only new databases created after this change have Time Travel disabled"
    correct Answer: "option B: This becomes the default for all new objects; existing objects retain their current settings"
    explanation: >
      CORRECT - option B: Setting DATA_RETENTION_TIME_IN_DAYS = 0 at the account level changes 
      the default for new objects. Existing objects that have their own explicit settings or 
      inherited settings from database/schema levels are not immediately affected. However, 
      existing objects without explicit settings would now inherit 0 going forward.
      WRONG - option A: Objects with explicit retention settings are not affected by the account-level 
      change. Only objects relying on account-level inheritance are affected.
      WRONG - option C: Existing Time Travel data is not immediately purged by changing the 
      account default; purging only happens when individual object retention is reduced.
      WRONG - option D: All new objects (tables, schemas, databases) without explicit settings 
      would inherit the new account default, not just new databases.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect

  - Question No: "179"
    question: "What is the relationship between Snowflake's Time Travel and the INFORMATION_SCHEMA.OBJECT_PRIVILEGES view?"
    their_options:
      option A: "OBJECT_PRIVILEGES shows Time Travel-specific access grants"
      option B: "No direct relationship — Time Travel uses regular object privileges (SELECT, etc.); OBJECT_PRIVILEGES shows all privilege grants"
      option C: "You must grant TIME_TRAVEL_ACCESS privilege to roles that need AT/BEFORE query access"
      option D: "OBJECT_PRIVILEGES controls the maximum Time Travel retention per role"
    correct Answer: "option B: No direct relationship — Time Travel uses regular object privileges (SELECT, etc.); OBJECT_PRIVILEGES shows all privilege grants"
    explanation: >
      CORRECT - option B: Time Travel access is not a separate privilege type. It's controlled by 
      the standard table privileges (SELECT). INFORMATION_SCHEMA.OBJECT_PRIVILEGES shows all 
      privilege grants on objects but has no Time Travel-specific entries or controls.
      WRONG - option A: OBJECT_PRIVILEGES doesn't show Time Travel-specific grants because 
      no such specific grants exist.
      WRONG - option C: TIME_TRAVEL_ACCESS is not a real Snowflake privilege; regular SELECT 
      suffices for Time Travel queries.
      WRONG - option D: Retention is controlled by DATA_RETENTION_TIME_IN_DAYS, not by role-based 
      privileges.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "180"
    question: "How would you check Time Travel storage consumption for a specific table named ORDERS in the database SALES_DB?"
    their_options:
      option A: >
        "SELECT TABLE_NAME, TIME_TRAVEL_BYTES
        FROM SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS
        WHERE TABLE_NAME = 'ORDERS' AND TABLE_SCHEMA = 'PUBLIC'
        AND TABLE_CATALOG = 'SALES_DB';"
      option B: >
        "SELECT TIME_TRAVEL_BYTES FROM SALES_DB.INFORMATION_SCHEMA.TABLES
        WHERE TABLE_NAME = 'ORDERS';"
      option C: "SHOW TABLE STORAGE FOR ORDERS IN DATABASE SALES_DB;"
      option D: "SELECT TIME_TRAVEL_BYTES FROM SNOWFLAKE.ACCOUNT_USAGE.TIME_TRAVEL_METRICS WHERE TABLE = 'ORDERS';"
    correct Answer: "option A"
    explanation: >
      CORRECT - option A: SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS contains a TIME_TRAVEL_BYTES 
      column. Filtering on TABLE_NAME, TABLE_SCHEMA, and TABLE_CATALOG properly identifies the 
      specific table in the SALES_DB database.
      WRONG - option B: INFORMATION_SCHEMA.TABLES does not have a TIME_TRAVEL_BYTES column; 
      it only has RETENTION_TIME (the configuration value, not the actual bytes consumed).
      WRONG - option C: SHOW TABLE STORAGE is not valid Snowflake syntax.
      WRONG - option D: SNOWFLAKE.ACCOUNT_USAGE.TIME_TRAVEL_METRICS is not a real Snowflake view.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "181"
    question: "What happens to automatic clustering DML operations and Time Travel storage?"
    their_options:
      option A: "Automatic clustering operations bypass Time Travel to avoid storage overhead"
      option B: "Automatic clustering reclustering operations write new micro-partitions and retain old ones in Time Travel, increasing Time Travel storage"
      option C: "Automatic clustering has no effect on Time Travel storage"
      option D: "Automatic clustering reduces Time Travel storage by compacting partitions"
    correct Answer: "option B: Automatic clustering reclustering operations write new micro-partitions and retain old ones in Time Travel, increasing Time Travel storage"
    explanation: >
      CORRECT - option B: When automatic clustering rearranges micro-partitions (reclustering), 
      it's essentially a DML operation that writes new optimally clustered partitions and marks 
      the old ones for deletion. With Time Travel enabled, the old pre-reclustering partitions 
      are retained in the Time Travel window, potentially causing significant Time Travel storage 
      accumulation on highly clustered, volatile tables.
      WRONG - option A: Automatic clustering does NOT bypass Time Travel; it's treated like 
      any other DML that modifies micro-partitions.
      WRONG - option C: Reclustering has a direct impact on Time Travel storage.
      WRONG - option D: Reclustering increases Time Travel storage (more old partition versions 
      to retain), not decreases it.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "182"
    question: "A developer issues a MERGE statement that updates 500,000 rows, inserts 200,000 new rows, and deletes 100,000 rows. How does Time Travel handle this operation?"
    their_options:
      option A: "Only the deleted rows are stored in Time Travel"
      option B: "Only the updated rows' before-images and deleted rows are stored in Time Travel"
      option C: "All 800,000 rows are stored in Time Travel as a full copy"
      option D: "The 200,000 inserted rows are stored in Time Travel immediately"
    correct Answer: "option B: Only the updated rows' before-images and deleted rows are stored in Time Travel"
    explanation: >
      CORRECT - option B: Time Travel stores historical (before-image) versions of changed data. 
      For this MERGE:
      - 500,000 updated rows: old versions retained in Time Travel (before the update)
      - 100,000 deleted rows: deleted row versions retained in Time Travel
      - 200,000 inserted rows: these are NEW rows with no previous state to retain in Time Travel 
      (no before-image)
      WRONG - option A: Both deleted rows AND before-images of updated rows are retained.
      WRONG - option C: A full 800,000 row copy is not made; only changed/deleted row versions.
      WRONG - option D: Newly inserted rows don't generate Time Travel storage until they are 
      later modified or deleted.
      difficulty level: Hard
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "183"
    question: "Which of the following is TRUE about Time Travel in Snowflake Snowpark?"
    their_options:
      option A: "Snowpark does not support Time Travel"
      option B: "Snowpark DataFrame operations can reference historical tables using the same AT/BEFORE mechanism via SQL or Snowpark API"
      option C: "Snowpark Time Travel requires special SDK version"
      option D: "Snowpark Time Travel only works with Python, not Java or Scala"
    correct Answer: "option B: Snowpark DataFrame operations can reference historical tables using the same AT/BEFORE mechanism via SQL or Snowpark API"
    explanation: >
      CORRECT - option B: Snowpark can use Time Travel through SQL string queries 
      (session.sql("SELECT * FROM T AT (OFFSET => -3600)")) or through the Snowpark DataFrame 
      API which supports historical query options. The underlying mechanism is the same Snowflake 
      Time Travel feature.
      WRONG - option A: Snowpark fully supports Time Travel through both SQL and the API.
      WRONG - option C: No special SDK version is required; standard Snowpark SDK supports Time Travel.
      WRONG - option D: Snowpark supports Time Travel across all supported languages (Python, Java, Scala).
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer

  - Question No: "184"
    question: "What does 'point-in-time consistency' mean in the context of Time Travel joins between multiple tables?"
    their_options:
      option A: "All tables in a Time Travel join automatically use the same timestamp"
      option B: "Each table can independently reference its own historical point; consistency is the developer's responsibility when joining"
      option C: "Snowflake guarantees all joined tables reference the same transaction snapshot"
      option D: "Time Travel joins always use CURRENT_TIMESTAMP for consistency"
    correct Answer: "option B: Each table can independently reference its own historical point; consistency is the developer's responsibility when joining"
    explanation: >
      CORRECT - option B: When joining multiple tables with different AT/BEFORE clauses, Snowflake 
      executes each table reference at its specified historical point independently. There is no 
      automatic cross-table consistency enforcement — the developer must deliberately use the same 
      timestamps across tables when point-in-time consistency is needed.
      WRONG - option A: Tables in a join do NOT automatically use the same timestamp; each has 
      its own independent AT/BEFORE clause.
      WRONG - option C: Snowflake does not enforce cross-table transaction snapshots for Time 
      Travel joins; consistency is the developer's responsibility.
      WRONG - option D: Using CURRENT_TIMESTAMP would return current data, not historical.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "185"
    question: "What is the INFORMATION_SCHEMA.CHANGE_TRACKING_TABLES view used for?"
    their_options:
      option A: "Lists tables with Snowflake Streams consuming their changes"
      option B: "Lists tables that have CHANGE_TRACKING = TRUE enabled, which is required for CHANGES clause queries without a Stream"
      option C: "Shows Time Travel query history for each table"
      option D: "INFORMATION_SCHEMA.CHANGE_TRACKING_TABLES is not a valid Snowflake view"
    correct Answer: "option D: INFORMATION_SCHEMA.CHANGE_TRACKING_TABLES is not a valid Snowflake view"
    explanation: >
      CORRECT - option D: INFORMATION_SCHEMA.CHANGE_TRACKING_TABLES is not a standard Snowflake 
      view. Change tracking status for tables can be checked via SHOW TABLES (look for the 
      change_tracking column) or INFORMATION_SCHEMA.TABLES.
      WRONG - option A: Stream information is in SHOW STREAMS or INFORMATION_SCHEMA.STREAMS, 
      not in this (non-existent) view.
      WRONG - option B: While CHANGE_TRACKING = TRUE is a table property, the view name cited 
      here does not exist in Snowflake.
      WRONG - option C: Time Travel query history is in QUERY_HISTORY, not in a change tracking view.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer

  - Question No: "186"
    question: "How does Snowflake Time Travel handle a scenario where COPY INTO was used to load data that was later found to be incorrect?"
    their_options:
      option A: "UNDROP TABLE restores the pre-COPY state"
      option B: "Time Travel can show the pre-COPY state using BEFORE (STATEMENT => '<copy_query_id>'); you can then truncate and reload from the snapshot"
      option C: "COPY INTO cannot be reversed using Time Travel"
      option D: "You must re-run COPY INTO with the correct files to overwrite"
    correct Answer: "option B: Time Travel can show the pre-COPY state using BEFORE (STATEMENT => '<copy_query_id>'); you can then truncate and reload from the snapshot"
    explanation: >
      CORRECT - option B: COPY INTO is a DML-like operation that inserts rows. Like other DML, 
      its effect can be undone via Time Travel. Using BEFORE (STATEMENT => '<copy_query_id>') 
      gives you the pre-COPY state. You can then TRUNCATE and INSERT from the pre-COPY snapshot.
      WRONG - option A: UNDROP TABLE is for dropped tables, not for reversing a COPY INTO load.
      WRONG - option C: COPY INTO effects ARE reversible via Time Travel since it's tracked 
      as a DML operation.
      WRONG - option D: Re-running COPY INTO would add more rows (or be blocked by load deduplication), 
      not replace incorrect ones.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer

  - Question No: "187"
    question: "Which of the following CANNOT be used as the value for the STATEMENT parameter in a Time Travel BEFORE clause?"
    their_options:
      option A: "A query ID from a SELECT statement"
      option B: "A query ID from a DELETE statement"
      option C: "A query ID from a MERGE statement"
      option D: "All of the above can be used as STATEMENT values"
    correct Answer: "option A: A query ID from a SELECT statement"
    explanation: >
      CORRECT - option A: The STATEMENT parameter is designed for DML statement IDs (INSERT, 
      UPDATE, DELETE, MERGE, TRUNCATE, COPY INTO). Using a SELECT statement's query ID doesn't 
      meaningfully represent a data change point. While technically it may accept it, the practical 
      use is with DML statements that changed data.
      WRONG - option B: DELETE query IDs are the primary use case for BEFORE (STATEMENT).
      WRONG - option C: MERGE query IDs are commonly used with BEFORE (STATEMENT) for complex recovery.
      WRONG - option D: Not all statement types are equally meaningful; SELECT IDs are not useful 
      for data recovery purposes.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer

  - Question No: "188"
    question: "What is the role of Time Travel in Snowflake's support for data governance compliance requirements like GDPR's right to access historical data?"
    their_options:
      option A: "Time Travel provides auditable historical data states for compliance reporting"
      option B: "Time Travel automatically generates compliance reports"
      option C: "Time Travel conflicts with GDPR by retaining deleted data"
      option D: "Time Travel has no role in data governance"
    correct Answer: "option A: Time Travel provides auditable historical data states for compliance reporting"
    explanation: >
      CORRECT - option A: Time Travel enables organizations to query what data existed at specific 
      points in time — valuable for compliance audits, data lineage verification, and regulatory 
      reporting that requires point-in-time accuracy. It supports access to historical data as 
      required by various compliance frameworks.
      WRONG - option B: Time Travel doesn't automatically generate reports; it's a query capability 
      that compliance tooling can leverage.
      WRONG - option C: While Time Travel does retain deleted data (which requires careful GDPR 
      management — see Q137), it doesn't fundamentally "conflict" with GDPR. Proper configuration 
      (retention periods aligned with privacy policies) manages this.
      WRONG - option D: Time Travel is directly relevant to data governance, auditing, and compliance.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect, DataEngineer

  - Question No: "189"
    question: "A Snowflake account admin sets: ALTER ACCOUNT SET DATA_RETENTION_TIME_IN_DAYS = 14; A new table is created without specifying any retention. What is the table's effective retention?"
    their_options:
      option A: "1 day (Snowflake table default always overrides account setting)"
      option B: "14 days (inherits from account-level setting)"
      option C: "0 days (new tables always default to 0)"
      option D: "7 days (Snowflake average of account default and table default)"
    correct Answer: "option B: 14 days (inherits from account-level setting)"
    explanation: >
      CORRECT - option B: When a new table is created without an explicit DATA_RETENTION_TIME_IN_DAYS, 
      it inherits the setting from the account level (if no database or schema level override exists). 
      With the account set to 14 days, the new table defaults to 14 days.
      WRONG - option A: The 1-day table default is the Snowflake platform default when no other 
      setting exists. But an explicit account-level setting of 14 days overrides this default.
      WRONG - option C: 0 days is not the default for new tables; the account setting takes precedence.
      WRONG - option D: Snowflake doesn't average settings; specific settings override account defaults.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "190"
    question: "Which of the following best describes a scenario where you would prefer BEFORE (TIMESTAMP => ...) over BEFORE (STATEMENT => ...)?"
    their_options:
      option A: "When you have the exact query ID of the problematic DML"
      option B: "When you want the most precise microsecond accuracy before a specific operation"
      option C: "When you know the approximate time of data corruption but not the specific query ID"
      option D: "BEFORE (TIMESTAMP) and BEFORE (STATEMENT) are always interchangeable"
    correct Answer: "option C: When you know the approximate time of data corruption but not the specific query ID"
    explanation: >
      CORRECT - option C: BEFORE (TIMESTAMP => ...) is appropriate when you know approximately 
      when data was changed (e.g., "the bad batch ran around 2 AM on Tuesday") but don't have 
      the specific query ID. BEFORE (STATEMENT) requires knowing the exact query ID.
      WRONG - option A: When you have the exact query ID, BEFORE (STATEMENT) is more precise 
      and preferred.
      WRONG - option B: BEFORE (STATEMENT) provides microsecond precision relative to a specific 
      DML operation; BEFORE (TIMESTAMP) uses the provided timestamp which may be less precise.
      WRONG - option D: They are not always interchangeable — STATEMENT requires a query ID 
      while TIMESTAMP requires a time value; they serve different information scenarios.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "191"
    question: "What is the CHANGE_TRACKING table property and how does it relate to Time Travel?"
    their_options:
      option A: "CHANGE_TRACKING = TRUE enables Time Travel on a table"
      option B: "CHANGE_TRACKING = TRUE is required for the CHANGES clause in Time Travel queries and for Streams"
      option C: "CHANGE_TRACKING is automatically enabled when DATA_RETENTION_TIME_IN_DAYS > 0"
      option D: "CHANGE_TRACKING stores additional audit metadata not available through Time Travel alone"
    correct Answer: "option B: CHANGE_TRACKING = TRUE is required for the CHANGES clause in Time Travel queries and for Streams"
    explanation: >
      CORRECT - option B: To use the CHANGES clause (SELECT ... FROM T CHANGES(INFORMATION => ...)), 
      the source table must have CHANGE_TRACKING = TRUE set (ALTER TABLE T SET CHANGE_TRACKING = TRUE). 
      Streams also require this. Regular AT/BEFORE Time Travel queries work without CHANGE_TRACKING.
      WRONG - option A: CHANGE_TRACKING enables the CHANGES clause; Time Travel (AT/BEFORE) is 
      enabled by DATA_RETENTION_TIME_IN_DAYS > 0. They are related but distinct settings.
      WRONG - option C: CHANGE_TRACKING is NOT automatically enabled with retention; it must 
      be explicitly set.
      WRONG - option D: CHANGE_TRACKING enables the CHANGES clause capability, not separate 
      audit metadata beyond Time Travel.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "192"
    question: "A query is issued: SELECT * FROM MY_TABLE AT (OFFSET => -100); The table was last modified 200 seconds ago. What does this return?"
    their_options:
      option A: "Error: 100-second offset is too small for Time Travel"
      option B: "The state of the table 100 seconds ago, which is after the last modification (200s ago), so current data is returned"
      option C: "The state of the table exactly 100 seconds ago — which reflects modifications made before that point but not the 200-second-ago change"
      option D: "An empty result since no data existed 100 seconds ago according to the system"
    correct Answer: "option C: The state of the table exactly 100 seconds ago — which reflects modifications made before that point but not the 200-second-ago change"
    explanation: >
      CORRECT - option C: AT (OFFSET => -100) queries the table state 100 seconds ago. The 
      last modification was 200 seconds ago (before the 100-second point). So 100 seconds ago, 
      the table already reflected that 200-second-ago modification. Wait — actually this is 
      subtle: if modification was 200 seconds ago and we query 100 seconds ago, the modification 
      had already happened 200s ago and would be visible at the 100s-ago point.
      The state at -100s includes all changes made before -100s, including the one from -200s.
      WRONG - option A: There is no minimum offset for Time Travel.
      WRONG - option B: "After the last modification" is correct reasoning — since the modification 
      was 200s ago and we're looking 100s ago (more recent), the modification IS visible.
      WRONG - option D: Empty results don't occur for a valid offset within the retention window.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "193"
    question: "What minimum role privilege is required to run UNDROP SCHEMA on a recently dropped schema?"
    their_options:
      option A: "USAGE privilege on the database"
      option B: "CREATE SCHEMA privilege on the database containing the dropped schema"
      option C: "OWNERSHIP on the schema"
      option D: "SYSADMIN role"
    correct Answer: "option B: CREATE SCHEMA privilege on the database containing the dropped schema"
    explanation: >
      CORRECT - option B: UNDROP SCHEMA is analogous to creating a new schema (restoring one). 
      The required privilege is CREATE SCHEMA on the parent database. This allows the role to 
      restore the schema back into the database.
      WRONG - option A: USAGE privilege allows reading from a schema but not creating or restoring schemas.
      WRONG - option C: Ownership of a dropped schema cannot be held (it's dropped); the 
      privilege needed is on the parent database.
      WRONG - option D: SYSADMIN is a convenient system role but not the minimum requirement; 
      any role with CREATE SCHEMA on the database can UNDROP.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "194"
    question: "What is the effect of running ALTER TABLE MY_TABLE UNSET DATA_RETENTION_TIME_IN_DAYS?"
    their_options:
      option A: "Sets DATA_RETENTION_TIME_IN_DAYS to 0, disabling Time Travel"
      option B: "Removes the table-level setting, causing the table to inherit from the schema/database/account hierarchy"
      option C: "Drops and recreates the table with default retention"
      option D: "UNSET DATA_RETENTION_TIME_IN_DAYS is not valid Snowflake syntax"
    correct Answer: "option B: Removes the table-level setting, causing the table to inherit from the schema/database/account hierarchy"
    explanation: >
      CORRECT - option B: ALTER TABLE <name> UNSET DATA_RETENTION_TIME_IN_DAYS removes the 
      table-level explicit retention setting. The table then falls back to the inheritance 
      hierarchy — inheriting from schema, database, or account level.
      WRONG - option A: UNSET does not set to 0; it removes the explicit setting entirely.
      WRONG - option C: UNSET is a metadata operation; no data is moved or tables recreated.
      WRONG - option D: UNSET DATA_RETENTION_TIME_IN_DAYS IS valid Snowflake syntax for 
      clearing an explicit parameter setting.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "195"
    question: "A Snowflake architect is designing a data vault. Hub tables have low DML frequency, Link tables have moderate DML, and Satellite tables have very high DML frequency. Which Time Travel configuration strategy is most cost-effective?"
    their_options:
      option A: "Set 90 days on all tables uniformly"
      option B: "Set 0 days on all tables to eliminate Time Travel costs"
      option C: "Set higher retention (e.g., 14-30 days) for Hub/Link tables and lower retention (e.g., 3-7 days) for high-DML Satellite tables"
      option D: "Use transient tables for all data vault tables"
    correct Answer: "option C: Set higher retention for Hub/Link and lower for Satellite tables"
    explanation: >
      CORRECT - option C: This is the right approach based on DML frequency and recovery value:
      - Hub/Link tables: Low DML, critical reference data → higher retention is cost-effective.
      - Satellite tables: Very high DML generates huge Time Travel storage → shorter retention 
      is cost-effective while still meeting recovery SLAs. This tiered approach optimizes costs.
      WRONG - option A: 90 days on high-DML Satellite tables would create extreme storage costs.
      WRONG - option B: Eliminating Time Travel entirely sacrifices all recovery capability.
      WRONG - option C: Using transient tables eliminates Fail-safe, reducing protection for 
      critical Hub and Link tables which may need more protection.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect

  - Question No: "196"
    question: "What is the SNOWFLAKE.ACCOUNT_USAGE data latency for TABLE_STORAGE_METRICS?"
    their_options:
      option A: "Real-time (no latency)"
      option B: "Up to 2 hours latency"
      option C: "Up to 3 hours latency"
      option D: "Up to 45 minutes latency"
    correct Answer: "option C: Up to 3 hours latency"
    explanation: >
      CORRECT - option C: SNOWFLAKE.ACCOUNT_USAGE views have varying latency. 
      TABLE_STORAGE_METRICS has a latency of up to 3 hours — meaning storage metrics including 
      TIME_TRAVEL_BYTES may be delayed by up to 3 hours from the actual storage changes.
      WRONG - option A: ACCOUNT_USAGE views are not real-time; they have documented latency.
      WRONG - option B: 2 hours is the latency for some other ACCOUNT_USAGE views (like QUERY_HISTORY); 
      TABLE_STORAGE_METRICS has up to 3 hours.
      WRONG - option D: 45 minutes is not the documented latency for TABLE_STORAGE_METRICS.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "197"
    question: "Can you use Time Travel on a Snowflake table that is the target of a Snowpipe continuous data loading?"
    their_options:
      option A: "No, Snowpipe disables Time Travel on target tables"
      option B: "Yes, Time Travel works normally on Snowpipe target tables — each micro-batch insert is tracked"
      option C: "Yes, but only with a maximum 1-day retention regardless of edition"
      option D: "Snowpipe tables require a special TIME_TRAVEL parameter to enable it"
    correct Answer: "option B: Yes, Time Travel works normally on Snowpipe target tables — each micro-batch insert is tracked"
    explanation: >
      CORRECT - option B: Snowpipe loads data via INSERT DML operations. The target table is a 
      regular Snowflake table, and Time Travel works on it like any other table. Each micro-batch 
      loaded by Snowpipe creates traceable DML with a query ID usable in BEFORE (STATEMENT => ...).
      WRONG - option A: Snowpipe doesn't disable Time Travel; it's a loading mechanism, not a 
      table property modifier.
      WRONG - option C: There's no special 1-day limit for Snowpipe tables; retention is 
      determined by the table's DATA_RETENTION_TIME_IN_DAYS setting and edition.
      WRONG - option D: No special TIME_TRAVEL parameter is needed beyond standard 
      DATA_RETENTION_TIME_IN_DAYS configuration.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer

  - Question No: "198"
    question: "Which Time Travel AT sub-clause type offers the most human-readable and business-friendly way to specify historical queries?"
    their_options:
      option A: "OFFSET — because negative numbers are intuitive"
      option B: "STATEMENT — because query IDs clearly identify operations"
      option C: "TIMESTAMP — because it uses recognizable dates and times"
      option D: "All three sub-clauses are equally readable"
    correct Answer: "option C: TIMESTAMP — because it uses recognizable dates and times"
    explanation: >
      CORRECT - option C: TIMESTAMP uses familiar date/time values like 
      '2024-06-15 09:00:00'::TIMESTAMP_LTZ, which clearly communicates to business users and 
      developers when the query references. Business analysts can intuitively understand 
      "as of June 15th at 9 AM" vs. negative second offsets or opaque query IDs.
      WRONG - option A: Negative second offsets (-86400 for 1 day ago) require mental math 
      to interpret; not immediately readable without calculation.
      WRONG - option B: Query IDs (like '018abc-def-1234') are opaque identifiers with no 
      human-readable meaning without context.
      WRONG - option D: The sub-clauses have different readability characteristics; TIMESTAMP 
      is most directly readable.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataAnalyst, DataEngineer

  - Question No: "199"
    question: "What are the TWO key prerequisites for using the CHANGES clause on a Snowflake table? (SELECT TWO)"
    their_options:
      option A: "The table must have CHANGE_TRACKING = TRUE enabled"
      option B: "The table must be a permanent table (not transient)"
      option C: "The table must have DATA_RETENTION_TIME_IN_DAYS > 0"
      option D: "The user must have the SYSADMIN role"
      option E: "The table must have a primary key defined"
    correct Answer: "option A: CHANGE_TRACKING = TRUE, option C: DATA_RETENTION_TIME_IN_DAYS > 0"
    explanation: >
      CORRECT - option A (CHANGE_TRACKING = TRUE): Required for the CHANGES clause. Must be 
      explicitly enabled: ALTER TABLE T SET CHANGE_TRACKING = TRUE.
      CORRECT - option C (DATA_RETENTION_TIME_IN_DAYS > 0): The CHANGES clause relies on Time 
      Travel to access the historical change data. Without retention > 0, no historical changes 
      are available.
      WRONG - option B (permanent table): Transient tables also support CHANGE_TRACKING and 
      CHANGES clause with their 0-1 day retention.
      WRONG - option D (SYSADMIN): SYSADMIN is not required; SELECT privilege on the table is sufficient.
      WRONG - option E (primary key): No primary key is required for CHANGE_TRACKING or the 
      CHANGES clause.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "200"
    question: "A company's Snowflake account manager tells them they can optimize costs by 'right-sizing Time Travel'. What does this mean in practice?"
    their_options:
      option A: "Reducing the virtual warehouse size to save compute costs on Time Travel queries"
      option B: "Setting DATA_RETENTION_TIME_IN_DAYS to match actual business recovery requirements per table, rather than using the maximum retention everywhere"
      option C: "Enabling Time Travel only on Enterprise Edition to get better storage rates"
      option D: "Compressing Time Travel data to reduce storage footprint"
    correct Answer: "option B: Setting DATA_RETENTION_TIME_IN_DAYS to match actual business recovery requirements per table, rather than using the maximum retention everywhere"
    explanation: >
      CORRECT - option B: 'Right-sizing Time Travel' means assigning retention periods based on 
      actual recovery SLAs for each table:
      - High-value, low-DML tables: higher retention
      - High-DML, lower-criticality tables: shorter retention
      - Staging/temp tables: 1 day or transient
      This prevents paying for 90 days of Time Travel on every table when only 7 days may be needed.
      WRONG - option A: Compute costs from Time Travel queries are minimal; storage cost 
      optimization is the primary concern.
      WRONG - option C: Edition choice affects the maximum available retention, not the per-GB 
      storage rate for Time Travel data.
      WRONG - option D: Snowflake automatically compresses all data (active and Time Travel); 
      users can't configure separate compression settings for Time Travel.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "201"
    question: "What is the correct way to enable CHANGE_TRACKING on an existing table ORDERS?"
    their_options:
      option A: "CREATE TABLE ORDERS (... CHANGE_TRACKING = TRUE);"
      option B: "ALTER TABLE ORDERS SET CHANGE_TRACKING = TRUE;"
      option C: "ENABLE CHANGE_TRACKING ON TABLE ORDERS;"
      option D: "ALTER TABLE ORDERS ADD CHANGE_TRACKING;"
    correct Answer: "option B: ALTER TABLE ORDERS SET CHANGE_TRACKING = TRUE;"
    explanation: >
      CORRECT - option B: To enable change tracking on an existing table, use 
      ALTER TABLE <name> SET CHANGE_TRACKING = TRUE. For new tables it can be added as a 
      table property in CREATE TABLE.
      WRONG - option A: While CHANGE_TRACKING = TRUE can be in a CREATE TABLE, it's not 
      inside the column definition parentheses — it's a table property. Also, this is 
      for a NEW table, not enabling on EXISTING.
      WRONG - option C: ENABLE CHANGE_TRACKING is not valid Snowflake SQL syntax.
      WRONG - option D: ALTER TABLE ADD CHANGE_TRACKING is not valid Snowflake syntax.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer

  - Question No: "202"
    question: "What SQL clause do you add to a CREATE TABLE statement to create a Transient table with 1 day of Time Travel?"
    their_options:
      option A: "CREATE TRANSIENT TABLE T (id INT) TIME_TRAVEL = 1;"
      option B: "CREATE TABLE T (id INT) TYPE = TRANSIENT DATA_RETENTION_TIME_IN_DAYS = 1;"
      option C: "CREATE TRANSIENT TABLE T (id INT) DATA_RETENTION_TIME_IN_DAYS = 1;"
      option D: "CREATE TABLE T (id INT) TRANSIENT DATA_RETENTION_TIME_IN_DAYS = 1;"
    correct Answer: "option C: CREATE TRANSIENT TABLE T (id INT) DATA_RETENTION_TIME_IN_DAYS = 1;"
    explanation: >
      CORRECT - option C: The correct Snowflake syntax for creating a transient table with 
      explicit retention uses the TRANSIENT keyword directly in CREATE TABLE and DATA_RETENTION_TIME_IN_DAYS 
      as a table property.
      WRONG - option A: TIME_TRAVEL = 1 is not a valid Snowflake property name.
      WRONG - option B: TYPE = TRANSIENT is not valid Snowflake DDL syntax; the TRANSIENT 
      keyword precedes TABLE.
      WRONG - option D: TRANSIENT after TABLE name is not valid syntax; it must precede TABLE.
    difficulty level: Easy
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer

  - Question No: "203"
    question: "Which of the following describes a scenario where Time Travel provides ZERO benefit?"
    their_options:
      option A: "A row was accidentally deleted 2 hours ago and the table has 7-day retention"
      option B: "A table was accidentally dropped 3 days ago and the table has 7-day retention"
      option C: "Ransomware encrypted all data in the Snowflake storage layer"
      option D: "A developer truncated a table 1 hour ago with 1-day retention"
    correct Answer: "option C: Ransomware encrypted all data in the Snowflake storage layer"
    explanation: >
      CORRECT - option C: Time Travel data is stored in the same cloud storage infrastructure 
      as active data. A storage-layer attack that compromises the cloud storage itself (like 
      theoretical ransomware at the storage level) could affect both active AND Time Travel data. 
      Time Travel provides zero benefit when the underlying storage is compromised because the 
      historical micro-partitions are in the same storage.
      WRONG - option A: Standard Time Travel recovery — fully recoverable using BEFORE + INSERT.
      WRONG - option B: Standard UNDROP TABLE scenario — fully recoverable within retention window.
      WRONG - option D: Standard Time Travel recovery from TRUNCATE — recoverable using 
      BEFORE (STATEMENT) within retention window.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect

  - Question No: "204"
    question: "What is the Snowflake-recommended approach for creating a 'point-in-time backup' of a table before a risky data migration?"
    their_options:
      option A: "Export data to an external S3 bucket before migration"
      option B: "Run a full COPY INTO to backup files in a stage"
      option C: "Use CREATE TABLE BACKUP_TABLE CLONE SOURCE_TABLE; before the migration"
      option D: "Trust Time Travel — no manual backup needed since Time Travel handles it"
    correct Answer: "option C: Use CREATE TABLE BACKUP_TABLE CLONE SOURCE_TABLE; before the migration"
    explanation: >
      CORRECT - option C: Creating a zero-copy clone before a migration is the Snowflake-recommended 
      best practice. It provides an instant, storage-efficient safety net that doesn't require 
      data movement and can be quickly swapped back using ALTER TABLE SWAP WITH if the migration fails.
      WRONG - option A: External exports work but require data movement, time, and storage costs. 
      Less efficient than zero-copy cloning.
      WRONG - option B: COPY INTO creates files in a stage (external) rather than a queryable 
      Snowflake table; less convenient for rollback.
      WRONG - option D: While Time Travel can recover, it requires knowing the correct query IDs 
      or timestamps. An explicit clone provides a clear, labeled recovery point that's easier 
      to manage in a migration context.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "205"
    question: "What happens to Time Travel data when a Snowflake account is migrated to a different cloud region?"
    their_options:
      option A: "Time Travel data is migrated along with active data"
      option B: "Time Travel data is NOT migrated; only active data is moved to the new region"
      option C: "Time Travel data is doubled during migration for redundancy"
      option D: "Time Travel is paused during migration and resumes after"
    correct Answer: "option B: Time Travel data is NOT migrated; only active data is moved to the new region"
    explanation: >
      CORRECT - option B: In a cross-region Snowflake account migration, typically only active 
      data is migrated. Time Travel historical data is generally not included in cross-region 
      migrations due to the volume of data and the operational complexity. The Time Travel 
      history effectively starts fresh in the new region from the migration point.
      WRONG - option A: Migrating full Time Travel history across regions would be impractical 
      due to data volume; typically only active data is migrated.
      WRONG - option C: Data is not doubled for redundancy in standard migrations.
      WRONG - option D: Time Travel doesn't have a "pause" state; it either exists or doesn't.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: Architect

  - Question No: "206"
    question: "A Snowflake table has both a Regular Stream and an Append-Only Stream. If the underlying table's retention is set to 0, what happens to BOTH streams?"
    their_options:
      option A: "Both streams continue working because streams have independent retention"
      option B: "Both streams become STALE immediately"
      option C: "Only the Regular Stream becomes stale; Append-Only Stream continues working"
      option D: "Only the Append-Only Stream becomes stale; Regular Stream is unaffected"
    correct Answer: "option B: Both streams become STALE immediately"
    explanation: >
      CORRECT - option B: Both Regular (delta) Streams and Append-Only Streams rely on the 
      underlying table's Time Travel to maintain their offset. Setting retention to 0 immediately 
      purges Time Travel data, causing both stream types to become STALE simultaneously.
      WRONG - option A: Neither stream type has independent retention from the underlying table.
      WRONG - option C: Both stream types depend on Time Travel; both become stale.
      WRONG - option D: Both stream types depend on Time Travel; both become stale.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "207"
    question: "What is the maximum OFFSET value (in absolute seconds) allowed for a Time Travel AT query on a table with DATA_RETENTION_TIME_IN_DAYS = 7?"
    their_options:
      option A: "-7 (7 seconds)"
      option B: "-604800 (7 days in seconds)"
      option C: "-86400 (1 day in seconds)"
      option D: "There is no maximum; any offset is valid"
    correct Answer: "option B: -604800 (7 days in seconds)"
    explanation: >
      CORRECT - option B: The maximum lookback for AT (OFFSET) is determined by 
      DATA_RETENTION_TIME_IN_DAYS. For 7 days: 7 × 24 × 3600 = 604,800 seconds. 
      The most negative valid offset is -604800. Any offset more negative than this 
      (e.g., -700000) would exceed the retention window and return an error.
      WRONG - option A: -7 seconds is a very short offset (7 seconds ago), not the maximum lookback.
      WRONG - option C: -86400 is 1 day, which is within the 7-day window but not the maximum.
      WRONG - option D: The maximum is bounded by the DATA_RETENTION_TIME_IN_DAYS window.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, DataAnalyst

  - Question No: "208"
    question: "When you run CREATE TABLE T2 CLONE T1, does T2 start with any Time Travel history?"
    their_options:
      option A: "Yes, T2 inherits all of T1's Time Travel history"
      option B: "No, T2's Time Travel history starts at the moment of clone creation"
      option C: "Yes, T2 inherits up to 7 days of T1's history"
      option D: "T2 has no Time Travel (clones cannot have Time Travel)"
    correct Answer: "option B: No, T2's Time Travel history starts at the moment of clone creation"
    explanation: >
      CORRECT - option B: A zero-copy clone creates a new table object with its own independent 
      Time Travel history beginning at clone creation time. The clone shares the underlying 
      micro-partitions with the source but does NOT inherit the source's historical Time Travel data.
      WRONG - option A: Clones do not inherit historical Time Travel from the source table.
      WRONG - option C: No historical inheritance occurs; the clone starts fresh.
      WRONG - option D: Clones are regular tables that fully support Time Travel; they just 
      start their history from creation.
    difficulty level: Medium
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "209"
    question: "A data engineer needs to ensure that a daily ETL process can ALWAYS recover the previous day's data. What minimum DATA_RETENTION_TIME_IN_DAYS setting and table type should be used?"
    their_options:
      option A: "DATA_RETENTION_TIME_IN_DAYS = 0, Permanent Table"
      option B: "DATA_RETENTION_TIME_IN_DAYS = 1, Transient Table"
      option C: "DATA_RETENTION_TIME_IN_DAYS = 1, Permanent Table"
      option D: "DATA_RETENTION_TIME_IN_DAYS = 2, Permanent Table"
    correct Answer: "option D: DATA_RETENTION_TIME_IN_DAYS = 2, Permanent Table"
    explanation: >
      CORRECT - option D: To guarantee recovery of "previous day's data," you need at least 2 days 
      of Time Travel. If the ETL runs at midnight and a problem is discovered the next evening 
      (nearly 24 hours later), with only 1-day retention you'd be at the edge of the window. 
      2 days provides a safety buffer. The table should be permanent (with Fail-safe) for 
      production data protection.
      WRONG - option A: 0 days disables Time Travel entirely — no recovery possible.
      WRONG - option B: 1-day Transient provides minimal safety margin and no Fail-safe; 
      risky for production ETL data.
      WRONG - option C: 1-day retention is exactly the window needed but provides no buffer 
      if discovery/recovery happens near the boundary. 2 days is safer.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect

  - Question No: "210"
    question: "What does the STALE_AFTER column in SHOW STREAMS output represent?"
    their_options:
      option A: "The date the stream was created"
      option B: "The timestamp after which the stream will become stale if not consumed, based on the source table's retention"
      option C: "The last time the stream was consumed"
      option D: "The expiry date of the stream object"
    correct Answer: "option B: The timestamp after which the stream will become stale if not consumed, based on the source table's retention"
    explanation: >
      CORRECT - option B: STALE_AFTER shows the deadline timestamp for the stream — if the stream 
      is not consumed before this time, the Time Travel data it references will expire and the 
      stream will become stale. It's calculated based on the source table's DATA_RETENTION_TIME_IN_DAYS 
      and the stream's current offset timestamp.
      WRONG - option A: Creation date is in the CREATED_ON column, not STALE_AFTER.
      WRONG - option C: Last consumption time would be tracked internally but isn't the STALE_AFTER column.
      WRONG - option D: Streams don't have a lifecycle expiry separate from the staleness concept; 
      they remain as objects until explicitly dropped.
    difficulty level: Hard
    topic: "Data Protection"
    sub topic: "Time Travel"
    exam: DataEngineer, Architect
