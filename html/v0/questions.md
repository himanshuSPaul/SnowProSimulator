# Snowflake Certification — Question Bank

> **Knowledge base for SnowQuiz app.**  
> Topic: Snowflake Architecture | Exams: Core · Architect · Data Engineer  
> Total Questions: 173

---

## How to add questions

1. Copy any existing row as a template.
2. Fill in all columns — pipe `|` characters inside cells must be escaped as `\|`.
3. `Answer` must be a single letter: `A`, `B`, `C`, or `D`.
4. `Level` must be one of: `Easy`, `Medium`, `Hard`.
5. Save the file, drag it into the app — done.

---

| No | Question | Option A | Option B | Option C | Option D | Answer | Explanation | Level | Exam | Topic |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | Snowflake is best described as which type of cloud service? | IaaS (Infrastructure as a Service) | PaaS (Platform as a Service) | SaaS (Software as a Service) | FaaS (Function as a Service) | C | Snowflake is a SaaS platform — customers use it via a web interface or connectors without managing any underlying infrastructure, servers, OS, or software. Snowflake handles all patching, upgrades, and scaling automatically. | Easy | Core | Architecture Overview |
| 2 | Which THREE cloud providers does Snowflake natively support? | AWS, Azure, GCP | AWS, Azure, Oracle Cloud | AWS, GCP, IBM Cloud | Azure, GCP, Alibaba Cloud | A | Snowflake runs natively on AWS (Amazon Web Services), Microsoft Azure, and GCP (Google Cloud Platform). It does not natively run on Oracle Cloud, IBM Cloud, or Alibaba Cloud. | Easy | Core | Architecture Overview |
| 3 | What is the key architectural innovation that distinguishes Snowflake from traditional databases? | Columnar storage format | Separation of storage and compute | In-memory processing | Shared-nothing architecture | B | While columnar storage is a feature, the defining innovation is the complete separation of storage and compute. This allows each to scale independently — more processing power without buying more storage, and vice versa. | Easy | Core | Architecture Overview |
| 4 | How many layers does Snowflake's architecture consist of? | 2 | 3 | 4 | 5 | B | Snowflake's architecture has exactly THREE layers: (1) Database Storage, (2) Query Processing / Compute (Virtual Warehouses), and (3) Cloud Services. Each layer is independently scalable and managed. | Easy | Core | Architecture Overview |
| 5 | Which Snowflake architecture layer handles query optimization and authentication? | Database Storage | Virtual Warehouses | Cloud Services | Metadata Layer | C | The Cloud Services layer is the intelligence layer. It handles authentication, authorization (RBAC), query parsing and optimization, metadata management, transaction management, and infrastructure coordination. It runs 24/7 without user provisioning. | Easy | Core | Architecture Overview |
| 6 | What format does Snowflake use to store table data internally? | Row-based format | JSON format | Columnar (column-based) format | Parquet files you can access directly | C | Snowflake stores all data in a proprietary columnar format. Each column is stored separately, enabling efficient reads (only needed columns are scanned), superior compression (similar values compress better), and effective partition pruning. | Easy | Core | Architecture Overview |
| 7 | A company migrates from an on-premises Oracle database to Snowflake. The DBA wants to manually tune indexes, manage buffer pools, and configure tablespaces. What should the DBA expect? | All these operations are available in Snowflake with equivalent commands | These concepts don't exist in Snowflake — the Cloud Services layer handles all optimization automatically | Only indexes need to be migrated; buffer pools and tablespaces are auto-managed | The DBA must use Snowpark to implement custom optimization logic | B | Snowflake has no traditional indexes, buffer pools, or tablespaces. The Cloud Services layer automatically handles query optimization using micro-partition metadata (min/max statistics). DBAs cannot and do not need to manually tune these low-level storage parameters. | Medium | Core | Architecture Overview |
| 8 | Multiple virtual warehouses are running simultaneously against the same Snowflake database. What is TRUE about data access? | Each warehouse has its own private copy of the data to prevent conflicts | All warehouses read from the same centralized storage with no data locking between them | Warehouses must take turns reading data to prevent corruption | Only one warehouse can read a table at a time; others must queue | B | This is a core Snowflake differentiator. All virtual warehouses share centralized storage. Multiple warehouses can read the same tables simultaneously without any locking or contention because the storage layer is completely decoupled from compute. This enables true multi-workload isolation. | Medium | Core | Architecture Overview |
| 9 | Which layer in Snowflake's architecture is billed per second of active use? | Cloud Services | Database Storage | Query Processing (Virtual Warehouses) | Metadata Layer | C | Virtual Warehouses (Query Processing layer) are billed per second when actively running, with a 60-second minimum per start. Storage is billed per TB/month continuously. Cloud Services are free up to 10% of compute usage. | Medium | Core | Architecture Overview |
| 10 | A Snowflake account's daily metrics show: Compute credits = $0 (all warehouses suspended all day), Storage = $50, Cloud Services = $15. What is the total bill? | $65 (all charges apply) | $50 (storage only; Cloud Services free when no compute used) | $65 but with a $15 Cloud Services overage beyond the 10% threshold | $50 storage + $15 Cloud Services = $65, but the 10% threshold doesn't apply when compute is $0 | D | The 10% free threshold for Cloud Services is calculated as 10% of daily compute credits. With $0 compute, the free threshold is $0, meaning ALL Cloud Services usage is billable. Total = $50 (storage) + $15 (Cloud Services) = $65. This is a critical edge case — low or zero compute usage means Cloud Services can become a significant cost. | Hard | Core | Architecture Overview |
| 11 | An organization uses Snowflake on AWS us-east-1. They want to add Snowflake capacity in Europe. What must they do? | Create additional virtual warehouses — they can serve any geography | Provision a new Snowflake account in an EU region | Enable geo-distribution in their existing account settings | Use Snowflake's Global Processing feature to route EU queries | B | Each Snowflake account exists within a single cloud provider and region. To have Snowflake running in Europe, you must create a new account in a European region. Organizations (the top-level grouping) can manage multiple accounts across regions and clouds. | Easy | Architect | Architecture Overview |
| 12 | An architect is designing a disaster-tolerant Snowflake system. They want to ensure data survives a complete cloud region failure. Which architecture addresses this? | Multi-cluster warehouse with 10 maximum clusters in the primary region | Database replication to a secondary Snowflake account in a different region with failover enabled | Enabling Fail-Safe on all critical tables (7-day retention) | Setting DATA_RETENTION_TIME_IN_DAYS = 90 on all databases | B | Only cross-region database replication with failover capability can survive a full regional outage. Multi-cluster warehouses provide concurrency in the same region. Fail-Safe is a local disaster recovery feature, not cross-region. Time Travel cannot help if the entire region is unavailable. | Medium | Architect | Architecture Overview |
| 13 | A data pipeline reads from Snowflake every 10 seconds to check for new data using a scheduled script. What is the performance impact? | None — Snowflake queries are always free for metadata operations | Each query starts the virtual warehouse if auto-resume is on, potentially consuming significant credits for many 60-second minimum billing periods | Snowflake automatically batches rapid queries to minimize cost | The 10-second polling triggers the Result Cache, making most calls free | B | Rapid polling causes the warehouse to start (60-second minimum billing each time), execute the query, and potentially suspend — each cycle wastes credits. Better patterns: Snowflake Streams + Tasks, Snowpipe event triggers, or SYSTEM$STREAM_HAS_DATA checks to avoid unnecessary warehouse starts. | Medium | Data Engineer | Architecture Overview |
| 14 | What is the approximate uncompressed size of a Snowflake micro-partition? | 1 MB | 16 MB | 256 MB | 1 GB | B | Snowflake micro-partitions are approximately 16 MB of uncompressed data (50–500 MB compressed). This relatively small size enables fine-grained data pruning — Snowflake can skip many partitions when answering selective queries. | Easy | Core | Micro-Partitions |
| 15 | Can you manually create or manage micro-partitions in Snowflake? | Yes, using the CREATE PARTITION command | Yes, using ALTER TABLE ... ADD PARTITION | No — Snowflake creates and manages all micro-partitions automatically | Yes, but only with ACCOUNTADMIN privileges | C | Micro-partitions are fully managed by Snowflake. Users have no direct control over their creation, merging, or splitting. Snowflake automatically partitions data during loading and reorganizes partitions when clustering keys are defined. | Easy | Core | Micro-Partitions |
| 16 | What metadata does Snowflake store for each column in every micro-partition? | Row hashes and checksums only | Minimum value, maximum value, null count, and distinct count | Only the column names and data types | Full column statistics including histograms and percentiles | B | Snowflake stores per-column metadata for each micro-partition: minimum value, maximum value, null count, and row count. This metadata enables partition pruning — Snowflake can determine whether a partition could possibly contain rows matching a WHERE clause without scanning the actual data. | Easy | Core | Micro-Partitions |
| 17 | Micro-partitions in Snowflake are described as 'immutable.' What does this mean? | They cannot be queried once written | They are never modified in-place; DML operations create new micro-partitions instead | They cannot be deleted, only archived | They are read-only after the table is clustered | B | Immutable means micro-partitions are never updated or overwritten. When a row is updated, Snowflake marks the old partition as deleted and creates a new partition with the updated data. This immutability is what enables Time Travel and zero-copy cloning. | Easy | Core | Micro-Partitions |
| 18 | A table has 500 micro-partitions. A query WHERE order_date = '2024-06-15' runs on the table. Snowflake scans only 12 micro-partitions. What process eliminated the other 488 partitions? | Table statistics sampling | Micro-partition pruning (partition elimination) using min/max metadata | Index scan using the order_date column index | Bloom filter applied to the order_date column | B | Micro-partition pruning (also called partition elimination) uses the per-partition min/max metadata to skip partitions that cannot contain the queried value. Any partition where max_date < '2024-06-15' OR min_date > '2024-06-15' is entirely skipped. Only 12 partitions had ranges overlapping June 15. | Medium | Core | Micro-Partitions |
| 19 | A developer loads data into a table with no clustering key. In what order are micro-partitions typically organized? | By primary key value, ascending | Alphabetically by the first column | By natural insertion/load order | Randomly, with no guaranteed order | C | Without a clustering key, Snowflake organizes micro-partitions by the natural ingestion order — the order rows were loaded. This often correlates with time (if data is loaded chronologically), but there's no guarantee of any particular sort order based on column values. | Medium | Core | Micro-Partitions |
| 20 | A clustered table on column REGION has 1,000 micro-partitions. Most partitions contain data for a single region. A query WHERE region = 'APAC' scans only 80 partitions. An unclustered version of the same table would scan all 1,000 partitions. What does the difference demonstrate? | Clustering key creates a traditional B-tree index | Clustering reduces partition overlap, dramatically improving pruning effectiveness | Clustering compresses APAC data into fewer micro-partitions | Clustering enables Snowflake to cache APAC data preferentially | B | Clustering keys co-locate similar values into the same micro-partitions, reducing value range overlap between partitions. This makes min/max pruning far more effective — instead of every partition potentially containing any region, partitions cluster around specific regions, enabling 92% fewer partitions to be scanned. | Medium | Core | Micro-Partitions |
| 21 | A table with columns: ID, ORDER_DATE, CUSTOMER_ID, AMOUNT. Data is loaded in ORDER_DATE order. Queries primarily filter on CUSTOMER_ID. The table has poor pruning efficiency on CUSTOMER_ID queries. Why? | The table needs more micro-partitions to improve pruning | Since data is organized by ORDER_DATE, CUSTOMER_ID values are scattered across many partitions — any given CUSTOMER_ID appears in partitions throughout the table, so min/max pruning on CUSTOMER_ID cannot eliminate many partitions | CUSTOMER_ID requires a unique index for efficient filtering | Pruning only works on date columns in Snowflake | B | Because data is sorted by ORDER_DATE at load time, any single customer's orders are spread across many partitions (one per load date). The min/max CUSTOMER_ID range of each partition spans the entire CUSTOMER_ID range, so no partition can be pruned for a CUSTOMER_ID filter. Defining a clustering key on CUSTOMER_ID would reorganize the data to co-locate each customer's rows. | Hard | Core | Micro-Partitions |
| 22 | An architect reviews a Query Profile and sees 'Partitions Scanned: 10,000 of 10,000 (100%)' for a selective query. What is the MOST likely root cause? | The query has no WHERE clause | The table's clustering depth is too high for the query's filter column | The virtual warehouse is too small to perform pruning | The Result Cache was bypassed | B | Scanning 100% of partitions for a selective query indicates that pruning is completely ineffective. This typically means the filter column's values are scattered evenly across all partitions (no correlation between the filter column and the data's physical organization). The solution is to define a clustering key on the frequently filtered column. | Medium | Architect | Micro-Partitions |
| 23 | A Data Engineer loads data in random GUID order into a table that is queried heavily by DATE_CREATED. After loading 10 billion rows, query performance is very poor. They define a CLUSTER BY (DATE_CREATED) clustering key. What happens next? | All existing micro-partitions are immediately reorganized — the table is instantly optimized | A background process (Automatic Clustering service) incrementally reorganizes micro-partitions over time, with no user intervention required | The engineer must run ALTER TABLE ... RECLUSTER to trigger the reorganization | A full table rebuild is required — the table must be recreated from scratch | B | When a clustering key is defined, Snowflake's Automatic Clustering service runs in the background (consuming credits from a dedicated internal warehouse) to incrementally reorganize micro-partitions so that rows with similar DATE_CREATED values are co-located. This happens automatically without user action, though it may take time to complete for very large tables. | Hard | Data Engineer | Micro-Partitions |
| 24 | What happens to a virtual warehouse's compute cost when it is suspended? | Cost drops by 50% | Cost continues at a reduced standby rate | Cost drops to $0 for compute | Cost is transferred to the Cloud Services layer | C | When a virtual warehouse is suspended, it stops consuming any compute credits — cost is exactly $0 for compute. Storage costs continue regardless. This is a major cost-saving feature: suspend warehouses when not in use. | Easy | Core | Virtual Warehouses |
| 25 | What is the minimum billing duration each time a virtual warehouse starts? | 1 second | 10 seconds | 60 seconds | 5 minutes | C | Snowflake bills a minimum of 60 seconds each time a virtual warehouse starts. If a warehouse runs for 30 seconds, you are still billed for 60 seconds. If it runs for 90 seconds, you are billed for 90 seconds. Design auto-suspend windows to be cost-efficient given this minimum. | Easy | Core | Virtual Warehouses |
| 26 | When AUTO_RESUME = TRUE is set on a warehouse, what triggers the warehouse to start? | A RESUME WAREHOUSE command must be run manually | The warehouse starts when a SQL query is submitted and the warehouse is suspended | The warehouse starts automatically at 8 AM every day | The warehouse starts when CPU usage on Cloud Services exceeds 50% | B | AUTO_RESUME = TRUE means the warehouse will automatically start when any query is submitted to it while it is suspended. The user doesn't need to manually run ALTER WAREHOUSE ... RESUME. This enables seamless query execution without manual warehouse management. | Easy | Core | Virtual Warehouses |
| 27 | Doubling the size of a virtual warehouse (e.g., from Medium to Large) approximately how does this affect the hourly cost? | Increases cost by 25% | Doubles the cost | Triples the cost | Increases cost by 50% | B | Each size up exactly doubles both the compute resources AND the cost. Medium to Large doubles cost. This is linear and predictable: XS=1x, S=2x, M=4x, L=8x, XL=16x, 2XL=32x, etc. | Easy | Core | Virtual Warehouses |
| 28 | What type of SQL operations require a running virtual warehouse? | Only DDL operations (CREATE TABLE, DROP TABLE) | Only SELECT queries | DML operations (SELECT, INSERT, UPDATE, DELETE, MERGE, COPY INTO) | Only COPY INTO data loading operations | C | All DML operations (data queries and manipulation) require a running virtual warehouse. DDL operations (CREATE, DROP, ALTER on schema objects), metadata queries (SHOW, DESCRIBE), and GRANT/REVOKE are handled by Cloud Services and do NOT require a running warehouse. | Easy | Core | Virtual Warehouses |
| 29 | A warehouse has AUTO_SUSPEND = 300 and AUTO_RESUME = TRUE. It is currently suspended. A query is submitted. What is the sequence of events? | Query is queued, warehouse starts, query runs, warehouse suspends after 300s idle | Query fails with 'Warehouse suspended' error | Query runs on Cloud Services since the warehouse is down | Query is discarded; user must manually resume the warehouse first | A | With AUTO_RESUME = TRUE: the query is first queued, then the warehouse starts automatically (takes a few seconds), the query executes, and then the warehouse suspends itself after 300 seconds of inactivity. The user experiences a slightly longer first query (due to startup time) but no manual intervention is needed. | Medium | Core | Virtual Warehouses |
| 30 | A virtual warehouse is set to X-Large. A query requires more memory than a single X-Large node can provide. What happens? | The query fails with an out-of-memory error | Snowflake automatically spills excess data to local SSD (and then to remote storage) to complete the query, though performance degrades | Snowflake automatically upgrades the warehouse to 2X-Large for the duration of the query | The query is split across multiple warehouse clusters | B | Snowflake handles memory overflow by spilling to local SSD disk first, and if that is insufficient, to remote cloud storage. The query completes but performance degrades significantly due to I/O. If spill-to-disk is frequent, the warehouse size should be increased. Spilling appears in Query Profile as 'Bytes spilled to local storage' and 'Bytes spilled to remote storage'. | Medium | Core | Virtual Warehouses |
| 31 | Two users submit identical queries to the same warehouse simultaneously. The first query returns in 5 seconds. The second query returns in under 1 second. Why? | The second user has a faster network connection | The first query populated the Result Cache; the second returned from cache without running | The warehouse scaled up automatically for the second query | The second query was routed to a different warehouse | B | The first query ran on the warehouse and its result was stored in the Query Result Cache (Cloud Services layer). When the second user submitted an identical query moments later (same SQL, same permissions, data unchanged), Snowflake returned the cached result instantly without running the warehouse again. | Medium | Core | Virtual Warehouses |
| 32 | What is the effect of resizing a virtual warehouse from Large to Small while queries are running? | All running queries are immediately cancelled | Running queries complete on the Large size; new queries use the Small size after the resize completes | The resize fails — warehouses cannot be resized while queries are running | Running queries are paused, the resize completes, then they resume on the new size | B | Snowflake handles warehouse resizes gracefully. Currently executing queries complete on the existing (Large) configuration. Once the resize completes, new queries use the new (Small) size. There is no cancellation of running work. | Medium | Core | Virtual Warehouses |
| 33 | A warehouse costs $2/credit and runs for exactly 90 seconds processing a query. The warehouse then auto-suspends after 300 seconds of idle time. How many total seconds is the warehouse billed for? | 90 seconds | 300 seconds | 390 seconds | 450 seconds | C | The warehouse runs for 90 seconds (billed 90 seconds, above the 60-second minimum). After the query completes, the warehouse remains running (idle) for 300 seconds waiting for new queries (AUTO_SUSPEND = 300). Then it suspends. Total billed time = 90 seconds (query) + 300 seconds (idle) = 390 seconds. | Hard | Core | Virtual Warehouses |
| 34 | STATEMENT_QUEUED_TIMEOUT_IN_SECONDS = 60 is set on a warehouse. 20 queries arrive simultaneously but only 8 can run concurrently (MAX_CONCURRENCY_LEVEL = 8). What happens to the remaining 12 queries? | They are immediately rejected with a concurrency error | They queue and fail with a timeout error if they haven't started running within 60 seconds | They are automatically routed to a secondary warehouse | They run on the Cloud Services layer while the primary slots are full | B | STATEMENT_QUEUED_TIMEOUT_IN_SECONDS defines how long a query can wait in the warehouse queue before failing. With 12 queries queued and only 8 running, each queued query waits. Any query that hasn't begun execution within 60 seconds fails with a timeout error. Users see an error message indicating the queue wait time exceeded the limit. | Hard | Core | Virtual Warehouses |
| 35 | An e-commerce company has two workloads: overnight batch ETL (complex, 3-hour job) and daytime analytics (simple, fast queries). What warehouse strategy best serves these workloads? | One warehouse sized for the batch job, running 24/7 | Two separate warehouses: one large for batch (runs overnight), one small for analytics (runs during business hours), each with AUTO_SUSPEND | One multi-cluster warehouse handling both workloads | One warehouse that is resized daily between batch and analytics runs | B | Separate warehouses provide workload isolation, right-sizing, and optimal cost. The batch warehouse (large, overnight) and analytics warehouse (smaller, business hours) each auto-suspend when not needed. This prevents the batch job from consuming resources needed by analysts, and vice versa. | Easy | Architect | Virtual Warehouses |
| 36 | A company reports that their BI tool queries frequently wait 30–60 seconds before starting. Queries themselves are fast (< 5 seconds). The warehouse is always running. What is the MOST likely cause and solution? | Queries are slow to parse — increase warehouse size to speed up the optimizer | The warehouse has insufficient concurrency; queries are queuing. Solution: Enable multi-cluster warehouse (Enterprise edition) | Auto-Resume is causing startup delays — set a static RUNNING schedule | The Result Cache is disabled — re-enable it in account settings | B | 30-60 second wait times before queries start (not during execution) indicate concurrency queuing. The warehouse is busy with other concurrent queries, so new ones wait in queue. Multi-cluster warehouses (Enterprise+) add additional clusters when queuing is detected, eliminating wait times. Warehouse size affects query speed, not queue wait times. | Medium | Architect | Virtual Warehouses |
| 37 | An architect wants to ensure that the finance team's month-end reports always complete even if they consume all warehouse credits in a resource monitor. Other teams should be unaffected. How should this be designed? | Set the finance warehouse resource monitor threshold to 'Notify Only' for month-end | Create a dedicated finance warehouse with its own resource monitor, separate from other teams' warehouses. Set the month-end schedule to temporarily increase the finance quota or suspend the monitor. | Remove resource monitors during month-end reporting | Assign all month-end reports to the ACCOUNTADMIN role which bypasses resource monitors | B | The correct design separates warehouses by team (each team has its own warehouse and resource monitor), ensuring one team's usage doesn't affect others. For month-end, the finance team's resource monitor quota can be temporarily increased or suspended by an administrator. ACCOUNTADMIN does not bypass resource monitors — they apply to warehouses regardless of the executing role. | Hard | Architect | Virtual Warehouses |
| 38 | A data pipeline uses COPY INTO to load 1,000 files sequentially. Each file loads in 2 seconds. What is the most efficient way to speed up the loading? | Increase the virtual warehouse size — larger warehouses load files faster | Use parallel COPY INTO statements with multiple warehouses | Snowflake automatically parallelizes COPY INTO across multiple threads within a single warehouse — larger warehouses process more files in parallel | Compress the files to reduce load time | C | COPY INTO automatically parallelizes file loading within a virtual warehouse. A larger warehouse has more compute threads, allowing it to process multiple files simultaneously. Upgrading from Small to Large could process 8x more files in parallel. The actual improvement depends on file sizes and the number of files relative to warehouse threads. | Medium | Data Engineer | Virtual Warehouses |
| 39 | A Snowpark ML model training job uses a warehouse. Training requires reading 500 GB of data, performing matrix operations, and writing results. The current X-Large warehouse spills 100 GB to local storage. What is the BEST resolution? | Add more virtual warehouses — distribute the training across multiple warehouses | Increase warehouse size to 2X-Large or larger to fit more data in memory and reduce spill | Use Snowpark Container Services instead for ML training | Reduce the dataset size — ML models don't need all 500 GB | B | Spilling to local storage indicates the warehouse's RAM is insufficient for the in-memory operations. Increasing the warehouse size provides more memory per node, reducing or eliminating spill. For ML operations (especially matrix operations), memory is typically the bottleneck. 2X-Large provides approximately 2x the memory of X-Large per cluster. | Hard | Data Engineer | Virtual Warehouses |
| 40 | The Query Result Cache is stored in which Snowflake layer? | Local SSD of the virtual warehouse | Cloud Services layer | The storage layer alongside table data | External object storage (S3/Azure Blob/GCS) | B | The Query Result Cache is maintained in the Cloud Services layer, making it accessible to all users and warehouses in the account. This is why the same query from any user or warehouse can benefit from a cached result — it's not tied to any specific virtual warehouse. | Easy | Core | Caching |
| 41 | How long does a Query Result Cache entry remain valid if the underlying table data does not change? | 1 hour | 24 hours | 7 days | Until the warehouse is suspended | B | Result Cache entries are valid for 24 hours from the last time the result was reused. If the same query is run every hour, the cache entry keeps getting refreshed. If the table data changes, the cache entry is immediately invalidated, regardless of the 24-hour window. | Easy | Core | Caching |
| 42 | Does serving a query from the Result Cache consume virtual warehouse compute credits? | Yes — the warehouse must be running to serve cached results | No — Result Cache is served directly from Cloud Services without any warehouse | Yes, but at a reduced rate (10% of normal credit consumption) | Only if the cached result is larger than 1 GB | B | Result Cache hits are completely free — no warehouse credits are consumed. The result is returned directly from the Cloud Services layer. This is why report authors often encourage users to run the same queries (especially parameterless dashboards) rather than variations that bypass the cache. | Easy | Core | Caching |
| 43 | When is the local disk (data) cache on a virtual warehouse cleared? | Every 24 hours, synchronized with the Result Cache | When the warehouse is suspended or resized | When a new user connects to the warehouse | When storage data is updated | B | The local disk cache (which stores micro-partition data files read from cloud storage) is lost when the warehouse is suspended or resized. This is a key consideration for frequently-used warehouses — aggressive auto-suspend settings clear the cache and force re-reading from remote storage on the next query. | Easy | Core | Caching |
| 44 | A user runs a query. The warehouse is suspended immediately after. The next day, a different user runs an identical query. Will the Result Cache be used? | No — Result Cache is cleared when the warehouse suspends | Yes — if the underlying data hasn't changed and it's within 24 hours, Result Cache is used. The warehouse being suspended is irrelevant. | No — Result Cache is user-specific; different users cannot share cached results | Yes, but only if both users have the same role | B | Result Cache is stored in Cloud Services (not on the warehouse) and is account-wide. Warehouse suspension does not affect it. The cache is shared across all users and warehouses. The two requirements are: (1) same SQL text and context, and (2) data unchanged. A different user with same permissions can use the same cache entry. | Medium | Core | Caching |
| 45 | A BI dashboard query uses CURRENT_TIMESTAMP() in its WHERE clause to filter today's data. Users complain that the dashboard sometimes shows stale data. Why? | CURRENT_TIMESTAMP() is not supported in WHERE clauses | Queries using non-deterministic functions like CURRENT_TIMESTAMP() cannot use the Result Cache — each execution re-runs the query | The Result Cache is too small to store the dashboard query results | CURRENT_TIMESTAMP() always returns the time of the last cache population | B | Non-deterministic functions (CURRENT_TIMESTAMP, CURRENT_DATE, RANDOM, UUID_STRING, SEQ) in a query prevent Result Cache from being used. Each execution is treated as a unique query. For BI dashboards, this means every refresh actually runs the query. If users are seeing stale data, it might be from the warehouse's local disk cache or an intermediate caching layer in the BI tool. | Medium | Core | Caching |
| 46 | The local disk cache on a warehouse is described as a 'warm cache.' What does a 'warm cache' mean for query performance? | The warehouse is running at high temperature | Micro-partition data has already been loaded from cloud storage into the warehouse's local SSD, so subsequent queries on the same data read from fast local SSD instead of slower remote storage | The Result Cache has been pre-populated with expected query results | The warehouse has been pre-scaled to handle anticipated load | B | A 'warm cache' means frequently-accessed micro-partitions are already resident on the warehouse's local SSD. Reads from SSD are much faster than reads from remote cloud storage (S3/Blob/GCS). Maintaining a warm cache is why avoiding unnecessary warehouse suspensions for production analytical workloads can improve performance. | Medium | Core | Caching |
| 47 | A query Q1 runs on Warehouse A and its result is cached. Warehouse A is then dropped. Later, the identical query Q1 is submitted to Warehouse B. What happens? | Q1 fails because its cached result was associated with the dropped warehouse | Q1 uses the Result Cache from Cloud Services — the result is available regardless of which warehouse originally ran it or whether that warehouse still exists | Q1 must re-execute on Warehouse B since there's no cache entry | Q1 partially uses the cache but must re-compute the aggregation step | B | Result Cache is stored in Cloud Services, completely independent of any specific virtual warehouse. Dropping Warehouse A has no effect on the Result Cache. The cached result from Q1 is available to any user or warehouse in the account for 24 hours (as long as underlying data hasn't changed). | Hard | Core | Caching |
| 48 | An architect wants to maximize Result Cache effectiveness for a company's morning financial reports that run at 7 AM daily. What design principle achieves this? | Use CURRENT_DATE() in all WHERE clauses to filter today's data | Standardize report SQL in the BI tool so all users run identical query text, and avoid non-deterministic functions in frequently-run reports | Pre-warm the warehouse by running reports manually before 7 AM | Enable a dedicated Result Cache warehouse with larger memory | B | Result Cache works when queries have identical SQL text. Standardizing queries in the BI tool (same parameterization, same formatting) means the first user who runs a report at 7 AM populates the cache, and all subsequent users get instant results. Non-deterministic functions must be avoided as they bypass the cache. | Medium | Architect | Caching |
| 49 | A data pipeline writes new data to table T every 5 minutes. Multiple analysts run reports against T. What happens to the Result Cache for T when new data arrives? | Cache entries for T are invalidated immediately when any DML commits to T | Cache entries expire on their 24-hour schedule regardless of data changes | Cache entries are invalidated at the next 5-minute interval boundary | Cache entries are preserved for 15 minutes after a data write to reduce thrashing | A | Any DML that modifies table T (INSERT, UPDATE, DELETE, MERGE, COPY INTO) immediately invalidates all Result Cache entries that reference T. With 5-minute writes, analysts lose the benefit of Result Cache. This is expected behavior — reporting on near-real-time data inherently cannot use Result Cache effectively. | Hard | Data Engineer | Caching |
| 50 | What is the maximum Time Travel retention period for a Standard edition account? | 0 days | 1 day | 30 days | 90 days | B | Standard edition supports a maximum of 1 day (24 hours) of Time Travel. Enterprise edition and above support up to 90 days. Setting DATA_RETENTION_TIME_IN_DAYS > 1 on Standard edition will return an error. | Easy | Core | Editions |
| 51 | Which feature is ONLY available starting from Enterprise edition? | Basic SQL queries | Multi-cluster virtual warehouses | Standard Time Travel (1 day) | External stages | B | Multi-cluster warehouses require Enterprise edition or above. Standard edition is limited to single-cluster warehouses. All other options listed (SQL, 1-day Time Travel, external stages) are available in Standard edition. | Easy | Core | Editions |
| 52 | A company handling PHI (Protected Health Information) needs to comply with HIPAA. Which is the MINIMUM Snowflake edition required? | Standard | Enterprise | Business Critical | Virtual Private | C | HIPAA compliance requires Business Critical edition. This edition provides enhanced encryption, private connectivity (PrivateLink), and the contractual ability for Snowflake to sign a Business Associate Agreement (BAA) — required for HIPAA compliance. Enterprise edition does not provide HIPAA compliance support. | Easy | Core | Editions |
| 53 | A customer wants Dynamic Data Masking to protect PII columns in production. They currently use Standard edition. What must they do? | Enable Dynamic Data Masking in account settings (it's available in all editions) | Upgrade to at least Enterprise edition | Upgrade to Business Critical edition | Use a workaround with Column-level Security views instead | B | Dynamic Data Masking is an Enterprise-edition (and above) feature. Standard edition does not support it. The customer must upgrade to Enterprise edition. Column-level security (a separate feature), Row Access Policies, and Object Tagging are all Enterprise+ features. | Medium | Core | Editions |
| 54 | An organization on Enterprise edition wants to store Snowflake encryption keys in their own AWS KMS. Is this possible? | Yes — Enterprise edition supports customer-managed keys via KMS | No — Enterprise edition uses Snowflake-managed encryption only. Tri-Secret Secure (customer-managed keys) requires Business Critical edition. | Yes, but only for tables, not for stages or pipes | No — Snowflake never supports external key management | B | Tri-Secret Secure (the ability to use a customer-managed key from AWS KMS, Azure Key Vault, or GCP KMS alongside Snowflake's key) is a Business Critical feature, not Enterprise. Enterprise uses Snowflake-managed hierarchical key encryption. | Medium | Core | Editions |
| 55 | A company uses Standard edition with DATA_RETENTION_TIME_IN_DAYS = 1 on all tables. A critical table is accidentally dropped at 10:00 AM. The error is discovered at 11:30 AM. Can the table be recovered? | No — 1 hour 30 minutes exceeds the 1-day retention for Standard edition | Yes — UNDROP TABLE will restore the table since only 1.5 hours have passed (within the 1-day Time Travel window) | Yes — but only the last committed transaction before the drop can be recovered | No — dropped tables can only be recovered through Fail-Safe (7-day window), which requires Snowflake Support | B | With DATA_RETENTION_TIME_IN_DAYS = 1, dropped objects are recoverable for up to 24 hours using UNDROP TABLE. Since only 1.5 hours have passed, UNDROP TABLE my_table will restore the table with all its data. This works even on Standard edition within the 1-day window. | Hard | Core | Editions |
| 56 | A company is designing a Snowflake architecture for cross-region failover. Which minimum edition is required to enable database failover and failback? | Standard | Enterprise | Business Critical | Virtual Private | C | Database Failover/Failback for high availability and business continuity is a Business Critical (and Virtual Private) feature. Enterprise edition supports database replication for read-only workloads but not the full failover/failback capability required for active DR scenarios. | Easy | Architect | Editions |
| 57 | A financial institution requires: 90-day Time Travel, column-level security, private network connectivity (no public internet), and PCI-DSS compliance. Which edition meets ALL requirements? | Enterprise | Business Critical | Virtual Private | Standard with security add-ons | B | Mapping requirements: 90-day Time Travel (Enterprise+), column-level security (Enterprise+), private network connectivity via PrivateLink (Business Critical+), PCI-DSS compliance (Business Critical+). Business Critical meets all requirements. Virtual Private would also work but is more expensive than needed. | Medium | Architect | Editions |
| 58 | An organization is evaluating cost/benefit of upgrading from Enterprise to Business Critical. Their primary drivers are: existing Snowflake encryption is acceptable, they have no compliance requirements, but they want dedicated hardware. What is the CORRECT edition recommendation? | Business Critical — it provides dedicated hardware | Virtual Private Snowflake — dedicated hardware is a VPS-only feature, not Business Critical | Enterprise is sufficient — dedicated hardware is not a Snowflake feature | Business Critical with Tri-Secret Secure for dedicated hardware | B | Dedicated virtual network infrastructure (isolated, dedicated hardware) is a Virtual Private Snowflake (VPS) feature, not Business Critical. Business Critical provides enhanced compliance, encryption, and private connectivity on shared Snowflake infrastructure. If the sole requirement is dedicated hardware, VPS is the correct (though most expensive) choice. | Hard | Architect | Editions |
| 59 | Which of the following is an account-level object in Snowflake (NOT contained within a database)? | Table | View | Virtual Warehouse | Stage | C | Virtual Warehouses exist at the account level. They are not contained in any database or schema. Other account-level objects include: Roles, Users, Resource Monitors, Network Policies, Shares, and Integrations. Tables, Views, and Stages are schema-level objects. | Easy | Core | Object Hierarchy |
| 60 | What is the default schema created automatically when a new database is created in Snowflake? | MAIN | DEFAULT | PUBLIC | SYSTEM | C | When a new database is created in Snowflake, the PUBLIC schema is automatically created as a default schema. The INFORMATION_SCHEMA schema is also created automatically and contains metadata views. Users can create additional schemas as needed. | Easy | Core | Object Hierarchy |
| 61 | What is the highest level of the Snowflake object hierarchy? | Account | Organization | Database | Cloud Services | B | Organization is the highest level in Snowflake's object hierarchy. An Organization groups multiple Snowflake accounts and enables cross-account features like replication, data sharing governance, and consolidated billing. Not all customers have an Organization — it's used primarily in multi-account setups. | Easy | Core | Object Hierarchy |
| 62 | A developer runs DROP DATABASE prod_db. Which objects are ALSO deleted as a result? | Only the database record — all tables are retained in their original state | All schemas, tables, views, stages, functions, and stored procedures within prod_db | All schemas and tables, but stages are not deleted (they reference external storage) | The database and schemas, but all tables are moved to PUBLIC schema in the default database | B | Dropping a database in Snowflake is a cascading operation. ALL child objects are dropped: schemas, tables, views, stages, file formats, sequences, functions, stored procedures, streams, tasks, and pipes within the database. Account-level objects (warehouses, roles, users) are NOT affected. Within Time Travel retention, the database can be restored with UNDROP DATABASE. | Medium | Core | Object Hierarchy |
| 63 | A Snowflake account has a parameter TIMEZONE = 'UTC' set at the account level. A specific database DB1 has TIMEZONE = 'America/New_York' set. A session using DB1 has no timezone set. What timezone applies to queries in this session? | UTC (account-level always wins) | America/New_York (database-level overrides account-level) | The system default timezone of the cloud provider | UTC/America/New_York hybrid depending on the query type | B | Snowflake parameter inheritance follows a hierarchy where more specific settings override broader ones. Database-level overrides Account-level. Session-level (highest priority) overrides Database-level. Since the session has no explicit timezone, the database-level setting (America/New_York) applies. | Medium | Core | Object Hierarchy |
| 64 | A Snowflake database named ANALYTICS has a schema named SALES_DATA. Inside this schema, there is a table named ORDERS. What is the fully qualified name of this table? | ORDERS.SALES_DATA.ANALYTICS | ANALYTICS/SALES_DATA/ORDERS | ANALYTICS.SALES_DATA.ORDERS | SALES_DATA.ORDERS in database ANALYTICS | C | Snowflake uses dot notation for fully qualified object names: DATABASE.SCHEMA.OBJECT. So the fully qualified name is ANALYTICS.SALES_DATA.ORDERS. This notation is used in SQL queries when referencing objects across databases or schemas, and in GRANT statements. | Hard | Core | Object Hierarchy |
| 65 | An architect wants to enforce consistent data retention policies across all tables in a production database. What is the MOST efficient approach? | Set DATA_RETENTION_TIME_IN_DAYS on every table individually | Set DATA_RETENTION_TIME_IN_DAYS at the database level — all tables inherit this setting unless overridden at the schema or table level | Create a stored procedure that sets retention on all tables weekly | Use Resource Monitors to control Time Travel storage costs | B | Setting parameters at the database level propagates to all schemas and tables within it via inheritance. Individual schemas or tables can override this with their own setting. This is far more efficient than setting retention on every table. For example: ALTER DATABASE prod SET DATA_RETENTION_TIME_IN_DAYS = 30; | Medium | Architect | Object Hierarchy |
| 66 | What SQL clause is used to query historical data in Snowflake using a specific timestamp? | HISTORICAL AS OF TIMESTAMP | AS OF DATETIME | AT (TIMESTAMP => ...) | BEFORE TIME | C | The AT clause with TIMESTAMP => syntax is used for Time Travel: SELECT * FROM table AT (TIMESTAMP => '2024-01-15 10:00:00'::timestamp). BEFORE is also valid for seeing data just before a specific event. There is no HISTORICAL AS OF syntax in Snowflake. | Easy | Core | Time Travel |
| 67 | A table has been accidentally dropped. Which command restores it using Time Travel? | RESTORE TABLE my_table | ROLLBACK DROP TABLE my_table | UNDROP TABLE my_table | CREATE TABLE my_table FROM BACKUP | C | UNDROP TABLE table_name is the command to restore a dropped table within the Time Travel retention period. Similarly, UNDROP SCHEMA and UNDROP DATABASE can restore accidentally dropped parent objects. This works as long as the object was dropped within the configured DATA_RETENTION_TIME_IN_DAYS window. | Easy | Core | Time Travel |
| 68 | What is Fail-Safe, and who can access data in Fail-Safe? | A user-accessible backup that extends Time Travel by 7 days | A Snowflake-managed disaster recovery layer, accessible only by Snowflake Support, that exists for 7 days after Time Travel expires | A feature to create standby databases in different regions | An automated backup to customer-owned cloud storage | B | Fail-Safe is a 7-day (fixed, non-configurable) disaster recovery layer that begins after Time Travel expires. Unlike Time Travel (user-accessible via SQL), Fail-Safe data can only be recovered by Snowflake Support upon request. It protects against catastrophic scenarios, not routine user errors. | Easy | Core | Time Travel |
| 69 | A table has DATA_RETENTION_TIME_IN_DAYS = 30. A row was deleted 35 days ago. Can the row be recovered? | Yes — using Time Travel with AT (OFFSET => -35 * 86400) | Yes — using Fail-Safe access within the 7-day Fail-Safe window (days 30–37) | No — Time Travel (30 days) has expired, and Fail-Safe (days 31–37) has also expired at day 35... wait, 35 < 37, so Snowflake Support could potentially recover via Fail-Safe | No — once Time Travel expires, data is permanently deleted | C | With 30-day Time Travel, the row's deletion was 35 days ago. Time Travel expired at day 30. Fail-Safe begins at day 30 and runs for 7 more days (days 30–37). Since we're at day 35, we are within the Fail-Safe window. Snowflake Support could potentially recover this data. However, Fail-Safe recovery is not guaranteed — it is a last-resort measure. | Medium | Core | Time Travel |
| 70 | Setting DATA_RETENTION_TIME_IN_DAYS = 0 on a table has what effect? | Time Travel is disabled for that table — historical data is deleted immediately when rows change | Time Travel uses the default (1 day) when set to 0 | Only Fail-Safe protection applies when Time Travel is 0 | Setting to 0 is not allowed — minimum is 1 day | A | DATA_RETENTION_TIME_IN_DAYS = 0 explicitly disables Time Travel for that table. Historical data is deleted as soon as rows are modified or deleted — no AT/BEFORE queries are possible. Fail-Safe also does not apply to tables with retention = 0. This reduces storage costs for transient/staging tables. | Medium | Core | Time Travel |
| 71 | A table's DATA_RETENTION_TIME_IN_DAYS is changed from 90 to 1. What happens to the historical data that was retained for the previous 90-day period? | Historical data is immediately purged when retention is reduced | Historical data retention is maintained for the old 90-day period; new changes use the 1-day retention | Historical data is retained for a grace period of 7 days before deletion | Historical data is moved to Fail-Safe automatically | A | When TIME TRAVEL retention is reduced, Snowflake does NOT immediately purge existing historical data — but the metadata governing its availability changes. Snowflake's background processes will eventually purge data older than the new retention window (1 day) through normal garbage collection. The exact timing may have a short delay but the data becomes inaccessible via Time Travel immediately upon the setting change. | Hard | Core | Time Travel |
| 72 | An architect wants to implement a system where any data corruption can be recovered going back 60 days. The organization uses Standard edition. What is the issue? | Standard edition supports only 1-day Time Travel — 60 days requires Enterprise edition | Standard edition supports Time Travel up to 30 days | 60-day Time Travel requires Business Critical edition | Standard edition does not support Time Travel at all | A | Standard edition is limited to a maximum of 1 day of Time Travel. Enterprise edition supports up to 90 days. To achieve 60-day recovery capability, the organization must upgrade to Enterprise edition and set DATA_RETENTION_TIME_IN_DAYS = 60 on the relevant tables. | Medium | Architect | Time Travel |
| 73 | A Data Engineer wants to create a daily audit comparison report showing what changed in table ORDERS between yesterday and today. What is the MOST efficient Snowflake-native approach? | Create two snapshots daily by INSERT INTO archive_table SELECT * FROM orders | Use Snowflake Streams on the ORDERS table to capture CDC (Change Data Capture) changes automatically | Run a complex self-join using Time Travel: compare ORDERS AT (OFFSET => -86400) with current ORDERS | Export data to S3 daily and compare files in an external system | B | Snowflake Streams are specifically designed for CDC tracking. A Stream on ORDERS captures all INSERT, UPDATE, and DELETE changes since the last time the stream was consumed. This is more efficient than periodic snapshots (no duplicated data) and more elegant than self-joining with Time Travel. | Medium | Data Engineer | Time Travel |
| 74 | What is the initial storage cost of creating a zero-copy clone of a 10 TB table? | 10 TB (full copy of all data) | 5 TB (half the data is copied initially) | $0 extra (no data is physically copied) | Only metadata storage cost (negligible) | C | Zero-copy cloning creates a metadata-only snapshot pointing to the same micro-partitions as the source. No data is physically copied at clone creation time. Additional storage is only consumed when either the clone or source is modified (creating new, non-shared micro-partitions). | Easy | Core | Zero-Copy Cloning |
| 75 | Which objects CAN be cloned in Snowflake? | Tables, schemas, databases | Only individual tables | Only databases | Tables and schemas only — databases cannot be cloned | A | In Snowflake, you can clone Tables, Schemas, and entire Databases. Cloning a Schema clones all objects within it. Cloning a Database clones all schemas and objects within it. Streams and Tasks can also be cloned with some limitations. | Easy | Core | Zero-Copy Cloning |
| 76 | After cloning a production table, a developer inserts 1 million rows into the clone. What is the storage impact? | 1 million rows of storage allocated, shared between clone and source | New micro-partitions are created only for the clone, containing the new 1 million rows. The original shared micro-partitions remain shared. | The entire clone is copied at first write (copy-on-write) — the developer now has two full copies | All 1 million new rows are written to the source table's micro-partitions to maintain linkage | B | Zero-copy cloning uses a copy-on-write mechanism at the micro-partition level. When the clone is modified, only the new or changed micro-partitions are created for the clone. The original partitions continue to be shared. Storage cost increases only for the new micro-partitions containing the 1 million inserted rows. | Medium | Core | Zero-Copy Cloning |
| 77 | A team clones production database PROD to create DEV. Which of the following is NOT cloned? | Tables and their data | Views and stored procedures | Grants (privileges) on objects within the database | Schemas and file formats | C | Zero-copy cloning does NOT copy grants (privilege assignments). The cloned objects exist in the new database but none of the GRANT statements that applied to the source are replicated. Access must be re-granted separately on the clone. This is a common exam trap and a real operational consideration for dev/test environments. | Medium | Core | Zero-Copy Cloning |
| 78 | A database is cloned using: CREATE DATABASE test CLONE prod AT (TIMESTAMP => '2024-01-01 00:00:00'). What does this create? | A clone of the current state of prod, ignoring the timestamp | A clone reflecting prod's state at midnight on January 1, 2024 — a point-in-time historical snapshot | A scheduled clone that will be created on January 1, 2024 | A clone that automatically resets to the January 1 state every day | B | You can combine zero-copy cloning with Time Travel to create a historical clone. The AT clause specifies the exact point-in-time for the clone. The resulting database reflects PROD's state at that moment. This is useful for debugging (what did data look like before a bad load?) or creating reproducible test environments. | Hard | Core | Zero-Copy Cloning |
| 79 | An architect wants to enable developers to test data pipeline changes safely without affecting production. Data must be realistic (not synthetic). What is the recommended Snowflake pattern? | Replicate the production database to a development Snowflake account weekly | Use zero-copy cloning to create a development database from production — instant, free at creation time, fully isolated once modified | Export production data to S3 and load it into a development schema | Use Snowflake Data Sharing to give developers read-only access to production | B | Zero-copy cloning is the definitive answer for dev/test environments. It's instant (regardless of data size), free at creation time, and completely isolated — changes in dev never affect production. Developers get full read-write access to realistic production-scale data. This is a core Snowflake best practice. | Medium | Architect | Zero-Copy Cloning |
| 80 | A Data Engineer runs: CREATE TABLE orders_clone CLONE orders AT (OFFSET => -7200). The current time is 3:00 PM. They then insert rows into orders. Later, they query orders_clone. What do they see? | The current state of orders, including the newly inserted rows | The state of orders as it was at 1:00 PM (2 hours ago), before the new inserts. The clone is isolated from changes to the source. | An error because orders was modified after the clone was created | The state of orders at 1:00 PM plus any changes to orders since the clone was created | B | Zero-copy cloning creates an isolated snapshot. The clone reflects orders' state at 1:00 PM (3:00 PM minus 7,200 seconds = 2 hours). Subsequent inserts to the source table (orders) create new micro-partitions for orders only — orders_clone still references the original 1:00 PM micro-partitions. The clone is completely isolated from source changes. | Hard | Data Engineer | Zero-Copy Cloning |
| 81 | Multi-cluster warehouses are designed to solve which problem? | Individual queries taking too long (complex, memory-intensive queries) | Too many concurrent users causing queries to queue | Insufficient storage capacity | High cost of running large warehouses | B | Multi-cluster warehouses solve the concurrency problem — too many simultaneous users causing queries to wait in queue. When the primary cluster is busy, additional clusters spin up to handle the extra load. They do NOT help with individual query complexity — that requires a larger warehouse size. | Easy | Core | Multi-Cluster Warehouses |
| 82 | With a multi-cluster warehouse configured as Min=1, Max=4, Scaling Policy=Standard, what triggers a second cluster to start? | CPU utilization on cluster 1 exceeds 80% | Any query submitted when cluster 1 is already processing queries | A query would have to queue (wait) due to cluster 1 being at MAX_CONCURRENCY_LEVEL | The number of active sessions exceeds 100 | C | Standard scaling policy adds a new cluster as soon as a query would have to wait in queue (i.e., the current cluster is at full concurrency). Economy policy waits longer to ensure sustained queue buildup before adding a cluster. CPU utilization and session count don't directly trigger cluster scaling. | Medium | Core | Multi-Cluster Warehouses |
| 83 | A multi-cluster warehouse is set to Min=3, Max=6. There are no active queries. How many clusters are running? | 0 — all clusters suspend when idle | 3 — Min clusters always run | 6 — Max clusters are pre-started for readiness | 1 — only one cluster runs until needed | B | Min clusters is the guaranteed minimum number of always-running clusters. With Min=3, at least 3 clusters are always active even with zero queries. Additional clusters (up to Max=6) spin up based on concurrency demand. Setting Min > 1 increases cost but ensures instant availability without any cold-start latency. | Medium | Architect | Multi-Cluster Warehouses |
| 84 | A warehouse has Min=1, Max=8, Scaling=Economy. During peak hour, 200 concurrent queries arrive. Each query takes 30 seconds. MAX_CONCURRENCY_LEVEL = 8. How many clusters does Snowflake need to handle this without any queuing? | 8 clusters | 25 clusters | More than 8 — Snowflake adds up to 200/8 = 25 clusters, but Max=8 limits it | 200 clusters (one per query) | C | With 200 concurrent queries and 8 concurrent queries per cluster: 200 / 8 = 25 clusters needed. However, Max=8. So at maximum scaling, only 64 queries (8 clusters × 8 concurrent) can run simultaneously — the remaining 136 will queue. Economy scaling may further delay cluster addition. The solution is to increase Max Clusters or reduce concurrency. | Hard | Architect | Multi-Cluster Warehouses |
| 85 | What is the purpose of a Resource Monitor in Snowflake? | Monitor query performance and automatically optimize slow queries | Set credit usage limits on virtual warehouses or the account to control costs | Monitor data quality and alert on anomalies | Track user login activity and suspicious access patterns | B | Resource Monitors are a cost governance tool. They set credit quotas on virtual warehouses (or the entire account) and can notify administrators and/or suspend warehouses when thresholds are exceeded. They do not affect query performance optimization or security. | Easy | Core | Resource Monitors |
| 86 | Which role is required to create a Resource Monitor? | SYSADMIN | SECURITYADMIN | ACCOUNTADMIN | Any role with MONITOR WAREHOUSE privilege | C | Creating Resource Monitors requires the ACCOUNTADMIN role (or a role with MANAGE GRANTS privilege, which is effectively ACCOUNTADMIN-level). This ensures that only top-level administrators can control account-wide cost governance. | Easy | Core | Resource Monitors |
| 87 | A Resource Monitor is set to: Credit Quota = 100, Trigger at 90% = Notify, Trigger at 100% = Notify & Suspend. The warehouse has consumed 89 credits. What happens when it consumes credit #90? | The warehouse is suspended immediately | An alert notification is sent (email to monitor administrators), but the warehouse continues running | The warehouse is suspended after currently-running queries complete | Nothing — the 90% trigger only applies to the account-level monitor, not individual warehouses | B | At the 90% threshold (90 credits out of 100), the configured action is 'Notify' — an email notification is sent to the users/roles assigned to receive alerts for this monitor. The warehouse continues running normally. The suspend action only triggers at the 100% threshold (100 credits). | Medium | Core | Resource Monitors |
| 88 | Resource Monitor RM1 is applied to the ACCOUNT level with a monthly quota of 1,000 credits. Warehouse WH1 also has its own Resource Monitor RM2 with a daily quota of 50 credits. WH1 has consumed 50 credits today (RM2 at 100%) and 200 credits this month (RM1 at 20%). What happens? | RM1 wins — WH1 continues running because account-level quota is not exceeded | RM2 triggers WH1 to suspend because its daily quota is reached, regardless of RM1 | Both monitors must be at 100% before any action is taken | RM1 and RM2 average their thresholds — WH1 suspends when both reach 60% | B | Resource Monitors trigger independently. RM2 (warehouse-level, daily 50 credits) has reached 100% and its action (e.g., Suspend) applies to WH1. The account-level RM1 being at only 20% does not prevent RM2 from acting. Multiple monitors can apply simultaneously — the most restrictive action takes effect. | Hard | Core | Resource Monitors |
| 89 | What is SnowSQL? | Snowflake's proprietary SQL dialect | A command-line interface (CLI) tool for connecting to Snowflake | The web-based UI for Snowflake (formerly called Snowsight) | A Java library for building Snowflake applications | B | SnowSQL is Snowflake's command-line interface. It allows users to execute SQL queries, run scripts, load/unload data, and perform administrative tasks from the terminal. It supports batch mode (script execution) and interactive mode. | Easy | Core | Connectivity |
| 90 | Which Snowflake interface requires NO software installation? | SnowSQL | JDBC Driver | Snowsight (Web UI) | Python Connector | C | Snowsight (formerly the Classic Web UI) is Snowflake's browser-based interface. It requires no local installation — users access it via a web browser at app.snowflake.com or their account URL. SnowSQL, JDBC, and Python Connector all require local installation. | Easy | Core | Connectivity |
| 91 | A BI tool needs to connect to Snowflake using a standard database connectivity protocol supported by most analytics software. Which connection method is MOST appropriate? | SnowSQL (CLI) | Snowpark Python API | ODBC or JDBC driver | Snowflake REST API | C | ODBC (Open Database Connectivity) and JDBC (Java Database Connectivity) are the universal standard protocols that virtually all BI tools (Tableau, Power BI, MicroStrategy, Looker, etc.) support. Snowflake provides both drivers. SnowSQL is for command-line use, Snowpark is for data engineering, and REST API is for programmatic access. | Medium | Core | Connectivity |
| 92 | A data team writes a Snowpark Python application to transform data within Snowflake. What is the key advantage of Snowpark compared to extracting data to a Pandas-based Python script? | Snowpark is faster because it runs on the Cloud Services layer | Snowpark pushes computation into Snowflake's engine — data doesn't move to the client. Transformations run on the virtual warehouse with full parallelism. | Snowpark automatically generates Snowflake SQL, which the developer can then review | Snowpark only works with structured data, so it's simpler than Pandas | B | With traditional Python (Pandas), you extract data to the client machine, transform it locally, and write it back — moving potentially terabytes of data over the network. Snowpark runs transformation code directly on the virtual warehouse inside Snowflake. The data never leaves Snowflake's infrastructure, and transformations leverage Snowflake's distributed compute. | Medium | Data Engineer | Connectivity |
| 93 | What is a Snowflake Stream? | A real-time data streaming service that replaces Kafka | A CDC (Change Data Capture) object that records DML changes (INSERT, UPDATE, DELETE) made to a table | A scheduled job that runs SQL on a defined interval | A continuous query that runs indefinitely on incoming data | B | A Snowflake Stream is a CDC object that tracks row-level changes (INSERT, UPDATE, DELETE) on a source table. It records what changed, when, and what type of change occurred. Streams do NOT move data in real-time like Kafka — they are a changelog capture mechanism used with batch processing. | Easy | Data Engineer | Streams & Tasks |
| 94 | What SQL object in Snowflake is used to schedule recurring SQL execution? | Procedure | Schedule | Task | Trigger | C | A Task is Snowflake's scheduled job mechanism. Tasks run SQL statements (including CALL for stored procedures) on a defined schedule (cron syntax or interval) or in response to a predecessor Task completing (task graphs/DAGs). Tasks require a virtual warehouse to execute. | Easy | Data Engineer | Streams & Tasks |
| 95 | A Stream is created on table CUSTOMERS. What columns does the stream add to each change record? | CHANGE_TYPE, CHANGE_TIME, CHANGED_BY | METADATA$ACTION, METADATA$ISUPDATE, METADATA$ROW_ID | CHANGE_ID, BEFORE_IMAGE, AFTER_IMAGE | STREAM_ID, DML_TYPE, TRANSACTION_ID | B | Snowflake Streams add three metadata columns to each record: METADATA$ACTION (INSERT or DELETE), METADATA$ISUPDATE (TRUE if the change resulted from an UPDATE), and METADATA$ROW_ID (unique identifier for the row). An UPDATE shows as DELETE (old row) + INSERT (new row) both with METADATA$ISUPDATE = TRUE. | Medium | Data Engineer | Streams & Tasks |
| 96 | A Serverless Task is configured with no virtual warehouse specified. How is compute billed for this task? | No compute is billed — serverless tasks run on Cloud Services for free | Compute is billed using Snowflake-managed compute (serverless), charged per compute-second consumed by the task | A default X-Small warehouse is automatically used and billed | The task cannot run without a warehouse — it will fail | B | Serverless Tasks use Snowflake-managed compute resources (not a user-provisioned warehouse). Billing is per compute-second actually consumed by the task — there's no minimum billing period. This is more cost-efficient for short, infrequent tasks because you avoid the 60-second minimum warehouse billing. | Medium | Data Engineer | Streams & Tasks |
| 97 | A task graph has Task A → Task B → Task C. Task B fails. What happens to Task C? | Task C runs anyway, ignoring Task B's failure | Task C is skipped automatically — dependent tasks do not run if their predecessor fails | Task C is retried along with Task B | Task C runs, but with an error flag in its execution metadata | B | In a Snowflake task graph (DAG), tasks only run if their predecessor completes successfully. If Task B fails, Task C will not execute. The failure can be monitored via the TASK_HISTORY view and alerts can be configured. Task A → Task B failure suspends all downstream tasks. | Hard | Data Engineer | Streams & Tasks |
| 98 | Snowpipe differs from COPY INTO in which fundamental way? | Snowpipe supports more file formats than COPY INTO | Snowpipe automatically loads files as they arrive in a stage (event-driven); COPY INTO is a manually triggered batch load | Snowpipe is faster for large file loads | Snowpipe can load from external tables; COPY INTO cannot | B | The fundamental difference is the trigger mechanism. COPY INTO is a manually executed SQL command (or scheduled via Tasks). Snowpipe is event-driven — it monitors a stage and automatically triggers when new files arrive, via cloud event notifications (SQS for S3, Event Grid for Azure Blob) or REST API calls. | Easy | Data Engineer | Snowpipe |
| 99 | A company uses Snowpipe to load from S3. Files arrive in the stage but are not being loaded. What is the MOST likely cause? | Snowpipe requires a running virtual warehouse to process files | The SQS event notification is misconfigured or not connected to the Snowpipe definition | Snowpipe only works with internal stages, not S3 external stages | The files are too small for Snowpipe to process | B | Snowpipe with S3 uses AWS SQS (Simple Queue Service) event notifications to detect new files. If SQS is misconfigured, Snowpipe never receives the 'new file' notification and doesn't process the files. The file size is irrelevant (though large numbers of small files are inefficient). Snowpipe uses Snowflake-managed serverless compute, not a user warehouse. | Medium | Data Engineer | Snowpipe |
| 100 | A company uses Snowpipe REST API to push file loading notifications. The application sends a notification for the same file twice. What is the expected behavior? | The file is loaded twice, causing duplicate data | Snowpipe deduplicates based on file name and stage — the second notification is ignored if the file was already processed within the deduplication window (24 hours) | The second notification fails with a 'file already loaded' error | Both notifications are processed, but Snowflake's internal deduplication merges the duplicate rows | B | Snowpipe provides at-least-once delivery semantics. It maintains a deduplication token based on file path and stage, and within a 24-hour window, duplicate notifications for the same file are typically ignored. However, this deduplication is not guaranteed beyond 24 hours. For absolute idempotency, applications should implement their own deduplication logic downstream. | Hard | Data Engineer | Snowpipe |
| 101 | Snowflake Secure Data Sharing allows data to be shared between accounts without which of the following? | Copying or moving any data | Using SQL to query shared data | The consumer needing a Snowflake account | Setting up network connectivity between accounts | A | Secure Data Sharing shares live data without copying or moving it. The consumer account reads directly from the provider's storage. No ETL, no file exports, no data duplication. The consumer does need a Snowflake account (or can use a Reader Account provisioned by the provider). | Easy | Architect | Data Sharing |
| 102 | A data provider wants to share data with an external partner who does not have a Snowflake account. What is the solution? | The partner must sign up for a Snowflake account before data can be shared | The provider creates a Reader Account (managed Snowflake account) for the partner — the provider pays for the partner's compute | Data can be exported to S3 and shared via pre-signed URLs | Cross-organization sharing requires Snowflake Marketplace listing | B | Reader Accounts are Snowflake accounts created and managed by a data provider specifically for external consumers who don't have their own Snowflake account. The provider controls the Reader Account and pays for its compute resources. The consumer can query shared data but cannot load their own data. | Medium | Architect | Data Sharing |
| 103 | A Snowflake data provider shares a database with a consumer in the same AWS region. The consumer reports that querying the shared database is slow. What is the MOST likely cause? | The shared database is locked by the provider's warehouse | The consumer's virtual warehouse is too small for the shared data volume — the consumer must use their own warehouse resources to query shared data | Data sharing has a latency penalty for same-region sharing | The share needs to be re-created to refresh performance metadata | B | When consuming shared data, the consumer's own virtual warehouse performs the computation. The consumer's warehouse size directly impacts query performance. The provider's warehouse is irrelevant — providers only serve the data storage. If the consumer has an X-Small warehouse querying a 10 TB shared table, performance will be poor. | Hard | Architect | Data Sharing |
| 104 | Snowflake database replication creates a secondary database that is: | An exact real-time mirror updated synchronously | A read-only replica updated asynchronously on a defined schedule or on demand | A writable copy that synchronizes changes back to primary every hour | An independent copy with no automatic synchronization | B | Secondary databases created via replication are read-only replicas. They are updated asynchronously — either on a schedule or when a REFRESH is manually triggered. There is no real-time synchronous replication in Snowflake (as of standard replication). This means the secondary may be slightly behind the primary. | Easy | Architect | Replication |
| 105 | A company replicates a database from Account A (AWS us-east-1) to Account B (AWS eu-west-1). The refresh schedule is every 6 hours. An outage occurs in us-east-1. The company fails over to Account B. What is the potential data loss? | No data loss — replication is synchronous | Up to 6 hours of data (since the last successful refresh) | Exactly 6 hours (replication always stays exactly on schedule) | No data loss because Fail-Safe covers the gap | B | With asynchronous replication on a 6-hour schedule, the secondary (Account B) may be up to 6 hours behind the primary at any given time. In a failover, data written to Account A in the last 0-6 hours (since the last refresh) is not present in Account B. This RPO (Recovery Point Objective) of up to 6 hours must be acceptable for the business. | Hard | Architect | Replication |
| 106 | A query scans a 5 TB table but only needs 3 columns out of 200. How does Snowflake's columnar storage format help? | Snowflake reads all 200 columns but ignores 197 during processing | Snowflake reads only the 3 requested columns' data from storage — 197 columns are not read at all | Columnar format compresses the 3 columns to read faster | Columnar format creates indexes on each column for faster access | B | Columnar storage stores each column separately. A SELECT requesting only 3 of 200 columns means Snowflake only reads those 3 columns' micro-partition data from storage. In row-based storage, every row's full 200-column record would need to be read. This is why columnar storage dramatically reduces I/O for analytics workloads. | Medium | Core | Performance |
| 107 | A query joins two large tables: ORDERS (1 billion rows) and CUSTOMERS (10 million rows). Performance is poor. The developer tries adding a clustering key to ORDERS on customer_id. Will this help? | Yes — clustering ORDERS on customer_id will always speed up join performance | It depends: if queries filter on customer_id before the join, clustering helps pruning. If the join is between all rows without selective filtering, clustering alone won't significantly help the join itself. | No — clustering keys only help for selective range queries, not joins | Yes — clustering ensures ORDERS rows with the same customer_id are on the same nodes as CUSTOMERS rows | B | Clustering helps if the query filters on customer_id before (or as part of) the join — fewer ORDERS partitions are scanned. If the join processes all rows (no selective filter), clustering doesn't reduce the join work. For large joins, the optimizer's choice of join type (hash join vs. merge join) and the sizes of both tables matter more. | Hard | Core | Performance |
| 108 | An architect reviews slow dashboard queries. The Query Profile shows 'Remote Disk I/O' as the dominant operation. What does this indicate, and what should the architect recommend? | Remote Disk I/O is normal — it means data is being read from the storage layer. This is expected behavior. | Remote Disk I/O indicates the warehouse's local SSD cache is full or cold, forcing reads from cloud storage. Recommendation: increase warehouse size (more SSD cache), avoid frequent warehouse suspension to preserve warm cache, or consider a dedicated warehouse for this workload. | Remote Disk I/O means the virtual warehouse is undersized — upgrade to a larger warehouse | Remote Disk I/O indicates network congestion — contact Snowflake support | B | Remote Disk I/O (reading from S3/Blob/GCS) is slower than local SSD reads. High Remote Disk I/O means the local warehouse cache is cold (data hasn't been cached yet) or the dataset is larger than the cache capacity. Solutions: warm the cache by running typical queries before peak hours, avoid warehouse suspension for active workloads, and consider larger warehouse sizes for larger cache capacity. | Medium | Architect | Performance |
| 109 | A COPY INTO command loads files from S3 but takes 4x longer than expected. Files are 5 MB each, and there are 10,000 of them. What is the performance problem and solution? | 10,000 files is too many for COPY INTO — use Snowpipe instead | 5 MB files are too small; Snowflake processes each file with overhead that outweighs the data. Consolidate files to 100–250 MB before staging for optimal COPY INTO performance. | COPY INTO doesn't support large numbers of files — batch into groups of 100 | The warehouse needs to be at least X-Large for 10,000 file loads | B | 5 MB files are the classic small file problem. COPY INTO (and Snowpipe) open and process each file individually. With 10,000 files at 5 MB, the file-processing overhead dominates. Optimal file size for Snowflake is 100–250 MB per file. Consolidating 10,000 × 5 MB files into ~200 × 250 MB files would dramatically improve throughput. | Medium | Data Engineer | Performance |
| 110 | What is Snowflake's primary access control model? | DAC (Discretionary Access Control) only | MAC (Mandatory Access Control) | RBAC (Role-Based Access Control) | ABAC (Attribute-Based Access Control) | C | Snowflake uses RBAC (Role-Based Access Control). Privileges are granted to roles, and roles are granted to users. Users inherit all privileges of their active role. Snowflake also supports DAC elements (object owners control their objects), but RBAC is the foundational model. | Easy | Core | Security |
| 111 | What is the purpose of a Network Policy in Snowflake? | Encrypt data in transit between Snowflake and clients | Restrict which IP addresses can connect to the Snowflake account | Control which cloud services Snowflake can access | Monitor network traffic and detect intrusions | B | Network Policies restrict access to a Snowflake account or user based on IP address ranges (IP allowlisting). You can specify allowed IP ranges and blocked IP ranges. Network Policies can be applied at the account level (affects all users) or at the individual user level. | Easy | Core | Security |
| 112 | A column contains credit card numbers. A masking policy is applied so that analysts see 'XXXX-XXXX-XXXX-1234' (only last 4 digits visible). Data engineers see the full number. What Snowflake feature enables this? | Row Access Policies | Column-level Security (Dynamic Data Masking) | Data Classification | Object Tagging | B | Dynamic Data Masking (part of Column-level Security) allows defining masking policies that return different values based on the querying user's role. Analysts with a lower-privileged role see the masked value; data engineers with a higher-privileged role see the full value. This requires Enterprise edition. | Medium | Core | Security |
| 113 | An organization uses Tri-Secret Secure. The customer's CMK (Customer Managed Key) in AWS KMS is revoked. What happens to Snowflake data access? | Data remains accessible because Snowflake maintains a copy of the key | All data encrypted with Tri-Secret Secure becomes immediately inaccessible to everyone, including Snowflake itself — both keys are required for decryption | Only external shares and replication are affected; internal queries continue | Snowflake Support can generate an emergency key to maintain access for 72 hours | B | Tri-Secret Secure requires both Snowflake's key AND the customer's CMK to decrypt data. If the CMK is revoked, decryption is impossible for everyone — including Snowflake employees. This provides the highest level of data sovereignty: a customer can 'destroy' their data immediately by revoking their key. This is by design, not a bug. | Hard | Architect | Security |
| 114 | A new employee asks why Snowflake doesn't require vacuuming or manual statistics updates like PostgreSQL. What is the correct explanation? | Snowflake uses a much simpler storage engine that doesn't need maintenance | Snowflake's immutable micro-partitions never accumulate dead rows, and micro-partition metadata (statistics) is maintained automatically by the Cloud Services layer on every write | Vacuuming is done automatically every Sunday at midnight | Snowflake users can run VACUUM if needed, but it's optional | B | In PostgreSQL, UPDATE/DELETE marks rows as 'dead' in the same page — VACUUM cleans them up. Snowflake uses immutable micro-partitions: changes create new partitions; old ones are retired automatically by garbage collection. Statistics are maintained per-partition on every DML. No manual maintenance is needed. | Medium | Core | Architecture Overview |
| 115 | A Snowflake account has 5 running virtual warehouses of various sizes. A sixth warehouse is created but immediately suspended. How many sets of data cache (local SSD cache) exist? | 6 — one per warehouse including the suspended one | 5 — only running warehouses have active SSD cache | 1 — all warehouses share a single cache pool | 0 — local cache only exists during active query execution | B | Local SSD cache only exists on running virtual warehouses. A suspended warehouse has no active compute nodes and therefore no local cache. When the sixth warehouse starts, it begins with an empty cache. Only the 5 active warehouses maintain their own independent local caches. | Hard | Core | Architecture Overview |
| 116 | An architect must design a Snowflake solution where the total monthly compute budget is strictly capped at $10,000 with zero tolerance for overrun. The solution must also ensure BI queries are never queued. Which combination achieves both goals? | Multi-cluster warehouse (no budget cap possible) + Resource Monitor on account | Dedicated BI warehouse with Resource Monitor (Notify & Suspend Immediately at $10,000) + separate batch warehouse with its own monitor; accept that queuing may occur when BI monitor is suspended | Set a single warehouse to MAX_CONCURRENCY_LEVEL=200 with a $10,000 resource monitor | These two requirements are contradictory — guaranteeing no queuing requires unlimited scaling, which conflicts with a strict budget cap | D | Strict zero-tolerance budget cap AND zero queuing are fundamentally contradictory in a usage-based billing model. When the resource monitor suspends the warehouse at budget limit, queries WILL queue (or fail). Any solution must acknowledge this trade-off. The architect must present this conflict to stakeholders and find an acceptable compromise (e.g., alert at 80%, hard limit only at month boundary). | Hard | Architect | Architecture Overview |
| 117 | What is the Snowflake storage layer built on top of for an AWS-based account? | AWS EFS (Elastic File System) | AWS S3 (Simple Storage Service) | AWS EBS (Elastic Block Store) | AWS Glacier | B | For AWS-based Snowflake accounts, the storage layer is built on Amazon S3. However, customers cannot access this S3 bucket directly — it is owned and managed by Snowflake. For Azure accounts, it's Azure Blob Storage; for GCP, it's Google Cloud Storage. | Easy | Data Engineer | Architecture Overview |
| 118 | A table in Snowflake is described as having 'high clustering depth.' What does this indicate? | The table has many nested/complex data types | The table's data is NOT well-sorted relative to its clustering key — many partitions have overlapping value ranges, reducing pruning effectiveness | The table has been clustered too many times and needs reclustering | The table has too many rows per partition | B | Clustering depth measures how many micro-partitions overlap for a given value range. High clustering depth means many partitions could contain the same range of values — pruning eliminates fewer partitions and query performance suffers. Low clustering depth (well-clustered data) means few partitions per value range, enabling aggressive pruning. | Easy | Core | Micro-Partitions |
| 119 | Which SQL command provides information about the clustering state of a Snowflake table? | DESCRIBE TABLE ... CLUSTERING | SHOW CLUSTERING KEYS IN TABLE | SYSTEM$CLUSTERING_INFORMATION('table_name') | SELECT * FROM INFORMATION_SCHEMA.CLUSTERING_STATS | C | SYSTEM$CLUSTERING_INFORMATION('table_name') returns a JSON object with clustering metrics including average overlaps, average depth, and the percentage of partitions in various clustering states. This is the primary tool for assessing whether a clustering key is effective. | Medium | Core | Micro-Partitions |
| 120 | A table has a clustering key on (COUNTRY, ORDER_DATE). A query filters WHERE COUNTRY = 'US' AND ORDER_DATE BETWEEN '2024-01-01' AND '2024-03-31'. How effective is the clustering key? | Not effective — compound clustering keys never work well | Very effective — both filter columns match the clustering key columns in the correct order | Partially effective — COUNTRY filter benefits from clustering, but ORDER_DATE filtering within US rows may have limited benefit | Only ORDER_DATE clustering is used — the COUNTRY column is ignored | B | The compound clustering key (COUNTRY, ORDER_DATE) means data is primarily sorted by COUNTRY, then by ORDER_DATE within each country. A query filtering on both columns in the same order as the key benefits from aggressive pruning: first, all non-US partitions are eliminated; then within US partitions, those outside the date range are eliminated. This is highly effective. | Medium | Architect | Micro-Partitions |
| 121 | A Data Engineer loads data daily in batches. Each batch contains records for the past 90 days (backfill pattern). After 6 months, query performance is poor. The table has a clustering key on ORDER_DATE. What is the issue? | The clustering key needs to be on LOAD_DATE instead of ORDER_DATE | Daily backfill loads create heavily overlapping micro-partitions for old dates — each new load adds partitions for all 90 days, causing partition overlap across the entire 90-day range. Automatic Clustering will reorganize this but may lag behind the load pattern. | The table is too large for clustering — remove the clustering key | Clustering keys only work for the most recent 30 days of data | B | Backfill patterns (loading historical date ranges repeatedly) are the enemy of date-based clustering. Each daily load creates new micro-partitions containing rows for the past 90 days, leading to many overlapping partitions for any given date. Automatic Clustering will attempt to reorganize, but aggressive backfill may consume significant clustering credits. Consider loading in date-order batches or using a MERGE approach. | Hard | Data Engineer | Micro-Partitions |
| 122 | What parameter controls how long a query can wait in the warehouse queue before failing? | QUERY_TIMEOUT | STATEMENT_QUEUED_TIMEOUT_IN_SECONDS | MAX_QUEUE_WAIT_SECONDS | CONCURRENCY_QUEUE_TIMEOUT | B | STATEMENT_QUEUED_TIMEOUT_IN_SECONDS defines the maximum time (in seconds) a query can wait in the warehouse's concurrency queue. If set to 60, a query that hasn't started executing within 60 seconds of being submitted will fail with a timeout error. Default is 0 (no timeout — queries queue indefinitely). | Easy | Core | Virtual Warehouses |
| 123 | A developer wants to ensure their long-running ETL query cannot run for more than 2 hours to prevent runaway jobs. What parameter should be set? | AUTO_SUSPEND = 7200 | STATEMENT_TIMEOUT_IN_SECONDS = 7200 on the warehouse or session | MAX_RUNTIME_SECONDS = 7200 on the query | QUERY_MAX_SECONDS = 7200 in the account settings | B | STATEMENT_TIMEOUT_IN_SECONDS defines the maximum execution time for a single query/statement. Setting it to 7200 (2 hours = 7200 seconds) will cancel any query that exceeds this runtime. It can be set at the account, warehouse, user, or session level. The most specific level takes precedence. | Medium | Core | Virtual Warehouses |
| 124 | Warehouse WH1 has MAX_CONCURRENCY_LEVEL = 8. User A runs a query that takes 60 seconds. 5 seconds after User A's query starts, 8 more users each submit a query simultaneously. What happens? | All 9 queries (A + 8 new) run immediately on the warehouse | User A's query continues; 7 new queries run (filling to MAX_CONCURRENCY_LEVEL=8 total); 1 new query queues | User A's query continues; all 8 new queries queue because A is using one slot (7 slots available... but 8 queries fill them + 1 queues) | All 8 new queries queue behind User A's query | B | MAX_CONCURRENCY_LEVEL = 8 means up to 8 queries run simultaneously. User A is query #1 (slot 1). When 8 more arrive, slots 2–8 are filled by 7 of them (total = 8 running). The 8th new query queues. Running: A + 7 others (8 total). Queued: 1. | Hard | Core | Virtual Warehouses |
| 125 | A production multi-cluster warehouse is configured with Min=2, Max=8, Economy scaling. During normal business hours, 3 clusters run. Outside business hours (nights and weekends), query volume drops to near zero. What is the compute cost during off-hours? | $0 — clusters suspend when no queries run | The cost of running 2 clusters continuously (Min=2 always keeps 2 running) | The cost of running 1 cluster (the minimum collapses to 1 at night) | The cost of all 8 clusters (Max stays provisioned for readiness) | B | With Min=2, at least 2 clusters always run regardless of query volume. Outside business hours, clusters 3+ will shut down as concurrency drops, but clusters 1 and 2 remain running. This represents continuous cost for the minimum clusters. Organizations with no off-hours workloads may prefer Min=0 if they can tolerate startup latency for the first morning queries. | Medium | Architect | Virtual Warehouses |
| 126 | A data pipeline needs a warehouse to load data, immediately transform it, and write results. The pipeline runs for exactly 45 seconds. How many seconds is the warehouse billed for? | 45 seconds | 60 seconds (minimum billing period) | 90 seconds (minimum period × 2 for load and transform phases) | 120 seconds (safety margin applied automatically) | B | Minimum billing per warehouse start is 60 seconds regardless of actual usage. Even though the pipeline completes in 45 seconds, 60 seconds are billed. If the pipeline ran for 61 seconds, 61 seconds would be billed. Design: if tasks take under 60 seconds, try to batch them together to amortize the per-start minimum. | Medium | Data Engineer | Virtual Warehouses |
| 127 | A user adds a comment to a query: /* department: finance */ SELECT SUM(revenue) FROM sales. The exact same query without the comment was run 5 minutes ago and is in the Result Cache. Will the cached result be used? | Yes — Snowflake ignores comments when matching Result Cache entries | No — the comment changes the SQL text, so it's treated as a different query and the cache is bypassed | Yes — Snowflake normalizes whitespace and comments before cache lookup | It depends on the CACHE_COMMENT_SENSITIVE account setting | B | Result Cache matching is based on the exact SQL text. A comment (/* department: finance */) changes the text, so Snowflake treats it as a different query. The cache from the comment-free version is not used. This is why standardizing query text in BI tools is important for maximizing Result Cache effectiveness. | Hard | Core | Caching |
| 128 | An architect wants maximum query performance for a fixed set of executive dashboards that run every morning. The dashboards use the same SQL every day but query data that was updated overnight. What is the BEST design? | Enable a permanent Result Cache for the dashboard queries | Accept that Result Cache is invalidated by overnight data updates — performance optimization should focus on clustering, warehouse sizing, and materialized views to pre-aggregate results | Create a static snapshot table that dashboards query instead of live data | Schedule dashboard queries to run at 2 AM before business users arrive to pre-warm the cache | B | Since overnight data updates invalidate Result Cache, pre-warming alone doesn't work (stale cache before data loads, valid cache only after load). Materialized Views can pre-aggregate expensive computations and are refreshed automatically when base data changes. Combined with proper clustering, this is the most robust solution for recurring executive dashboards. | Hard | Architect | Caching |
| 129 | A company runs Standard edition and wants to set DATA_RETENTION_TIME_IN_DAYS = 7 on a critical table. What happens? | The command succeeds and 7-day retention is applied | The command fails with an error — Standard edition maximum is 1 day. Enterprise edition is required for retention > 1 day. | The command succeeds but is silently capped at 1 day | The command requires ACCOUNTADMIN role which can override edition limits | B | Standard edition hard caps DATA_RETENTION_TIME_IN_DAYS at 1. Running ALTER TABLE t SET DATA_RETENTION_TIME_IN_DAYS = 7 on Standard edition returns an error: 'Time Travel retention for table T is too high for account edition Standard.' Enterprise edition must be purchased to use retention > 1 day. | Hard | Core | Editions |
| 130 | Which statement is TRUE about Standard edition Fail-Safe? | Standard edition does not have Fail-Safe — it's an Enterprise feature | Standard edition includes 7-day Fail-Safe for all permanent tables, same as all editions | Standard edition includes only 1-day Fail-Safe (matching its Time Travel period) | Fail-Safe in Standard edition requires additional purchase | B | Fail-Safe is available in ALL Snowflake editions, always 7 days (fixed). The edition difference is only in Time Travel duration (Standard: 1 day max, Enterprise+: 90 days max). A Standard edition table with 1-day retention still has 7-day Fail-Safe after Time Travel expires. | Medium | Core | Editions |
| 131 | What type of object is a STAGE in Snowflake? | An account-level object like a warehouse | A schema-level object — it lives inside a database and schema | A database-level object — it lives directly inside a database | A special cross-database object | B | Stages (both internal and external) are schema-level objects. They live inside a database.schema namespace. For example: CREATE STAGE mydb.myschema.my_stage. Warehouses, on the other hand, are account-level objects. | Easy | Core | Object Hierarchy |
| 132 | A user queries table A in database DB1. They are currently connected to database DB2. Which query syntax is CORRECT? | SELECT * FROM A (Snowflake auto-detects the database) | SELECT * FROM DB1..A (double dot notation) | SELECT * FROM DB1.SCHEMA1.A (fully qualified name) | USE DATABASE DB1 must be run — cross-database queries are not allowed in Snowflake | C | Cross-database queries are fully supported in Snowflake using fully qualified three-part names: DATABASE.SCHEMA.OBJECT. The current USE DATABASE setting doesn't restrict access to objects in other databases — you simply reference them by their full path. Two-dot notation (DB1..A) is also sometimes used, defaulting to PUBLIC schema. | Medium | Core | Object Hierarchy |
| 133 | A developer creates a stored procedure in schema MY_SCHEMA that uses CREATE TABLE to build a result table. The procedure owner is role ENGINEER_ROLE. The procedure is called by an analyst with role ANALYST_ROLE. ANALYST_ROLE does NOT have CREATE TABLE privilege on MY_SCHEMA. Will the CREATE TABLE inside the procedure succeed? | No — the analyst's role (ANALYST_ROLE) is checked for CREATE TABLE privilege | Yes — stored procedures in Snowflake run with owner's rights by default (EXECUTE AS OWNER), so ENGINEER_ROLE's privileges apply | Only if the procedure has EXECUTE AS CALLER specified | Yes — analyst privilege doesn't matter inside stored procedures | B | Snowflake stored procedures have two execution modes: EXECUTE AS OWNER (default) uses the procedure creator/owner's role privileges, and EXECUTE AS CALLER uses the calling user's role privileges. By default (owner's rights), ENGINEER_ROLE's CREATE TABLE privilege applies, so the table creation succeeds even though ANALYST_ROLE lacks that privilege. | Hard | Core | Object Hierarchy |
| 134 | A user wants to undo all changes made to table ORDERS in the last 10 minutes. What is the correct approach? | ROLLBACK LAST 10 MINUTES ON ORDERS | CREATE OR REPLACE TABLE ORDERS AS SELECT * FROM ORDERS AT (OFFSET => -600) | RESTORE TABLE ORDERS TO 10 MINUTES AGO | ALTER TABLE ORDERS RESET TO OFFSET -600 | B | To restore a table to a previous state using Time Travel, the pattern is: CREATE OR REPLACE TABLE t AS SELECT * FROM t AT (OFFSET => -600) — this overwrites the current table with its state from 600 seconds (10 minutes) ago. Alternatively, create a new table with the historical data and swap. | Medium | Core | Time Travel |
| 135 | A schema has DATA_RETENTION_TIME_IN_DAYS = 14. A table within the schema has DATA_RETENTION_TIME_IN_DAYS = 3. What retention period applies to the table? | 14 days (schema setting overrides table setting) | 3 days (table-level setting overrides schema-level setting) | 17 days (settings are additive) | 3 days for Time Travel; 14 days for Fail-Safe | B | Lower-level (more specific) parameter settings override higher-level settings. The table-level DATA_RETENTION_TIME_IN_DAYS = 3 overrides the schema-level setting of 14 days. The table has only 3 days of Time Travel, not 14. This allows fine-grained control: set a default at schema level, override for specific tables. | Hard | Core | Time Travel |
| 136 | A Data Engineer accidentally runs UPDATE orders SET amount = 0 WHERE 1=1 (updating all rows). The error is discovered 2 hours later. The table has 90-day Time Travel. What is the FASTEST recovery? | Contact Snowflake Support for Fail-Safe recovery | CREATE OR REPLACE TABLE orders AS SELECT * FROM orders AT (OFFSET => -7200) — restore from Time Travel snapshot 2 hours ago | Run UNDROP TABLE orders (doesn't apply to updates, only drops) | Restore from an external backup in S3 | B | UNDROP only recovers dropped tables. For a mass UPDATE, Time Travel is used: the query SELECT * FROM orders AT (OFFSET => -7200) returns the table state 2 hours ago (before the bad UPDATE). Wrapping in CREATE OR REPLACE TABLE replaces the current corrupted table with the clean historical version. With 90-day retention and only 2 hours elapsed, this is straightforward. | Medium | Data Engineer | Time Travel |
| 137 | Which of the following objects CANNOT be zero-copy cloned in Snowflake? | Tables | Schemas | Databases | External Tables (read-only metadata objects pointing to external files) | D | External tables are metadata-only objects pointing to files in external storage — there's no Snowflake-managed data to clone. Other limitations: pipes (Snowpipe) and some integrations are not cloned. Tables, schemas, databases, and their contents (views, functions, stages) CAN be cloned. | Medium | Core | Zero-Copy Cloning |
| 138 | A production database is cloned daily for a data science team. The clone is deleted each evening and re-created the next morning. After 30 days, the team complains that the clone creation is increasingly slow. Why is this happening? | More data has been added to production, making clone creation slower | Clone creation time is proportional to data volume — this is expected behavior as data grows | This should NOT be happening — zero-copy clones are always instantaneous regardless of data size. If clone creation is taking longer, it might indicate underlying infrastructure issues or excessive metadata complexity from extremely large schema objects (millions of tables/partitions), not data volume. | Clones get slower each time because Snowflake tracks clone lineage | C | Zero-copy clone creation is a metadata operation and should be nearly instantaneous regardless of data volume. If clone creation of a database is slowing down over 30 days, the issue is NOT the data volume but potentially: increasing schema complexity (many new tables/objects), metadata management overhead, or a potential Snowflake platform issue. The developer's premise may be incorrect — they should measure actual clone creation time to verify. | Hard | Architect | Zero-Copy Cloning |
| 139 | What is an Internal Stage in Snowflake? | A staging area inside a user's own S3 bucket | A Snowflake-managed storage location for staging files before loading into tables | A temporary table used as intermediate storage during ETL | A schema designated for pre-production data | B | Internal Stages are Snowflake-managed storage locations. Files are uploaded to Snowflake's own cloud storage (not the customer's). Three types: Named Stages (CREATE STAGE), User Stages (one per user, ~@~user/stage), and Table Stages (one per table, @%table_name). External stages reference customer-owned cloud storage. | Easy | Data Engineer | Data Loading |
| 140 | Which command is used to load data from a Snowflake stage into a table? | LOAD INTO my_table FROM @my_stage | IMPORT INTO my_table FROM @my_stage | COPY INTO my_table FROM @my_stage | INSERT INTO my_table FROM @my_stage | C | COPY INTO is the primary command for bulk loading data from a stage into a table. Syntax: COPY INTO table_name FROM @stage_name FILE_FORMAT = (type = 'CSV'). It also supports loading from external cloud storage URLs directly. | Easy | Data Engineer | Data Loading |
| 141 | A developer runs COPY INTO but wants to load only specific files from the stage, not all files. How is this done? | Use WHERE clause: COPY INTO t FROM @s WHERE filename LIKE '2024%' | Specify files in the FILES parameter: COPY INTO t FROM @s FILES = ('file1.csv', 'file2.csv') | Move unwanted files out of the stage first | COPY INTO always loads all files — use SELECT statements to filter rows after loading | B | The FILES parameter in COPY INTO allows specifying a list of specific files to load. Alternatively, PATTERN = '.*2024.*\.csv' uses a regex to match file names. Both provide selective loading without moving files. | Medium | Data Engineer | Data Loading |
| 142 | A COPY INTO command runs with ON_ERROR = CONTINUE. 5 of 100 files have formatting errors. What is the outcome? | All 100 files fail — CONTINUE only applies to row-level errors within a file | 95 files are loaded successfully; the 5 error files are skipped and their errors are recorded in the load history and accessible via VALIDATE() | All 100 files are attempted; error rows within the 5 bad files are skipped; good rows within those files are loaded | The command fails with a warning about the 5 error files | B | ON_ERROR = CONTINUE instructs Snowflake to skip files that encounter errors and continue loading the remaining files. The 95 good files load successfully. The 5 error files are recorded as failed in load history with their error details. VALIDATE() can be used to inspect errors after the fact. Note: at the row level within a file, CONTINUE skips error rows. | Hard | Data Engineer | Data Loading |
| 143 | What is the recommended file size for optimal COPY INTO performance? | As large as possible (multi-GB files are ideal) | 100–250 MB per file | Exactly 16 MB to match micro-partition size | Less than 1 MB for parallel loading | B | Snowflake recommends files of 100–250 MB for optimal COPY INTO performance. Files in this range allow efficient parallelism while avoiding excessive metadata overhead from too many small files. Very large files (multi-GB) can limit parallelism. The 16 MB micro-partition size is the storage unit after loading, not the input file size. | Medium | Data Engineer | Data Loading |
| 144 | A Stream is created on a table with the default STREAM_TYPE. What type of stream is this? | APPEND_ONLY stream — captures only INSERT operations | STANDARD stream — captures INSERT, UPDATE, and DELETE operations | DELTA stream — captures net changes (before and after images merged) | FULL stream — captures a full snapshot of the table on each refresh | B | The default stream type is STANDARD, which captures all DML changes: INSERTs (ACTION=INSERT), DELETEs (ACTION=DELETE), and UPDATEs (as DELETE + INSERT pairs with ISUPDATE=TRUE). APPEND_ONLY streams exist for tables that only receive inserts (like event log tables) — they're more efficient because they only track new rows. | Medium | Data Engineer | Streams & Tasks |
| 145 | A Stream's STALE status is TRUE. What does this mean and what action is required? | The stream has no unconsumed data — it's 'stale' because there's nothing to process. No action needed. | The stream's offset has fallen outside the Time Travel retention window of the source table — the historical data needed to reconstruct changes is gone. The stream must be dropped and recreated. | The stream is paused because the source table is locked | The stream needs a warehouse to refresh its data — submit a query to reactivate it | B | A STALE stream means the stream's offset (the point in time from which it tracks changes) has fallen outside the source table's Time Travel retention window. The historical micro-partitions the stream needs to compute changes no longer exist. A stale stream cannot be used — it must be dropped and recreated. To prevent staleness: ensure streams are consumed regularly, or increase the table's Time Travel retention. | Hard | Data Engineer | Streams & Tasks |
| 146 | What happens to a Stream's offset after a Task successfully executes a DML statement using the stream's data? | The offset doesn't advance until the Task is manually reset | The offset advances to the current time (commits to the point after the DML completed) — consumed changes are no longer visible in the stream | The offset advances by exactly 24 hours | The offset is cleared and the stream starts capturing changes from scratch | B | When a Task executes a DML that reads a Stream within a transaction, and that transaction commits successfully, the Stream's offset advances to the current timestamp. The consumed changes are no longer visible — the stream now only shows changes since the last successful consumption. This is how Streams + Tasks implement reliable incremental processing. | Medium | Data Engineer | Streams & Tasks |
| 147 | What does 'spillage to remote storage' in a Query Profile indicate? | Data is being backed up to external storage during processing | The warehouse ran out of local SSD space and is writing intermediate results to cloud storage (S3/Blob/GCS) — a significant performance degradation indicator | The query is writing its final results to an external stage | Snowflake is auto-optimizing by offloading cold data | B | Spillage to remote storage means the warehouse exhausted both memory AND local SSD, and is writing intermediate query data (hash join tables, sort buffers) to cloud storage. Remote storage I/O is orders of magnitude slower than memory. This indicates the warehouse is too small for the query — increase the warehouse size. | Easy | Core | Performance |
| 148 | A query runs the same JOIN 5 times with different parameter values. Each run takes 10 minutes. An analyst suggests using a materialized view to pre-compute the JOIN result. What is the benefit? | Materialized views eliminate JOIN computation entirely, so each query is instant | Materialized views pre-compute and store the JOIN result. Queries against the materialized view read the stored result instead of re-computing the JOIN, dramatically reducing query time. | Materialized views cache the JOIN parameters, not the results | Materialized views convert the JOIN to a simpler UNION operation | B | Materialized Views pre-compute expensive transformations (JOINs, aggregations) and store the physical results. Queries against the MV read the pre-computed data instead of re-running the JOIN on base tables. Snowflake automatically keeps MVs fresh when base data changes. MVs require Enterprise edition. | Medium | Core | Performance |
| 149 | An architect notices that the same large aggregation query runs well the first time (15 seconds) but subsequent identical runs are taking 15 seconds again instead of using cache. Investigation shows the underlying table is updated with new rows every 2 minutes. What should the architect implement? | Increase warehouse size to speed up repeated queries | Use a Materialized View to pre-aggregate the data — the MV is refreshed automatically when new data arrives, and queries against it benefit from MV-level caching | Disable automatic updates and only refresh data hourly to preserve Result Cache | Accept 15-second query times — this is expected for frequently-updated data | B | Frequent data updates (every 2 minutes) invalidate Result Cache immediately. Materialized Views solve this because: (1) the MV stores pre-aggregated results, (2) MV refresh happens in the background when data changes, and (3) queries against a refreshed MV are fast because they read pre-computed data. The MV itself can benefit from Result Cache between refreshes. | Medium | Architect | Performance |
| 150 | A Data Engineer is optimizing a 1 TB table join. The small table has 50,000 rows (10 MB). The large table has 10 billion rows. What join optimization does Snowflake apply automatically? | Hash join — both tables are hashed on the join key | Broadcast join (shuffle replicate) — the small 10 MB table is broadcast to all compute nodes, each of which then performs a local join with its local partition of the large table | Merge join — both tables are sorted and merged | Sort-merge join — requires a clustering key on both tables | B | Snowflake's query optimizer automatically selects a broadcast join (also called a replicated join) when one table is significantly smaller than the other. The 10 MB small table is sent to every compute node. Each node then performs a local hash join with its portion of the large table, eliminating the need to redistribute the large table's data across nodes. | Hard | Data Engineer | Performance |
| 151 | Which built-in role has the highest level of privileges in a Snowflake account? | SYSADMIN | SECURITYADMIN | ACCOUNTADMIN | SUPERUSER | C | ACCOUNTADMIN is the top-level system-defined role in Snowflake. It can manage all aspects of the account: billing, users, roles, warehouses, databases, security policies, and more. Best practice: minimize the number of users assigned to ACCOUNTADMIN and use it only for administrative tasks. | Medium | Core | Security |
| 152 | A user is assigned roles ROLE_A and ROLE_B. ROLE_A has SELECT on TABLE_X. ROLE_B has no privileges on TABLE_X. The user's primary role is ROLE_B. Can the user SELECT from TABLE_X? | Yes — Snowflake automatically uses the most privileged role across all assigned roles | No — only the primary role's privileges apply in a session. The user must explicitly use ROLE_A with USE ROLE ROLE_A to access TABLE_X. | Yes — secondary roles activate automatically when a primary role lacks access | No — users with multiple roles always use the lowest-privileged role for security | B | In Snowflake, the active session uses ONE role at a time (the primary role or a role explicitly activated with USE ROLE). By default, only the primary role's privileges apply. The user must run USE ROLE ROLE_A to access TABLE_X. Note: Secondary roles (activated with USE SECONDARY ROLES ALL) can be explicitly enabled to combine privileges from all assigned roles. | Hard | Core | Security |
| 153 | What is a Snowflake Network Policy? | A policy that encrypts all network traffic | A policy that restricts Snowflake access to specified IP address ranges | A policy that controls which external services Snowflake can call | A policy that limits the number of concurrent network connections | B | Network Policies in Snowflake control access based on IP addresses. You define allowed IP ranges (and optionally blocked IP ranges). Network Policies can be applied at the account level (affects all users) or at the user level. They are a first line of defense against unauthorized access from unexpected network locations. | Easy | Core | Security |
| 154 | An organization wants analysts to see customer email addresses as 'j***@example.com' (partial masking) while data stewards see the full email. Snowflake's Dynamic Data Masking should be used. What determines which masked value a user sees? | The table's owner role | The user's current active role at query time | The database where the table lives | The type of warehouse used for the query | B | Dynamic Data Masking policies use conditions based on CURRENT_ROLE() (or IS_ROLE_IN_SESSION()) to determine which value to return. The masking function checks the active role: if the user's role is 'DATA_STEWARD', return the full value; otherwise return the masked version. Different users see different values based on their role when running the query. | Medium | Architect | Security |
| 155 | What is Snowpark? | A new Snowflake data sharing marketplace | A developer framework for building data pipelines and ML applications entirely within Snowflake using Python, Java, or Scala | A Snowflake add-on for real-time streaming analytics | A drag-and-drop ETL tool built into Snowsight | B | Snowpark is Snowflake's developer framework that allows writing data transformations and ML pipelines using Python, Java, or Scala. Code runs directly on Snowflake's compute engine — data doesn't move to external systems. It provides a DataFrame API similar to Spark/Pandas for Snowflake-native distributed processing. | Easy | Data Engineer | Snowpark |
| 156 | A Snowpark Python UDF is defined as a vectorized UDF (table function). What is the primary benefit over a scalar UDF? | Vectorized UDFs process entire batches of rows as Apache Arrow batches, matching Snowflake's columnar processing model and dramatically improving throughput vs row-by-row scalar UDFs | Vectorized UDFs run on the Cloud Services layer, bypassing warehouse billing | Vectorized UDFs can return multiple columns while scalar UDFs return one value | Vectorized UDFs can access external APIs that scalar UDFs cannot | A | Snowflake processes data in columnar batches internally (using Apache Arrow format). Vectorized UDFs receive entire batches (columns) as Arrow arrays, enabling NumPy/Pandas-style vectorized operations on entire column arrays at once. Scalar UDFs process one row at a time, invoking Python for every row — the per-row function call overhead is massive for large datasets. Vectorized UDFs can achieve 10–100x better performance. | Medium | Data Engineer | Snowpark |
| 157 | A Snowpark ML feature store pipeline needs to maintain a real-time feature table updated within 1 minute of source changes. Which Snowflake components form the most robust solution? | Hourly Tasks + COPY INTO | Streams on source tables + Tasks with SYSTEM$STREAM_HAS_DATA + Serverless Tasks for near-real-time processing (sub-minute scheduling) | Snowpipe for source loading + a View joining the source data | Materialized Views with automatic refresh + a 1-minute Task | B | For near-real-time (< 1 minute) feature updates: (1) Streams capture CDC changes on source tables instantly, (2) Tasks can be scheduled as frequently as 1 minute using SCHEDULE='1 MINUTE' or triggered as children of loading tasks, (3) SYSTEM$STREAM_HAS_DATA prevents unnecessary runs. Serverless Tasks avoid warehouse start/stop overhead for frequent short runs. | Hard | Data Engineer | Snowpark |
| 158 | What is the recommended warehouse strategy for isolating a reporting workload from an ETL workload? | Use one warehouse with separate schemas for each workload | Create separate virtual warehouses for reporting and ETL — each gets its own isolated compute resources | Use the SYSADMIN role for ETL and ANALYST role for reporting on the same warehouse | Use resource monitors to limit each workload's credit usage on a shared warehouse | B | Separate warehouses provide complete compute isolation. An ETL job consuming all resources on one warehouse has zero impact on a reporting warehouse because each warehouse has its own independent compute pool. This is one of Snowflake's core architectural advantages — unlimited virtual warehouses with no resource contention. | Easy | Architect | Architecture Design |
| 159 | A company's data architecture includes Kafka for event streaming and Snowflake for analytics. What is the recommended pattern for getting Kafka events into Snowflake? | Use Snowflake's native Kafka connector with Snowpipe or Kafka Snowflake Connector (Confluent/Snowflake) | Export Kafka events to S3 every hour and use COPY INTO | Use Snowflake's built-in Kafka listener (LISTEN TO KAFKA command) | Install Kafka inside a Snowflake virtual warehouse | A | The Snowflake Kafka Connector (available for Confluent and Apache Kafka) is the recommended integration. It uses Snowflake internal stages and Snowpipe to automatically ingest Kafka topic messages into Snowflake tables with low latency. There is no native LISTEN TO KAFKA command in Snowflake SQL. | Medium | Architect | Architecture Design |
| 160 | An organization wants to implement a 'data mesh' architecture where multiple business domains own their own Snowflake data products. Cross-domain data access must be governed, audited, and discoverable. Which Snowflake features support this architecture? | Multiple databases with cross-database joins for all sharing | Separate Snowflake accounts per domain (Organization), Secure Data Sharing for cross-domain access, Snowflake Data Marketplace for discovery, and Access History for auditing | A single account with separate schemas per domain, using Row Access Policies for isolation | External tables pointing to a shared data lake accessible by all domains | B | Data mesh on Snowflake maps naturally to: separate accounts per domain (Organization-level management), Secure Data Sharing for governed cross-domain data products (no copies, governed by provider), Snowflake Marketplace/Data Exchange for discovery of domain data products, and Access History/Query History for auditing cross-domain consumption. This provides true data mesh autonomy with platform-level governance. | Hard | Architect | Architecture Design |
| 161 | In a Snowflake medallion architecture (Bronze/Silver/Gold layers), how are layers typically implemented? | Three separate Snowflake accounts (Bronze, Silver, Gold) | Three separate databases (or schemas) within the same Snowflake account: Bronze for raw data, Silver for cleaned/conformed, Gold for aggregated/business-ready data | Three separate warehouses that each manage their own data layer | Bronze uses external tables, Silver uses regular tables, Gold uses materialized views | B | The medallion architecture in Snowflake is typically implemented as separate databases (e.g., BRONZE_DB, SILVER_DB, GOLD_DB) or schemas (e.g., RAW, CLEAN, MART) within the same account. Bronze stores raw data as-landed. Silver applies cleaning and conforming. Gold provides business-ready aggregations and star/snowflake schemas for BI consumption. Cross-database queries connect the layers. | Medium | Architect | Architecture Design |
| 162 | A table column is defined as VARIANT. What type of data can it store? | Only JSON data | Any semi-structured data format: JSON, XML, Avro, Parquet, ORC | Any data type including structured (VARCHAR, NUMBER) and semi-structured | Only unstructured binary data | B | VARIANT is Snowflake's universal semi-structured data type. It can store JSON, Avro, ORC, Parquet, and XML data natively. VARIANT columns can hold up to 16 MB per value. Snowflake provides dot and bracket notation for navigating VARIANT data: my_variant:address:city, my_variant['items'][0]. | Medium | Core | Data Types |
| 163 | Which Snowflake data type is used to store semi-structured data like JSON? | TEXT | JSON | VARIANT | OBJECT | C | VARIANT is the primary data type for semi-structured data. OBJECT is a VARIANT subtype for JSON objects specifically. ARRAY is a VARIANT subtype for arrays. In practice, VARIANT is used for columns that receive JSON, Avro, Parquet, or XML data. | Easy | Core | Data Types |
| 164 | A VARIANT column contains JSON: {'user': {'id': 123, 'name': 'Alice'}}. How is the user's name accessed in a SQL query? | json_column.user.name | json_column['user']['name'] OR json_column:user:name | EXTRACT(name FROM json_column.user) | JSON_VALUE(json_column, '$.user.name') | B | Snowflake supports two equivalent notations for navigating VARIANT data: colon notation (column:key:subkey) and bracket notation (column['key']['subkey']). Both access the same nested values. So my_col:user:name and my_col['user']['name'] both return 'Alice'. The result is a VARIANT type that can be cast: my_col:user:name::VARCHAR. | Medium | Data Engineer | Data Types |
| 165 | Snowflake supports ACID transactions. What does ACID stand for? | Authentication, Compression, Integrity, Distribution | Atomicity, Consistency, Isolation, Durability | Asynchronous, Concurrent, Independent, Distributed | Automatic, Clustered, Indexed, Deduplicated | B | ACID stands for: Atomicity (all operations in a transaction succeed or all fail), Consistency (database goes from one valid state to another), Isolation (concurrent transactions don't interfere with each other), Durability (committed data survives system failures). Snowflake supports full ACID transactions including multi-statement transactions with BEGIN/COMMIT/ROLLBACK. | Medium | Core | Transactions |
| 166 | Two sessions run simultaneously: Session 1 starts a transaction and INSERTs 1,000 rows but has NOT committed. Session 2 runs SELECT COUNT(*) on the same table. What count does Session 2 see? | The count including Session 1's 1,000 uncommitted rows (dirty read) | The count excluding Session 1's uncommitted rows — Snowflake uses snapshot isolation, so Session 2 sees only committed data | An error because the table is locked by Session 1's transaction | It depends on the ISOLATION_LEVEL setting of Session 2's transaction | B | Snowflake uses multi-version concurrency control (MVCC) with snapshot isolation. Session 2 sees the snapshot of the table as of when its transaction/statement began — uncommitted changes from other sessions are invisible. There are no dirty reads in Snowflake. Table locks don't block reads. This is enabled by Snowflake's immutable micro-partition model. | Hard | Core | Transactions |
| 167 | Which of the following is TRUE about Snowflake's compute layer? | Virtual warehouses share compute resources from a common pool | Each virtual warehouse has its own dedicated compute resources (CPU, memory, SSD) | Compute is managed by the user's cloud provider (AWS/Azure/GCP) directly | Compute resources are shared between storage and processing | B | Each virtual warehouse has its own dedicated compute cluster — its own set of CPUs, memory, and local SSD. Warehouses do not share compute resources with each other. This is what enables true workload isolation: one warehouse's resource usage cannot impact another. | Easy | Core | Architecture Overview |
| 168 | Snowflake charges for Cloud Services when they exceed 10% of daily compute. Which operations primarily consume Cloud Services credits? | COPY INTO and data loading operations | SHOW commands, query compilation/optimization, metadata queries, authentication, and infrastructure management | SELECT queries and data scanning | Storage operations like compression and garbage collection | B | Cloud Services handles: authentication (every connection), query optimization/compilation (every query), metadata operations (SHOW, DESCRIBE, INFORMATION_SCHEMA queries), infrastructure management, and transaction coordination. Heavy use of SHOW commands or metadata queries without much compute can push Cloud Services above the 10% threshold. | Medium | Core | Architecture Overview |
| 169 | A Snowflake pipeline needs to process real-time CDC events from a relational database. What is the recommended architecture? | COPY INTO from S3 every 5 minutes using a scheduled Task | Use a CDC tool (Debezium, Fivetran, Attunity) to capture changes from the source, publish to Kafka or S3, then use Snowflake Kafka Connector or Snowpipe to ingest into Snowflake | Connect directly from the source DB to Snowflake using the JDBC driver | Use Snowflake external tables pointing to the source database | B | Snowflake doesn't natively connect to source databases for CDC. The standard pattern is: (1) CDC tool extracts changes from source DB transaction logs, (2) events published to Kafka or cloud storage, (3) Snowflake Kafka Connector or Snowpipe ingests into Snowflake. This decoupled architecture handles backpressure and provides fault tolerance. | Medium | Data Engineer | Architecture Overview |
| 170 | What is the relationship between warehouse size and query execution parallelism? | Larger warehouses enable higher parallelism because they have more compute nodes — queries can process more data partitions simultaneously | Parallelism is fixed regardless of warehouse size — only query complexity affects runtime | Larger warehouses only help with memory, not parallelism | Parallelism is controlled by MAX_CONCURRENCY_LEVEL, not warehouse size | A | Virtual warehouse size directly controls parallelism. An X-Small has 1 compute node. A Small has 2. Each size up doubles the nodes. More nodes = more micro-partitions processed in parallel = faster scans, joins, and aggregations. This is why sizing up helps complex, large-data queries: the work is divided across more parallel workers. | Easy | Core | Virtual Warehouses |
| 171 | A company needs to comply with SOC 2 Type II. Which Snowflake edition is required? | Standard | Enterprise | Business Critical | All editions are SOC 2 Type II compliant — it's a platform-level certification | D | SOC 2 Type II is a Snowflake platform-level certification that applies to ALL editions, not just specific tiers. HIPAA, PCI-DSS, and FedRAMP require Business Critical (or above). SOC 2, ISO 27001, and similar certifications are Snowflake-wide. This is a common exam trap — candidates assume all compliance requires Business Critical. | Hard | Core | Editions |
| 172 | A Snowflake administrator wants to ensure that all data in a specific database is encrypted with a key that can be rotated on demand. What Snowflake feature enables this? | Automatic key rotation (built into all editions, no configuration needed) | Tri-Secret Secure with customer-managed keys (Business Critical) | TLS encryption in transit (applies to all editions automatically) | Column-level encryption using UDFs | B | Tri-Secret Secure (Business Critical) allows customers to use their own Customer Managed Key (CMK) from AWS KMS, Azure Key Vault, or GCP KMS alongside Snowflake's key for encrypting data. The customer can rotate their CMK on demand. Without Tri-Secret Secure, Snowflake's automatic key rotation occurs but is managed by Snowflake, not the customer. | Medium | Architect | Security |
| 173 | A MERGE statement runs nightly to upsert 100K rows into a 1 billion row table. Performance degrades over time. What is the MOST likely cause and solution? | The 100K rows overwhelm the table — use INSERT instead of MERGE | As the target table grows, MERGE scans more data to find matching rows. Solution: ensure the join key is a clustering key on the target table to minimize partition scans during the match phase. | MERGE is deprecated — use streams and tasks instead | Nightly runs cause cache invalidation — schedule MERGE in the morning | B | MERGE performance degrades as the target table grows because finding matching rows requires scanning more data. If the join key (e.g., ORDER_ID) is a clustering key, Snowflake can prune most partitions and find matches in only the relevant micro-partitions. Without clustering, every MERGE scans the entire 1 billion row table to find the 100K matching rows. | Medium | Data Engineer | Performance |

---
*173 questions · Last updated: February 2026*