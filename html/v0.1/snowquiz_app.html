<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>SnowflakeExamSimulator</title>
<link href="https://fonts.googleapis.com/css2?family=Syne:wght@400;600;700;800&family=Instrument+Sans:wght@400;500;600&display=swap" rel="stylesheet">
<style>
:root {
  --bg:#0b0e14; --surf:#111620; --surf2:#192030; --border:#1e2d42; --border2:#253550;
  --accent:#00d4ff; --accent-dim:#003d52; --text:#d0dff0; --muted:#637087;
  --easy:#00c896; --easy-bg:#00201a; --easy-dim:#003d2e;
  --med:#f5a623;  --med-bg:#1f1400;  --med-dim:#3d2800;
  --hard:#ff4757; --hard-bg:#200010; --hard-dim:#3d0020;
  --r:12px;
}
*,*::before,*::after{box-sizing:border-box;margin:0;padding:0}
body{font-family:'Instrument Sans',sans-serif;background:var(--bg);color:var(--text);min-height:100vh;overflow-x:hidden}

/* â”€â”€ LOAD SCREEN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
#ls{position:fixed;inset:0;background:var(--bg);display:flex;flex-direction:column;align-items:center;justify-content:center;gap:24px;z-index:1000;transition:opacity .4s,visibility .4s;padding:20px}
#ls.gone{opacity:0;visibility:hidden;pointer-events:none}
.logo{font-family:'Syne',sans-serif;font-size:52px;font-weight:800;background:linear-gradient(135deg,var(--accent),#7b61ff);-webkit-background-clip:text;-webkit-text-fill-color:transparent;letter-spacing:-.03em;line-height:1}
.logo-sub{font-size:12px;color:var(--muted);letter-spacing:.18em;text-transform:uppercase;margin-top:5px;text-align:center}

/* drop zone */
.dz{
  width:480px;max-width:94vw;
  border:2px dashed var(--border2);border-radius:20px;
  padding:40px 32px;text-align:center;cursor:pointer;
  background:var(--surf);transition:all .25s;position:relative;
}
.dz:hover,.dz.over{border-color:var(--accent);background:rgba(0,212,255,.04);box-shadow:0 0 48px rgba(0,212,255,.12);transform:translateY(-2px)}
.dz-icon{font-size:44px;margin-bottom:14px}
.dz-title{font-family:'Syne',sans-serif;font-size:19px;font-weight:700;margin-bottom:8px}
.dz-hint{font-size:13px;color:var(--muted);line-height:1.7}
.dz-hint strong{color:var(--accent)}
.dz-pattern{
  display:inline-block;margin-top:12px;
  font-family:monospace;font-size:13px;
  background:var(--surf2);color:var(--accent);
  border:1px solid var(--accent-dim);border-radius:6px;
  padding:4px 14px;letter-spacing:.02em;
}
#fi{display:none}

/* loaded files list */
.files-list{
  width:480px;max-width:94vw;
  background:var(--surf);border:1px solid var(--border);border-radius:var(--r);
  overflow:hidden;display:none;
}
.files-list-header{
  display:flex;align-items:center;justify-content:space-between;
  padding:10px 16px;background:var(--surf2);border-bottom:1px solid var(--border);
  font-family:'Syne',sans-serif;font-size:11px;font-weight:700;
  letter-spacing:.1em;text-transform:uppercase;color:var(--muted);
}
.files-list-header span{color:var(--easy);font-size:13px}
.file-row{display:flex;align-items:center;gap:10px;padding:9px 16px;border-bottom:1px solid var(--border);font-size:13px}
.file-row:last-child{border-bottom:none}
.file-icon{font-size:16px;flex-shrink:0}
.file-name{flex:1;color:var(--text);font-family:monospace;font-size:12px}
.file-count{font-family:'Syne',sans-serif;font-size:11px;color:var(--easy);background:var(--easy-bg);border:1px solid var(--easy-dim);border-radius:4px;padding:1px 7px;white-space:nowrap}
.file-err{font-family:'Syne',sans-serif;font-size:11px;color:var(--hard);background:var(--hard-bg);border:1px solid var(--hard-dim);border-radius:4px;padding:1px 7px}

.load-btn{
  padding:12px 40px;border-radius:10px;
  background:var(--accent);border:none;color:#000;
  font-family:'Syne',sans-serif;font-size:15px;font-weight:700;
  cursor:pointer;transition:all .2s;display:none;
}
.load-btn:hover{background:#00b8d9;transform:translateY(-1px)}

.fmt{width:480px;max-width:94vw;background:var(--surf);border:1px solid var(--border);border-radius:var(--r);padding:16px 20px;font-size:12px;color:var(--muted);line-height:1.8}
.fmt b{color:var(--text);font-family:'Syne',sans-serif;font-size:10px;letter-spacing:.1em;text-transform:uppercase;display:block;margin-bottom:6px}
.fmt-cols{display:flex;flex-wrap:wrap;gap:4px;margin-top:6px}
.fc{background:var(--surf2);color:var(--accent);border-radius:4px;padding:2px 7px;font-family:monospace;font-size:11px}
.err{color:var(--hard);font-size:13px;background:var(--hard-bg);border:1px solid var(--hard-dim);border-radius:8px;padding:10px 16px;max-width:480px;display:none}

/* â”€â”€ APP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
#app{display:none;flex-direction:column;min-height:100vh}
#app.on{display:flex}

.topbar{height:54px;background:var(--surf);border-bottom:1px solid var(--border);display:flex;align-items:center;padding:0 20px;gap:12px;position:sticky;top:0;z-index:50;flex-wrap:nowrap;overflow:hidden}
.tlogo{font-family:'Syne',sans-serif;font-weight:800;font-size:17px;background:linear-gradient(135deg,var(--accent),#7b61ff);-webkit-background-clip:text;-webkit-text-fill-color:transparent;white-space:nowrap;flex-shrink:0}
.tfiles{display:flex;gap:6px;flex:1;overflow:hidden;min-width:0}
.tfile-tag{font-size:11px;color:var(--muted);background:var(--surf2);border:1px solid var(--border);border-radius:5px;padding:2px 8px;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;max-width:130px;flex-shrink:0}
.tfile-tag.more{color:var(--accent);border-color:var(--accent-dim);background:var(--accent-dim)}
.tstats{display:flex;gap:12px;margin-left:auto;flex-shrink:0}
.ts{font-size:12px;color:var(--muted);display:flex;align-items:center;gap:5px;white-space:nowrap}
.ts strong{color:var(--text);font-family:'Syne',sans-serif}
.gbtn{background:transparent;border:1px solid var(--border2);color:var(--muted);font-size:12px;border-radius:6px;padding:4px 10px;cursor:pointer;transition:all .2s;font-family:'Instrument Sans',sans-serif;white-space:nowrap;flex-shrink:0}
.gbtn:hover{border-color:var(--accent);color:var(--accent)}

.body{display:flex;flex:1;overflow:hidden}

/* sidebar */
.sb{width:256px;min-width:256px;background:var(--surf);border-right:1px solid var(--border);overflow-y:auto;padding:16px;display:flex;flex-direction:column;gap:20px}
.sblk h4{font-family:'Syne',sans-serif;font-size:10px;font-weight:700;letter-spacing:.14em;text-transform:uppercase;color:var(--muted);margin-bottom:8px}
.ring-wrap{display:flex;flex-direction:column;align-items:center;gap:5px}
.rpct{font-family:'Syne',sans-serif;font-size:22px;font-weight:700;color:var(--accent)}
.rlbl{font-size:11px;color:var(--muted)}
.sgrid{display:grid;grid-template-columns:1fr 1fr;gap:8px}
.spill{background:var(--surf2);border:1px solid var(--border);border-radius:10px;padding:10px 8px;text-align:center}
.spill .v{font-family:'Syne',sans-serif;font-size:20px;display:block}
.spill .l{font-size:10px;color:var(--muted);margin-top:2px}
.vc{color:var(--easy)}.vw{color:var(--hard)}.vs{color:var(--med)}.va{color:var(--accent)}
.chips{display:flex;flex-wrap:wrap;gap:6px}
.chip{display:inline-flex;align-items:center;gap:5px;padding:5px 10px;border-radius:20px;border:1px solid var(--border2);background:transparent;color:var(--muted);font-size:12px;cursor:pointer;transition:all .18s;font-family:'Instrument Sans',sans-serif;white-space:nowrap}
.chip:hover{border-color:var(--accent);color:var(--text)}
.chip.on{background:var(--accent-dim);border-color:var(--accent);color:var(--accent)}
.chip .n{font-family:'Syne',sans-serif;font-size:10px;opacity:.7}
.dot{width:7px;height:7px;border-radius:50%;display:inline-block;flex-shrink:0}
.qf-btn{display:flex;align-items:center;justify-content:space-between;width:100%;padding:8px 10px;background:transparent;border:1px solid var(--border);border-radius:8px;color:var(--muted);font-size:12px;cursor:pointer;transition:all .18s;font-family:'Instrument Sans',sans-serif;text-align:left}
.qf-btn:hover{border-color:var(--border2);color:var(--text)}
.qf-btn.on{border-color:var(--accent);color:var(--accent);background:rgba(0,212,255,.05)}
.qf-btn .n{font-family:'Syne',sans-serif;font-size:11px;background:var(--surf2);border-radius:4px;padding:1px 6px}

/* main */
.main{flex:1;overflow-y:auto;padding:20px 24px;display:flex;flex-direction:column;gap:14px}
.toolbar{display:flex;gap:8px;align-items:center;flex-wrap:wrap}
.btn{display:inline-flex;align-items:center;gap:6px;padding:8px 16px;border-radius:8px;border:1px solid var(--border2);background:var(--surf);color:var(--text);font-size:13px;font-weight:500;cursor:pointer;transition:all .18s;font-family:'Instrument Sans',sans-serif}
.btn:hover{border-color:var(--accent);color:var(--accent)}
.btn.pr{background:var(--accent);border-color:var(--accent);color:#000;font-weight:700}
.btn.pr:hover{background:#00b8d9}
.btn.dr{border-color:var(--hard-dim);color:var(--hard)}
.btn.dr:hover{background:var(--hard-bg)}
.btn:disabled{opacity:.35;cursor:default;pointer-events:none}
.sw{flex:1;max-width:280px;display:flex;align-items:center;gap:8px;background:var(--surf);border:1px solid var(--border2);border-radius:8px;padding:0 12px;transition:border-color .18s}
.sw:focus-within{border-color:var(--accent)}
.sw input{background:transparent;border:none;outline:none;color:var(--text);font-size:13px;padding:8px 0;width:100%;font-family:'Instrument Sans',sans-serif}
.sw input::placeholder{color:var(--muted)}
.rc{font-size:12px;color:var(--muted);margin-left:auto;font-family:'Syne',sans-serif}

/* browse */
#bv{display:flex;flex-direction:column;gap:10px}
#bv.hide{display:none}
.qcard{background:var(--surf);border:1px solid var(--border);border-radius:var(--r);padding:16px 18px;cursor:pointer;transition:border-color .18s,transform .18s;position:relative}
.qcard:hover{border-color:var(--border2);transform:translateX(3px)}
.qcard-meta{display:flex;gap:7px;flex-wrap:wrap;align-items:center;margin-bottom:9px}
.qcard-text{font-size:14px;line-height:1.55;color:#c8dcf0}
.qcard-num{position:absolute;top:14px;right:14px;font-family:'Syne',sans-serif;font-size:11px;color:var(--muted)}
.qcard.ok{border-left:3px solid var(--easy)}
.qcard.miss{border-left:3px solid var(--hard)}
.badge{font-size:10px;font-weight:600;font-family:'Syne',sans-serif;padding:3px 9px;border-radius:20px;letter-spacing:.04em;white-space:nowrap}
.be{background:var(--easy-bg);border:1px solid var(--easy-dim);color:var(--easy)}
.bm{background:var(--med-bg);border:1px solid var(--med-dim);color:var(--med)}
.bh{background:var(--hard-bg);border:1px solid var(--hard-dim);color:var(--hard)}
.bx{background:rgba(0,212,255,.08);border:1px solid rgba(0,212,255,.25);color:var(--accent)}
.bt{background:rgba(255,255,255,.03);border:1px solid var(--border);color:var(--muted)}
.bsrc{background:rgba(123,97,255,.1);border:1px solid rgba(123,97,255,.3);color:#a78bfa}
.bok{background:var(--easy-bg);border:1px solid var(--easy-dim);color:var(--easy)}
.bno{background:var(--hard-bg);border:1px solid var(--hard-dim);color:var(--hard)}

/* quiz */
#qv{display:none;flex-direction:column;gap:16px}
#qv.on{display:flex}
.qbar{display:flex;align-items:center;gap:14px}
.qtrack{flex:1;height:3px;background:var(--surf2);border-radius:2px}
.qfill{height:100%;border-radius:2px;background:linear-gradient(90deg,var(--accent),#7b61ff);transition:width .4s cubic-bezier(.4,0,.2,1)}
.qcnt{font-family:'Syne',sans-serif;font-size:12px;color:var(--muted);white-space:nowrap}
.qc{background:var(--surf);border:1px solid var(--border);border-radius:16px;padding:28px 30px;animation:up .3s cubic-bezier(.4,0,.2,1)}
@keyframes up{from{opacity:0;transform:translateY(10px)}to{opacity:1;transform:translateY(0)}}
.qmeta{display:flex;gap:7px;flex-wrap:wrap;margin-bottom:14px}
.qidx{font-size:11px;color:var(--muted);margin-bottom:6px;font-family:'Syne',sans-serif}
.qtxt{font-size:17px;font-weight:500;line-height:1.65;color:#e0eeff;margin-bottom:24px}
.opts{display:flex;flex-direction:column;gap:10px}
.opt{display:flex;align-items:flex-start;gap:14px;padding:14px 16px;border-radius:10px;background:var(--surf2);border:1px solid var(--border);cursor:pointer;transition:all .18s;text-align:left;font-family:'Instrument Sans',sans-serif;font-size:14px;color:var(--text);line-height:1.5}
.opt:hover:not(:disabled){border-color:var(--accent);background:rgba(0,212,255,.05)}
.opt:disabled{cursor:default}
.ol{width:28px;min-width:28px;height:28px;border-radius:7px;background:var(--surf);border:1px solid var(--border2);display:flex;align-items:center;justify-content:center;font-family:'Syne',sans-serif;font-size:11px;font-weight:700;flex-shrink:0;transition:all .18s}
.opt.cor{border-color:var(--easy);background:var(--easy-bg);color:#a0f4d4}
.opt.cor .ol{background:var(--easy);color:#000;border-color:var(--easy)}
.opt.wrg{border-color:var(--hard);background:var(--hard-bg);color:#ffb0b8}
.opt.wrg .ol{background:var(--hard);color:#fff;border-color:var(--hard)}
.expl{margin-top:14px;padding:14px 16px;background:rgba(0,212,255,.05);border:1px solid rgba(0,212,255,.18);border-left:4px solid var(--accent);border-radius:8px;font-size:13px;line-height:1.75;color:#8ec8e8;animation:up .25s ease}
.expl strong{color:var(--accent)}
.qnav{display:flex;justify-content:space-between;align-items:center;margin-top:4px}
.bmbtn{background:transparent;border:1px solid var(--border2);border-radius:8px;padding:8px 14px;font-size:16px;color:var(--muted);cursor:pointer;transition:all .18s}
.bmbtn:hover,.bmbtn.on{color:var(--med);border-color:var(--med);background:var(--med-bg)}
.nbtns{display:flex;gap:8px}
.empty{text-align:center;padding:64px 20px;color:var(--muted)}
.empty .ico{font-size:52px;margin-bottom:16px}
.empty h3{font-family:'Syne',sans-serif;font-size:18px;color:var(--text);margin-bottom:8px}

/* modal */
.mbg{display:none;position:fixed;inset:0;background:rgba(0,0,0,.85);backdrop-filter:blur(10px);z-index:200;align-items:center;justify-content:center}
.mbg.on{display:flex}
.modal{background:var(--surf);border:1px solid var(--border2);border-radius:20px;padding:40px;max-width:460px;width:90%;text-align:center;animation:up .3s ease}
.modal h2{font-family:'Syne',sans-serif;font-size:26px;font-weight:700;margin-bottom:6px}
.modal p{font-size:14px;color:var(--muted);margin-bottom:24px}
.mgrd{display:grid;grid-template-columns:1fr 1fr 1fr;gap:12px;margin-bottom:28px}
.mi{background:var(--surf2);border-radius:10px;padding:14px 8px}
.mi .v{font-family:'Syne',sans-serif;font-size:24px;display:block}
.mi .l{font-size:11px;color:var(--muted);margin-top:3px}
.mbtns{display:flex;gap:10px;justify-content:center}

::-webkit-scrollbar{width:5px}::-webkit-scrollbar-track{background:transparent}::-webkit-scrollbar-thumb{background:var(--border2);border-radius:3px}
</style>
</head>
<body>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â• LOAD SCREEN â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<div id="ls">
  <div style="text-align:center">
    <div class="logo">SnowflakeExamSimulator</div>
    <div class="logo-sub">Drop your knowledge base files Â· Start practising</div>
  </div>

  <!-- drop zone -->
  <div class="dz" id="dz" onclick="document.getElementById('fi').click()">
    <div class="dz-icon">ğŸ“‚</div>
    <div class="dz-title">Drop your knowledge base files here</div>
    <div class="dz-hint">
      or <strong>click to browse</strong> â€” select one or multiple files<br>
      Accepts <strong>.yml/.yaml</strong> &nbsp;Â·&nbsp; <strong>.md</strong> Markdown tables &nbsp;Â·&nbsp; <strong>.csv</strong> spreadsheet exports
    </div>
    <div class="dz-pattern">snowflake_kb_*.yml &nbsp;Â·&nbsp; knowledgebase_*.md</div>
    <input type="file" id="fi" accept=".md,.csv,.txt,.yml,.yaml" multiple>
  </div>

  <!-- staged files preview -->
  <div class="files-list" id="files-list">
    <div class="files-list-header">
      Files staged <span id="staged-count">0 questions</span>
    </div>
    <div id="files-rows"></div>
  </div>

  <!-- launch button -->
  <button class="load-btn" id="load-btn" onclick="launchApp()">â–¶ Load All &amp; Start</button>

  <!-- format reminder -->
  <div class="fmt">
    <b>Supported Formats</b>
    YAML files (<code style="color:var(--accent)">snowflake_kb_*.yml</code>) or Markdown table files. MD required columns:
    <div class="fmt-cols">
      <span class="fc">No</span><span class="fc">Question</span>
      <span class="fc">Option A</span><span class="fc">Option B</span>
      <span class="fc">Option C</span><span class="fc">Option D</span>
      <span class="fc">Answer</span><span class="fc">Explanation</span>
      <span class="fc">Level</span><span class="fc">Exam</span><span class="fc">Topic</span>
    </div>
    <div style="margin-top:8px;font-size:11px;color:var(--muted)">
      Name files as <code style="color:var(--accent)">knowledgebase_topic.md</code> â€” e.g.
      <code style="color:var(--accent)">knowledgebase_architecture.md</code>,
      <code style="color:var(--accent)">knowledgebase_security.md</code> â€¦<br>
      Drop all of them at once and the app merges everything automatically.
    </div>
  </div>

  <div class="err" id="err"></div>
</div>

<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â• APP â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
<div id="app">
  <header class="topbar">
    <div class="tlogo">SnowflakeExamSimulator</div>
    <div class="tfiles" id="tb-files"></div>
    <div class="tstats">
      <div class="ts">Questions <strong id="tb-n">0</strong></div>
      <div class="ts">Answered <strong id="tb-a">0</strong></div>
      <div class="ts">Accuracy <strong id="tb-p">â€”</strong></div>
    </div>
    <button class="gbtn" onclick="chgFile()">â‡„ Change Files</button>
  </header>

  <div class="body">
    <aside class="sb">
      <div class="sblk">
        <div class="ring-wrap">
          <svg width="90" height="90" viewBox="0 0 90 90">
            <circle cx="45" cy="45" r="38" fill="none" stroke="#192030" stroke-width="7"/>
            <circle id="ring" cx="45" cy="45" r="38" fill="none" stroke="var(--accent)"
              stroke-width="7" stroke-linecap="round"
              stroke-dasharray="238.76" stroke-dashoffset="238.76"
              transform="rotate(-90 45 45)" style="transition:stroke-dashoffset .5s"/>
          </svg>
          <div class="rpct" id="rpct">0%</div>
          <div class="rlbl" id="rlbl">0 of 0</div>
        </div>
      </div>
      <div class="sblk">
        <div class="sgrid">
          <div class="spill"><span class="v vc" id="sc-c">0</span><span class="l">Correct</span></div>
          <div class="spill"><span class="v vw" id="sc-w">0</span><span class="l">Wrong</span></div>
          <div class="spill"><span class="v vs" id="sc-s">0</span><span class="l">Remaining</span></div>
          <div class="spill"><span class="v va" id="sc-a">â€”</span><span class="l">Accuracy</span></div>
        </div>
      </div>

      <!-- Source filter (one chip per file) -->
      <div class="sblk" id="sb-src"><h4>Knowledge Base</h4><div class="chips" id="c-src"></div></div>
      <div class="sblk"><h4>Exam</h4><div class="chips" id="c-exam"></div></div>
      <div class="sblk"><h4>Difficulty</h4><div class="chips" id="c-lvl"></div></div>
      <div class="sblk"><h4>Topic</h4><div class="chips" id="c-topic"></div></div>

      <div class="sblk">
        <h4>Quick Filters</h4>
        <div style="display:flex;flex-direction:column;gap:6px">
          <button class="qf-btn" id="qf-bm" onclick="toggleSp('bm')">â˜… Bookmarked <span class="n" id="n-bm">0</span></button>
          <button class="qf-btn" id="qf-wr" onclick="toggleSp('wr')">âœ— Wrong Answers <span class="n" id="n-wr">0</span></button>
          <button class="qf-btn" id="qf-ua" onclick="toggleSp('ua')">â—‹ Not Answered <span class="n" id="n-ua">0</span></button>
        </div>
      </div>
    </aside>

    <main class="main">
      <div class="toolbar">
        <button class="btn pr" onclick="startQuiz()">â–¶ Start Quiz</button>
        <button class="btn" onclick="showBrowse()">â˜° Browse</button>
        <button class="btn" onclick="doShuffle()">â‡„ Shuffle</button>
        <button class="btn dr" onclick="doReset()">â†º Reset</button>
        <div class="sw">
          <span style="color:var(--muted);font-size:15px">âŒ•</span>
          <input type="text" placeholder="Search questionsâ€¦" id="srch" oninput="applyFilters()">
        </div>
        <span class="rc" id="rc"></span>
      </div>

      <div id="bv"><div id="ql"></div></div>

      <div id="qv">
        <div class="qbar">
          <button class="btn" style="padding:5px 12px;font-size:12px" onclick="showBrowse()">â† Back</button>
          <div class="qtrack"><div class="qfill" id="qfill"></div></div>
          <div class="qcnt" id="qcnt">1 / 0</div>
        </div>
        <div class="qc" id="qc">
          <div class="qmeta" id="qmeta"></div>
          <div class="qidx"  id="qidx"></div>
          <div class="qtxt"  id="qtxt"></div>
          <div class="opts"  id="qopts"></div>
          <div id="qexpl"></div>
        </div>
        <div class="qnav">
          <button class="bmbtn" id="bmbtn" onclick="toggleBm()">â˜†</button>
          <div class="nbtns">
            <button class="btn" id="btn-p" onclick="qPrev()">â† Prev</button>
            <button class="btn pr" id="btn-n" onclick="qNext()">Next â†’</button>
            <button class="btn pr" id="btn-f" style="display:none" onclick="finishQuiz()">âœ“ Finish</button>
          </div>
        </div>
      </div>
    </main>
  </div>
</div>

<!-- results modal -->
<div class="mbg" id="modal">
  <div class="modal">
    <svg style="display:block;margin:0 auto 20px" width="110" height="110" viewBox="0 0 110 110">
      <circle cx="55" cy="55" r="46" fill="none" stroke="#192030" stroke-width="9"/>
      <circle id="mring" cx="55" cy="55" r="46" fill="none" stroke="var(--easy)"
        stroke-width="9" stroke-linecap="round" stroke-dasharray="289.03" stroke-dashoffset="289.03"
        transform="rotate(-90 55 55)" style="transition:stroke-dashoffset 1s .3s ease"/>
    </svg>
    <h2 id="m-t">Quiz Complete!</h2>
    <p id="m-s">Here's how you did</p>
    <div class="mgrd">
      <div class="mi"><span class="v vc" id="m-c">0</span><span class="l">Correct</span></div>
      <div class="mi"><span class="v vw" id="m-w">0</span><span class="l">Wrong</span></div>
      <div class="mi"><span class="v va" id="m-p">0%</span><span class="l">Score</span></div>
    </div>
    <div class="mbtns">
      <button class="btn" onclick="revWrong()">Review Wrong</button>
      <button class="btn pr" onclick="closeModal()">Continue</button>
    </div>
  </div>
</div>

<script>
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  YAML PARSER (for snowflake_kb_*.yml format)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
function parseYAML(text) {
  const blocks = text.split(/\n  - Question No:/);
  const questions = [];
  for (let i = 1; i < blocks.length; i++) {
    const block = blocks[i];
    const getField = (pattern) => {
      const m = block.match(pattern);
      return m ? m[1].trim() : '';
    };
    const question = getField(/    question: (.+)/);
    if (!question) continue;
    // Parse options
    const opts = [];
    const optRe = /      option ([A-D]): (.+)/g;
    let om;
    while ((om = optRe.exec(block)) !== null) {
      opts.push({ key: om[1], val: om[2].trim() });
    }
    opts.sort((a, b) => a.key.localeCompare(b.key));
    const options = opts.map(o => o.val);
    if (options.length < 2) continue;
    // Answer
    const ansMatch = block.match(/    correct Answer: option ([A-D])/);
    const ansIdx = ansMatch ? { A: 0, B: 1, C: 2, D: 3 }[ansMatch[1]] ?? 0 : 0;
    // Explanation (block scalar â€” collect indented lines after "explanation: >")
    let explanation = '';
    const expStartIdx = block.indexOf('    explanation: >');
    if (expStartIdx !== -1) {
      const afterExp = block.slice(expStartIdx + '    explanation: >'.length);
      const expLines = [];
      for (const line of afterExp.split('\n')) {
        if (line.startsWith('      ') || line.trim() === '') {
          expLines.push(line.trim());
        } else if (expLines.length > 0) {
          break;
        }
      }
      explanation = expLines.join(' ').replace(/\s+/g, ' ').trim();
    }
    const level    = getField(/    difficulty level: (.+)/) || 'Medium';
    const topic    = getField(/    topic: (.+)/) || 'General';
    const exam     = getField(/    exam: (.+)/) || 'General';
    questions.push({ question, options, answer: ansIdx, explanation, level, topic, exam });
  }
  return questions.length ? questions : null;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  BUNDLED KB â€” auto-loaded on startup
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
const BUNDLED_YAML_FILENAME = 'snowflake_kb_architecture.yml';
const BUNDLED_YAML_CONTENT = `Topic Name: Snowflake Architecture
Sub Topic Name: Architecture Overview, Micro-Partitions, Virtual Warehouses, Caching, Editions, Object Hierarchy, Time Travel, Zero-Copy Cloning, Multi-Cluster Warehouses, Resource Monitors, Connectivity, Streams & Tasks, Snowpipe, Data Sharing, Replication, Performance, Security, Snowpark, Data Loading, Transactions, Data Types
Total Question Count: 173

all questions:

  - Question No: "001"
    question: Snowflake is best described as which type of cloud service?
    their options:
      option A: IaaS (Infrastructure as a Service)
      option B: PaaS (Platform as a Service)
      option C: SaaS (Software as a Service)
      option D: FaaS (Function as a Service)
    correct Answer: option C
    explanation: >
      Snowflake is a SaaS platform â€” customers use it via a web interface or connectors without managing any underlying infrastructure, servers, OS, or software. 
      Snowflake handles all patching, upgrades, and scaling automatically. 
    difficulty level: Easy
    topic: Architecture Overview
    exam: Core

  - Question No: "002"
    question: Which THREE cloud providers does Snowflake natively support?
    their options:
      option A: AWS, Azure, GCP
      option B: AWS, Azure, Oracle Cloud
      option C: AWS, GCP, IBM Cloud
      option D: Azure, GCP, Alibaba Cloud
    correct Answer: option A
    explanation: >
      Snowflake runs natively on AWS (Amazon Web Services), Microsoft Azure, and GCP (Google Cloud Platform). 
      It does not natively run on Oracle Cloud, IBM Cloud, or Alibaba Cloud. 
    difficulty level: Easy
    topic: Architecture Overview
    exam: Core

  - Question No: "003"
    question: What is the key architectural innovation that distinguishes Snowflake from traditional databases?
    their options:
      option A: Columnar storage format
      option B: Separation of storage and compute
      option C: In-memory processing
      option D: Shared-nothing architecture
    correct Answer: option B
    explanation: >
      While columnar storage is a feature, the defining innovation is the complete separation of storage and compute. 
      This allows each to scale independently â€” more processing power without buying more storage, and vice versa. 
    difficulty level: Easy
    topic: Architecture Overview
    exam: Core

  - Question No: "004"
    question: How many layers does Snowflake's architecture consist of?
    their options:
      option A: 2
      option B: 3
      option C: 4
      option D: 5
    correct Answer: option B
    explanation: >
      Snowflake's architecture has exactly THREE layers: (1) Database Storage, (2) Query Processing / Compute (Virtual Warehouses), and (3) Cloud Services. 
      Each layer is independently scalable and managed. 
    difficulty level: Easy
    topic: Architecture Overview
    exam: Core

  - Question No: "005"
    question: Which Snowflake architecture layer handles query optimization and authentication?
    their options:
      option A: Database Storage
      option B: Virtual Warehouses
      option C: Cloud Services
      option D: Metadata Layer
    correct Answer: option C
    explanation: >
      The Cloud Services layer is the intelligence layer. 
      It handles authentication, authorization (RBAC), query parsing and optimization, metadata management, transaction management, and infrastructure coordination. 
      It runs 24/7 without user provisioning. 
    difficulty level: Easy
    topic: Architecture Overview
    exam: Core

  - Question No: "006"
    question: What format does Snowflake use to store table data internally?
    their options:
      option A: Row-based format
      option B: JSON format
      option C: Columnar (column-based) format
      option D: Parquet files you can access directly
    correct Answer: option C
    explanation: >
      Snowflake stores all data in a proprietary columnar format. 
      Each column is stored separately, enabling efficient reads (only needed columns are scanned), superior compression (similar values compress better), and effective partition pruning. 
    difficulty level: Easy
    topic: Architecture Overview
    exam: Core

  - Question No: "007"
    question: A company migrates from an on-premises Oracle database to Snowflake. The DBA wants to manually tune indexes, manage buffer pools, and configure tablespaces. What should the DBA expect?
    their options:
      option A: All these operations are available in Snowflake with equivalent commands
      option B: These concepts don't exist in Snowflake â€” the Cloud Services layer handles all optimization automatically
      option C: Only indexes need to be migrated; buffer pools and tablespaces are auto-managed
      option D: The DBA must use Snowpark to implement custom optimization logic
    correct Answer: option B
    explanation: >
      Snowflake has no traditional indexes, buffer pools, or tablespaces. 
      The Cloud Services layer automatically handles query optimization using micro-partition metadata (min/max statistics). 
      DBAs cannot and do not need to manually tune these low-level storage parameters. 
    difficulty level: Medium
    topic: Architecture Overview
    exam: Core

  - Question No: "008"
    question: Multiple virtual warehouses are running simultaneously against the same Snowflake database. What is TRUE about data access?
    their options:
      option A: Each warehouse has its own private copy of the data to prevent conflicts
      option B: All warehouses read from the same centralized storage with no data locking between them
      option C: Warehouses must take turns reading data to prevent corruption
      option D: Only one warehouse can read a table at a time; others must queue
    correct Answer: option B
    explanation: >
      This is a core Snowflake differentiator. 
      All virtual warehouses share centralized storage. 
      Multiple warehouses can read the same tables simultaneously without any locking or contention because the storage layer is completely decoupled from compute. 
      This enables true multi-workload isolation. 
    difficulty level: Medium
    topic: Architecture Overview
    exam: Core

  - Question No: "009"
    question: Which layer in Snowflake's architecture is billed per second of active use?
    their options:
      option A: Cloud Services
      option B: Database Storage
      option C: Query Processing (Virtual Warehouses)
      option D: Metadata Layer
    correct Answer: option C
    explanation: >
      Virtual Warehouses (Query Processing layer) are billed per second when actively running, with a 60-second minimum per start. 
      Storage is billed per TB/month continuously. 
      Cloud Services are free up to 10% of compute usage. 
    difficulty level: Medium
    topic: Architecture Overview
    exam: Core

  - Question No: "010"
    question: A Snowflake account's daily metrics show: Compute credits = $0 (all warehouses suspended all day), Storage = $50, Cloud Services = $15. What is the total bill?
    their options:
      option A: $65 (all charges apply)
      option B: $50 (storage only; Cloud Services free when no compute used)
      option C: $65 but with a $15 Cloud Services overage beyond the 10% threshold
      option D: $50 storage + $15 Cloud Services = $65, but the 10% threshold doesn't apply when compute is $0
    correct Answer: option D
    explanation: >
      The 10% free threshold for Cloud Services is calculated as 10% of daily compute credits. 
      With $0 compute, the free threshold is $0, meaning ALL Cloud Services usage is billable. 
      Total = $50 (storage) + $15 (Cloud Services) = $65. 
      This is a critical edge case â€” low or zero compute usage means Cloud Services can become a significant cost. 
    difficulty level: Hard
    topic: Architecture Overview
    exam: Core

  - Question No: "011"
    question: An organization uses Snowflake on AWS us-east-1. They want to add Snowflake capacity in Europe. What must they do?
    their options:
      option A: Create additional virtual warehouses â€” they can serve any geography
      option B: Provision a new Snowflake account in an EU region
      option C: Enable geo-distribution in their existing account settings
      option D: Use Snowflake's Global Processing feature to route EU queries
    correct Answer: option B
    explanation: >
      Each Snowflake account exists within a single cloud provider and region. 
      To have Snowflake running in Europe, you must create a new account in a European region. 
      Organizations (the top-level grouping) can manage multiple accounts across regions and clouds. 
    difficulty level: Easy
    topic: Architecture Overview
    exam: Architect

  - Question No: "012"
    question: An architect is designing a disaster-tolerant Snowflake system. They want to ensure data survives a complete cloud region failure. Which architecture addresses this?
    their options:
      option A: Multi-cluster warehouse with 10 maximum clusters in the primary region
      option B: Database replication to a secondary Snowflake account in a different region with failover enabled
      option C: Enabling Fail-Safe on all critical tables (7-day retention)
      option D: Setting DATA_RETENTION_TIME_IN_DAYS = 90 on all databases
    correct Answer: option B
    explanation: >
      Only cross-region database replication with failover capability can survive a full regional outage. 
      Multi-cluster warehouses provide concurrency in the same region. 
      Fail-Safe is a local disaster recovery feature, not cross-region. 
      Time Travel cannot help if the entire region is unavailable. 
    difficulty level: Medium
    topic: Architecture Overview
    exam: Architect

  - Question No: "013"
    question: A data pipeline reads from Snowflake every 10 seconds to check for new data using a scheduled script. What is the performance impact?
    their options:
      option A: None â€” Snowflake queries are always free for metadata operations
      option B: Each query starts the virtual warehouse if auto-resume is on, potentially consuming significant credits for many 60-second minimum billing periods
      option C: Snowflake automatically batches rapid queries to minimize cost
      option D: The 10-second polling triggers the Result Cache, making most calls free
    correct Answer: option B
    explanation: >
      Rapid polling causes the warehouse to start (60-second minimum billing each time), execute the query, and potentially suspend â€” each cycle wastes credits. 
      Better patterns: Snowflake Streams + Tasks, Snowpipe event triggers, or SYSTEM$STREAM_HAS_DATA checks to avoid unnecessary warehouse starts. 
    difficulty level: Medium
    topic: Architecture Overview
    exam: Data Engineer

  - Question No: "014"
    question: What is the approximate uncompressed size of a Snowflake micro-partition?
    their options:
      option A: 1 MB
      option B: 16 MB
      option C: 256 MB
      option D: 1 GB
    correct Answer: option B
    explanation: >
      Snowflake micro-partitions are approximately 16 MB of uncompressed data (50â€“500 MB compressed). 
      This relatively small size enables fine-grained data pruning â€” Snowflake can skip many partitions when answering selective queries. 
    difficulty level: Easy
    topic: Micro-Partitions
    exam: Core

  - Question No: "015"
    question: Can you manually create or manage micro-partitions in Snowflake?
    their options:
      option A: Yes, using the CREATE PARTITION command
      option B: Yes, using ALTER TABLE ... ADD PARTITION
      option C: No â€” Snowflake creates and manages all micro-partitions automatically
      option D: Yes, but only with ACCOUNTADMIN privileges
    correct Answer: option C
    explanation: >
      Micro-partitions are fully managed by Snowflake. 
      Users have no direct control over their creation, merging, or splitting. 
      Snowflake automatically partitions data during loading and reorganizes partitions when clustering keys are defined. 
    difficulty level: Easy
    topic: Micro-Partitions
    exam: Core

  - Question No: "016"
    question: What metadata does Snowflake store for each column in every micro-partition?
    their options:
      option A: Row hashes and checksums only
      option B: Minimum value, maximum value, null count, and distinct count
      option C: Only the column names and data types
      option D: Full column statistics including histograms and percentiles
    correct Answer: option B
    explanation: >
      Snowflake stores per-column metadata for each micro-partition: minimum value, maximum value, null count, and row count. 
      This metadata enables partition pruning â€” Snowflake can determine whether a partition could possibly contain rows matching a WHERE clause without scanning the actual data. 
    difficulty level: Easy
    topic: Micro-Partitions
    exam: Core

  - Question No: "017"
    question: Micro-partitions in Snowflake are described as 'immutable.' What does this mean?
    their options:
      option A: They cannot be queried once written
      option B: They are never modified in-place; DML operations create new micro-partitions instead
      option C: They cannot be deleted, only archived
      option D: They are read-only after the table is clustered
    correct Answer: option B
    explanation: >
      Immutable means micro-partitions are never updated or overwritten. 
      When a row is updated, Snowflake marks the old partition as deleted and creates a new partition with the updated data. 
      This immutability is what enables Time Travel and zero-copy cloning. 
    difficulty level: Easy
    topic: Micro-Partitions
    exam: Core

  - Question No: "018"
    question: A table has 500 micro-partitions. A query WHERE order_date = '2024-06-15' runs on the table. Snowflake scans only 12 micro-partitions. What process eliminated the other 488 partitions?
    their options:
      option A: Table statistics sampling
      option B: Micro-partition pruning (partition elimination) using min/max metadata
      option C: Index scan using the order_date column index
      option D: Bloom filter applied to the order_date column
    correct Answer: option B
    explanation: >
      Micro-partition pruning (also called partition elimination) uses the per-partition min/max metadata to skip partitions that cannot contain the queried value. 
      Any partition where max_date < '2024-06-15' OR min_date > '2024-06-15' is entirely skipped. 
      Only 12 partitions had ranges overlapping June 15. 
    difficulty level: Medium
    topic: Micro-Partitions
    exam: Core

  - Question No: "019"
    question: A developer loads data into a table with no clustering key. In what order are micro-partitions typically organized?
    their options:
      option A: By primary key value, ascending
      option B: Alphabetically by the first column
      option C: By natural insertion/load order
      option D: Randomly, with no guaranteed order
    correct Answer: option C
    explanation: >
      Without a clustering key, Snowflake organizes micro-partitions by the natural ingestion order â€” the order rows were loaded. 
      This often correlates with time (if data is loaded chronologically), but there's no guarantee of any particular sort order based on column values. 
    difficulty level: Medium
    topic: Micro-Partitions
    exam: Core

  - Question No: "020"
    question: A clustered table on column REGION has 1,000 micro-partitions. Most partitions contain data for a single region. A query WHERE region = 'APAC' scans only 80 partitions. An unclustered version of the same table would scan all 1,000 partitions. What does the difference demonstrate?
    their options:
      option A: Clustering key creates a traditional B-tree index
      option B: Clustering reduces partition overlap, dramatically improving pruning effectiveness
      option C: Clustering compresses APAC data into fewer micro-partitions
      option D: Clustering enables Snowflake to cache APAC data preferentially
    correct Answer: option B
    explanation: >
      Clustering keys co-locate similar values into the same micro-partitions, reducing value range overlap between partitions. 
      This makes min/max pruning far more effective â€” instead of every partition potentially containing any region, partitions cluster around specific regions, enabling 92% fewer partitions to be scanned. 
    difficulty level: Medium
    topic: Micro-Partitions
    exam: Core

  - Question No: "021"
    question: A table with columns: ID, ORDER_DATE, CUSTOMER_ID, AMOUNT. Data is loaded in ORDER_DATE order. Queries primarily filter on CUSTOMER_ID. The table has poor pruning efficiency on CUSTOMER_ID queries. Why?
    their options:
      option A: The table needs more micro-partitions to improve pruning
      option B: Since data is organized by ORDER_DATE, CUSTOMER_ID values are scattered across many partitions â€” any given CUSTOMER_ID appears in partitions throughout the table, so min/max pruning on CUSTOMER_ID cannot eliminate many partitions
      option C: CUSTOMER_ID requires a unique index for efficient filtering
      option D: Pruning only works on date columns in Snowflake
    correct Answer: option B
    explanation: >
      Because data is sorted by ORDER_DATE at load time, any single customer's orders are spread across many partitions (one per load date). 
      The min/max CUSTOMER_ID range of each partition spans the entire CUSTOMER_ID range, so no partition can be pruned for a CUSTOMER_ID filter. 
      Defining a clustering key on CUSTOMER_ID would reorganize the data to co-locate each customer's rows. 
    difficulty level: Hard
    topic: Micro-Partitions
    exam: Core

  - Question No: "022"
    question: An architect reviews a Query Profile and sees 'Partitions Scanned: 10,000 of 10,000 (100%)' for a selective query. What is the MOST likely root cause?
    their options:
      option A: The query has no WHERE clause
      option B: The table's clustering depth is too high for the query's filter column
      option C: The virtual warehouse is too small to perform pruning
      option D: The Result Cache was bypassed
    correct Answer: option B
    explanation: >
      Scanning 100% of partitions for a selective query indicates that pruning is completely ineffective. 
      This typically means the filter column's values are scattered evenly across all partitions (no correlation between the filter column and the data's physical organization). 
      The solution is to define a clustering key on the frequently filtered column. 
    difficulty level: Medium
    topic: Micro-Partitions
    exam: Architect

  - Question No: "023"
    question: A Data Engineer loads data in random GUID order into a table that is queried heavily by DATE_CREATED. After loading 10 billion rows, query performance is very poor. They define a CLUSTER BY (DATE_CREATED) clustering key. What happens next?
    their options:
      option A: All existing micro-partitions are immediately reorganized â€” the table is instantly optimized
      option B: A background process (Automatic Clustering service) incrementally reorganizes micro-partitions over time, with no user intervention required
      option C: The engineer must run ALTER TABLE ... RECLUSTER to trigger the reorganization
      option D: A full table rebuild is required â€” the table must be recreated from scratch
    correct Answer: option B
    explanation: >
      When a clustering key is defined, Snowflake's Automatic Clustering service runs in the background (consuming credits from a dedicated internal warehouse) to incrementally reorganize micro-partitions so that rows with similar DATE_CREATED values are co-located. 
      This happens automatically without user action, though it may take time to complete for very large tables. 
    difficulty level: Hard
    topic: Micro-Partitions
    exam: Data Engineer

  - Question No: "024"
    question: What happens to a virtual warehouse's compute cost when it is suspended?
    their options:
      option A: Cost drops by 50%
      option B: Cost continues at a reduced standby rate
      option C: Cost drops to $0 for compute
      option D: Cost is transferred to the Cloud Services layer
    correct Answer: option C
    explanation: >
      When a virtual warehouse is suspended, it stops consuming any compute credits â€” cost is exactly $0 for compute. 
      Storage costs continue regardless. 
      This is a major cost-saving feature: suspend warehouses when not in use. 
    difficulty level: Easy
    topic: Virtual Warehouses
    exam: Core

  - Question No: "025"
    question: What is the minimum billing duration each time a virtual warehouse starts?
    their options:
      option A: 1 second
      option B: 10 seconds
      option C: 60 seconds
      option D: 5 minutes
    correct Answer: option C
    explanation: >
      Snowflake bills a minimum of 60 seconds each time a virtual warehouse starts. 
      If a warehouse runs for 30 seconds, you are still billed for 60 seconds. 
      If it runs for 90 seconds, you are billed for 90 seconds. 
      Design auto-suspend windows to be cost-efficient given this minimum. 
    difficulty level: Easy
    topic: Virtual Warehouses
    exam: Core

  - Question No: "026"
    question: When AUTO_RESUME = TRUE is set on a warehouse, what triggers the warehouse to start?
    their options:
      option A: A RESUME WAREHOUSE command must be run manually
      option B: The warehouse starts when a SQL query is submitted and the warehouse is suspended
      option C: The warehouse starts automatically at 8 AM every day
      option D: The warehouse starts when CPU usage on Cloud Services exceeds 50%
    correct Answer: option B
    explanation: >
      AUTO_RESUME = TRUE means the warehouse will automatically start when any query is submitted to it while it is suspended. 
      The user doesn't need to manually run ALTER WAREHOUSE ... 
      RESUME. 
      This enables seamless query execution without manual warehouse management. 
    difficulty level: Easy
    topic: Virtual Warehouses
    exam: Core

  - Question No: "027"
    question: Doubling the size of a virtual warehouse (e.g., from Medium to Large) approximately how does this affect the hourly cost?
    their options:
      option A: Increases cost by 25%
      option B: Doubles the cost
      option C: Triples the cost
      option D: Increases cost by 50%
    correct Answer: option B
    explanation: >
      Each size up exactly doubles both the compute resources AND the cost. 
      Medium to Large doubles cost. 
      This is linear and predictable: XS=1x, S=2x, M=4x, L=8x, XL=16x, 2XL=32x, etc. 
    difficulty level: Easy
    topic: Virtual Warehouses
    exam: Core

  - Question No: "028"
    question: What type of SQL operations require a running virtual warehouse?
    their options:
      option A: Only DDL operations (CREATE TABLE, DROP TABLE)
      option B: Only SELECT queries
      option C: DML operations (SELECT, INSERT, UPDATE, DELETE, MERGE, COPY INTO)
      option D: Only COPY INTO data loading operations
    correct Answer: option C
    explanation: >
      All DML operations (data queries and manipulation) require a running virtual warehouse. 
      DDL operations (CREATE, DROP, ALTER on schema objects), metadata queries (SHOW, DESCRIBE), and GRANT/REVOKE are handled by Cloud Services and do NOT require a running warehouse. 
    difficulty level: Easy
    topic: Virtual Warehouses
    exam: Core

  - Question No: "029"
    question: A warehouse has AUTO_SUSPEND = 300 and AUTO_RESUME = TRUE. It is currently suspended. A query is submitted. What is the sequence of events?
    their options:
      option A: Query is queued, warehouse starts, query runs, warehouse suspends after 300s idle
      option B: Query fails with 'Warehouse suspended' error
      option C: Query runs on Cloud Services since the warehouse is down
      option D: Query is discarded; user must manually resume the warehouse first
    correct Answer: option A
    explanation: >
      With AUTO_RESUME = TRUE: the query is first queued, then the warehouse starts automatically (takes a few seconds), the query executes, and then the warehouse suspends itself after 300 seconds of inactivity. 
      The user experiences a slightly longer first query (due to startup time) but no manual intervention is needed. 
    difficulty level: Medium
    topic: Virtual Warehouses
    exam: Core

  - Question No: "030"
    question: A virtual warehouse is set to X-Large. A query requires more memory than a single X-Large node can provide. What happens?
    their options:
      option A: The query fails with an out-of-memory error
      option B: Snowflake automatically spills excess data to local SSD (and then to remote storage) to complete the query, though performance degrades
      option C: Snowflake automatically upgrades the warehouse to 2X-Large for the duration of the query
      option D: The query is split across multiple warehouse clusters
    correct Answer: option B
    explanation: >
      Snowflake handles memory overflow by spilling to local SSD disk first, and if that is insufficient, to remote cloud storage. 
      The query completes but performance degrades significantly due to I/O. 
      If spill-to-disk is frequent, the warehouse size should be increased. 
      Spilling appears in Query Profile as 'Bytes spilled to local storage' and 'Bytes spilled to remote storage'. 
    difficulty level: Medium
    topic: Virtual Warehouses
    exam: Core

  - Question No: "031"
    question: Two users submit identical queries to the same warehouse simultaneously. The first query returns in 5 seconds. The second query returns in under 1 second. Why?
    their options:
      option A: The second user has a faster network connection
      option B: The first query populated the Result Cache; the second returned from cache without running
      option C: The warehouse scaled up automatically for the second query
      option D: The second query was routed to a different warehouse
    correct Answer: option B
    explanation: >
      The first query ran on the warehouse and its result was stored in the Query Result Cache (Cloud Services layer). 
      When the second user submitted an identical query moments later (same SQL, same permissions, data unchanged), Snowflake returned the cached result instantly without running the warehouse again. 
    difficulty level: Medium
    topic: Virtual Warehouses
    exam: Core

  - Question No: "032"
    question: What is the effect of resizing a virtual warehouse from Large to Small while queries are running?
    their options:
      option A: All running queries are immediately cancelled
      option B: Running queries complete on the Large size; new queries use the Small size after the resize completes
      option C: The resize fails â€” warehouses cannot be resized while queries are running
      option D: Running queries are paused, the resize completes, then they resume on the new size
    correct Answer: option B
    explanation: >
      Snowflake handles warehouse resizes gracefully. 
      Currently executing queries complete on the existing (Large) configuration. 
      Once the resize completes, new queries use the new (Small) size. 
      There is no cancellation of running work. 
    difficulty level: Medium
    topic: Virtual Warehouses
    exam: Core

  - Question No: "033"
    question: A warehouse costs $2/credit and runs for exactly 90 seconds processing a query. The warehouse then auto-suspends after 300 seconds of idle time. How many total seconds is the warehouse billed for?
    their options:
      option A: 90 seconds
      option B: 300 seconds
      option C: 390 seconds
      option D: 450 seconds
    correct Answer: option C
    explanation: >
      The warehouse runs for 90 seconds (billed 90 seconds, above the 60-second minimum). 
      After the query completes, the warehouse remains running (idle) for 300 seconds waiting for new queries (AUTO_SUSPEND = 300). 
      Then it suspends. 
      Total billed time = 90 seconds (query) + 300 seconds (idle) = 390 seconds. 
    difficulty level: Hard
    topic: Virtual Warehouses
    exam: Core

  - Question No: "034"
    question: STATEMENT_QUEUED_TIMEOUT_IN_SECONDS = 60 is set on a warehouse. 20 queries arrive simultaneously but only 8 can run concurrently (MAX_CONCURRENCY_LEVEL = 8). What happens to the remaining 12 queries?
    their options:
      option A: They are immediately rejected with a concurrency error
      option B: They queue and fail with a timeout error if they haven't started running within 60 seconds
      option C: They are automatically routed to a secondary warehouse
      option D: They run on the Cloud Services layer while the primary slots are full
    correct Answer: option B
    explanation: >
      STATEMENT_QUEUED_TIMEOUT_IN_SECONDS defines how long a query can wait in the warehouse queue before failing. 
      With 12 queries queued and only 8 running, each queued query waits. 
      Any query that hasn't begun execution within 60 seconds fails with a timeout error. 
      Users see an error message indicating the queue wait time exceeded the limit. 
    difficulty level: Hard
    topic: Virtual Warehouses
    exam: Core

  - Question No: "035"
    question: An e-commerce company has two workloads: overnight batch ETL (complex, 3-hour job) and daytime analytics (simple, fast queries). What warehouse strategy best serves these workloads?
    their options:
      option A: One warehouse sized for the batch job, running 24/7
      option B: Two separate warehouses: one large for batch (runs overnight), one small for analytics (runs during business hours), each with AUTO_SUSPEND
      option C: One multi-cluster warehouse handling both workloads
      option D: One warehouse that is resized daily between batch and analytics runs
    correct Answer: option B
    explanation: >
      Separate warehouses provide workload isolation, right-sizing, and optimal cost. 
      The batch warehouse (large, overnight) and analytics warehouse (smaller, business hours) each auto-suspend when not needed. 
      This prevents the batch job from consuming resources needed by analysts, and vice versa. 
    difficulty level: Easy
    topic: Virtual Warehouses
    exam: Architect

  - Question No: "036"
    question: A company reports that their BI tool queries frequently wait 30â€“60 seconds before starting. Queries themselves are fast (< 5 seconds). The warehouse is always running. What is the MOST likely cause and solution?
    their options:
      option A: Queries are slow to parse â€” increase warehouse size to speed up the optimizer
      option B: The warehouse has insufficient concurrency; queries are queuing. Solution: Enable multi-cluster warehouse (Enterprise edition)
      option C: Auto-Resume is causing startup delays â€” set a static RUNNING schedule
      option D: The Result Cache is disabled â€” re-enable it in account settings
    correct Answer: option B
    explanation: >
      30-60 second wait times before queries start (not during execution) indicate concurrency queuing. 
      The warehouse is busy with other concurrent queries, so new ones wait in queue. 
      Multi-cluster warehouses (Enterprise+) add additional clusters when queuing is detected, eliminating wait times. 
      Warehouse size affects query speed, not queue wait times. 
    difficulty level: Medium
    topic: Virtual Warehouses
    exam: Architect

  - Question No: "037"
    question: An architect wants to ensure that the finance team's month-end reports always complete even if they consume all warehouse credits in a resource monitor. Other teams should be unaffected. How should this be designed?
    their options:
      option A: Set the finance warehouse resource monitor threshold to 'Notify Only' for month-end
      option B: Create a dedicated finance warehouse with its own resource monitor, separate from other teams' warehouses. Set the month-end schedule to temporarily increase the finance quota or suspend the monitor.
      option C: Remove resource monitors during month-end reporting
      option D: Assign all month-end reports to the ACCOUNTADMIN role which bypasses resource monitors
    correct Answer: option B
    explanation: >
      The correct design separates warehouses by team (each team has its own warehouse and resource monitor), ensuring one team's usage doesn't affect others. 
      For month-end, the finance team's resource monitor quota can be temporarily increased or suspended by an administrator. 
      ACCOUNTADMIN does not bypass resource monitors â€” they apply to warehouses regardless of the executing role. 
    difficulty level: Hard
    topic: Virtual Warehouses
    exam: Architect

  - Question No: "038"
    question: A data pipeline uses COPY INTO to load 1,000 files sequentially. Each file loads in 2 seconds. What is the most efficient way to speed up the loading?
    their options:
      option A: Increase the virtual warehouse size â€” larger warehouses load files faster
      option B: Use parallel COPY INTO statements with multiple warehouses
      option C: Snowflake automatically parallelizes COPY INTO across multiple threads within a single warehouse â€” larger warehouses process more files in parallel
      option D: Compress the files to reduce load time
    correct Answer: option C
    explanation: >
      COPY INTO automatically parallelizes file loading within a virtual warehouse. 
      A larger warehouse has more compute threads, allowing it to process multiple files simultaneously. 
      Upgrading from Small to Large could process 8x more files in parallel. 
      The actual improvement depends on file sizes and the number of files relative to warehouse threads. 
    difficulty level: Medium
    topic: Virtual Warehouses
    exam: Data Engineer

  - Question No: "039"
    question: A Snowpark ML model training job uses a warehouse. Training requires reading 500 GB of data, performing matrix operations, and writing results. The current X-Large warehouse spills 100 GB to local storage. What is the BEST resolution?
    their options:
      option A: Add more virtual warehouses â€” distribute the training across multiple warehouses
      option B: Increase warehouse size to 2X-Large or larger to fit more data in memory and reduce spill
      option C: Use Snowpark Container Services instead for ML training
      option D: Reduce the dataset size â€” ML models don't need all 500 GB
    correct Answer: option B
    explanation: >
      Spilling to local storage indicates the warehouse's RAM is insufficient for the in-memory operations. 
      Increasing the warehouse size provides more memory per node, reducing or eliminating spill. 
      For ML operations (especially matrix operations), memory is typically the bottleneck. 
      2X-Large provides approximately 2x the memory of X-Large per cluster. 
    difficulty level: Hard
    topic: Virtual Warehouses
    exam: Data Engineer

  - Question No: "040"
    question: The Query Result Cache is stored in which Snowflake layer?
    their options:
      option A: Local SSD of the virtual warehouse
      option B: Cloud Services layer
      option C: The storage layer alongside table data
      option D: External object storage (S3/Azure Blob/GCS)
    correct Answer: option B
    explanation: >
      The Query Result Cache is maintained in the Cloud Services layer, making it accessible to all users and warehouses in the account. 
      This is why the same query from any user or warehouse can benefit from a cached result â€” it's not tied to any specific virtual warehouse. 
    difficulty level: Easy
    topic: Caching
    exam: Core

  - Question No: "041"
    question: How long does a Query Result Cache entry remain valid if the underlying table data does not change?
    their options:
      option A: 1 hour
      option B: 24 hours
      option C: 7 days
      option D: Until the warehouse is suspended
    correct Answer: option B
    explanation: >
      Result Cache entries are valid for 24 hours from the last time the result was reused. 
      If the same query is run every hour, the cache entry keeps getting refreshed. 
      If the table data changes, the cache entry is immediately invalidated, regardless of the 24-hour window. 
    difficulty level: Easy
    topic: Caching
    exam: Core

  - Question No: "042"
    question: Does serving a query from the Result Cache consume virtual warehouse compute credits?
    their options:
      option A: Yes â€” the warehouse must be running to serve cached results
      option B: No â€” Result Cache is served directly from Cloud Services without any warehouse
      option C: Yes, but at a reduced rate (10% of normal credit consumption)
      option D: Only if the cached result is larger than 1 GB
    correct Answer: option B
    explanation: >
      Result Cache hits are completely free â€” no warehouse credits are consumed. 
      The result is returned directly from the Cloud Services layer. 
      This is why report authors often encourage users to run the same queries (especially parameterless dashboards) rather than variations that bypass the cache. 
    difficulty level: Easy
    topic: Caching
    exam: Core

  - Question No: "043"
    question: When is the local disk (data) cache on a virtual warehouse cleared?
    their options:
      option A: Every 24 hours, synchronized with the Result Cache
      option B: When the warehouse is suspended or resized
      option C: When a new user connects to the warehouse
      option D: When storage data is updated
    correct Answer: option B
    explanation: >
      The local disk cache (which stores micro-partition data files read from cloud storage) is lost when the warehouse is suspended or resized. 
      This is a key consideration for frequently-used warehouses â€” aggressive auto-suspend settings clear the cache and force re-reading from remote storage on the next query. 
    difficulty level: Easy
    topic: Caching
    exam: Core

  - Question No: "044"
    question: A user runs a query. The warehouse is suspended immediately after. The next day, a different user runs an identical query. Will the Result Cache be used?
    their options:
      option A: No â€” Result Cache is cleared when the warehouse suspends
      option B: Yes â€” if the underlying data hasn't changed and it's within 24 hours, Result Cache is used. The warehouse being suspended is irrelevant.
      option C: No â€” Result Cache is user-specific; different users cannot share cached results
      option D: Yes, but only if both users have the same role
    correct Answer: option B
    explanation: >
      Result Cache is stored in Cloud Services (not on the warehouse) and is account-wide. 
      Warehouse suspension does not affect it. 
      The cache is shared across all users and warehouses. 
      The two requirements are: (1) same SQL text and context, and (2) data unchanged. 
      A different user with same permissions can use the same cache entry. 
    difficulty level: Medium
    topic: Caching
    exam: Core

  - Question No: "045"
    question: A BI dashboard query uses CURRENT_TIMESTAMP() in its WHERE clause to filter today's data. Users complain that the dashboard sometimes shows stale data. Why?
    their options:
      option A: CURRENT_TIMESTAMP() is not supported in WHERE clauses
      option B: Queries using non-deterministic functions like CURRENT_TIMESTAMP() cannot use the Result Cache â€” each execution re-runs the query
      option C: The Result Cache is too small to store the dashboard query results
      option D: CURRENT_TIMESTAMP() always returns the time of the last cache population
    correct Answer: option B
    explanation: >
      Non-deterministic functions (CURRENT_TIMESTAMP, CURRENT_DATE, RANDOM, UUID_STRING, SEQ) in a query prevent Result Cache from being used. 
      Each execution is treated as a unique query. 
      For BI dashboards, this means every refresh actually runs the query. 
      If users are seeing stale data, it might be from the warehouse's local disk cache or an intermediate caching layer in the BI tool. 
    difficulty level: Medium
    topic: Caching
    exam: Core

  - Question No: "046"
    question: The local disk cache on a warehouse is described as a 'warm cache.' What does a 'warm cache' mean for query performance?
    their options:
      option A: The warehouse is running at high temperature
      option B: Micro-partition data has already been loaded from cloud storage into the warehouse's local SSD, so subsequent queries on the same data read from fast local SSD instead of slower remote storage
      option C: The Result Cache has been pre-populated with expected query results
      option D: The warehouse has been pre-scaled to handle anticipated load
    correct Answer: option B
    explanation: >
      A 'warm cache' means frequently-accessed micro-partitions are already resident on the warehouse's local SSD. 
      Reads from SSD are much faster than reads from remote cloud storage (S3/Blob/GCS). 
      Maintaining a warm cache is why avoiding unnecessary warehouse suspensions for production analytical workloads can improve performance. 
    difficulty level: Medium
    topic: Caching
    exam: Core

  - Question No: "047"
    question: A query Q1 runs on Warehouse A and its result is cached. Warehouse A is then dropped. Later, the identical query Q1 is submitted to Warehouse B. What happens?
    their options:
      option A: Q1 fails because its cached result was associated with the dropped warehouse
      option B: Q1 uses the Result Cache from Cloud Services â€” the result is available regardless of which warehouse originally ran it or whether that warehouse still exists
      option C: Q1 must re-execute on Warehouse B since there's no cache entry
      option D: Q1 partially uses the cache but must re-compute the aggregation step
    correct Answer: option B
    explanation: >
      Result Cache is stored in Cloud Services, completely independent of any specific virtual warehouse. 
      Dropping Warehouse A has no effect on the Result Cache. 
      The cached result from Q1 is available to any user or warehouse in the account for 24 hours (as long as underlying data hasn't changed). 
    difficulty level: Hard
    topic: Caching
    exam: Core

  - Question No: "048"
    question: An architect wants to maximize Result Cache effectiveness for a company's morning financial reports that run at 7 AM daily. What design principle achieves this?
    their options:
      option A: Use CURRENT_DATE() in all WHERE clauses to filter today's data
      option B: Standardize report SQL in the BI tool so all users run identical query text, and avoid non-deterministic functions in frequently-run reports
      option C: Pre-warm the warehouse by running reports manually before 7 AM
      option D: Enable a dedicated Result Cache warehouse with larger memory
    correct Answer: option B
    explanation: >
      Result Cache works when queries have identical SQL text. 
      Standardizing queries in the BI tool (same parameterization, same formatting) means the first user who runs a report at 7 AM populates the cache, and all subsequent users get instant results. 
      Non-deterministic functions must be avoided as they bypass the cache. 
    difficulty level: Medium
    topic: Caching
    exam: Architect

  - Question No: "049"
    question: A data pipeline writes new data to table T every 5 minutes. Multiple analysts run reports against T. What happens to the Result Cache for T when new data arrives?
    their options:
      option A: Cache entries for T are invalidated immediately when any DML commits to T
      option B: Cache entries expire on their 24-hour schedule regardless of data changes
      option C: Cache entries are invalidated at the next 5-minute interval boundary
      option D: Cache entries are preserved for 15 minutes after a data write to reduce thrashing
    correct Answer: option A
    explanation: >
      Any DML that modifies table T (INSERT, UPDATE, DELETE, MERGE, COPY INTO) immediately invalidates all Result Cache entries that reference T. 
      With 5-minute writes, analysts lose the benefit of Result Cache. 
      This is expected behavior â€” reporting on near-real-time data inherently cannot use Result Cache effectively. 
    difficulty level: Hard
    topic: Caching
    exam: Data Engineer

  - Question No: "050"
    question: What is the maximum Time Travel retention period for a Standard edition account?
    their options:
      option A: 0 days
      option B: 1 day
      option C: 30 days
      option D: 90 days
    correct Answer: option B
    explanation: >
      Standard edition supports a maximum of 1 day (24 hours) of Time Travel. 
      Enterprise edition and above support up to 90 days. 
      Setting DATA_RETENTION_TIME_IN_DAYS > 1 on Standard edition will return an error. 
    difficulty level: Easy
    topic: Editions
    exam: Core

  - Question No: "051"
    question: Which feature is ONLY available starting from Enterprise edition?
    their options:
      option A: Basic SQL queries
      option B: Multi-cluster virtual warehouses
      option C: Standard Time Travel (1 day)
      option D: External stages
    correct Answer: option B
    explanation: >
      Multi-cluster warehouses require Enterprise edition or above. 
      Standard edition is limited to single-cluster warehouses. 
      All other options listed (SQL, 1-day Time Travel, external stages) are available in Standard edition. 
    difficulty level: Easy
    topic: Editions
    exam: Core

  - Question No: "052"
    question: A company handling PHI (Protected Health Information) needs to comply with HIPAA. Which is the MINIMUM Snowflake edition required?
    their options:
      option A: Standard
      option B: Enterprise
      option C: Business Critical
      option D: Virtual Private
    correct Answer: option C
    explanation: >
      HIPAA compliance requires Business Critical edition. 
      This edition provides enhanced encryption, private connectivity (PrivateLink), and the contractual ability for Snowflake to sign a Business Associate Agreement (BAA) â€” required for HIPAA compliance. 
      Enterprise edition does not provide HIPAA compliance support. 
    difficulty level: Easy
    topic: Editions
    exam: Core

  - Question No: "053"
    question: A customer wants Dynamic Data Masking to protect PII columns in production. They currently use Standard edition. What must they do?
    their options:
      option A: Enable Dynamic Data Masking in account settings (it's available in all editions)
      option B: Upgrade to at least Enterprise edition
      option C: Upgrade to Business Critical edition
      option D: Use a workaround with Column-level Security views instead
    correct Answer: option B
    explanation: >
      Dynamic Data Masking is an Enterprise-edition (and above) feature. 
      Standard edition does not support it. 
      The customer must upgrade to Enterprise edition. 
      Column-level security (a separate feature), Row Access Policies, and Object Tagging are all Enterprise+ features. 
    difficulty level: Medium
    topic: Editions
    exam: Core

  - Question No: "054"
    question: An organization on Enterprise edition wants to store Snowflake encryption keys in their own AWS KMS. Is this possible?
    their options:
      option A: Yes â€” Enterprise edition supports customer-managed keys via KMS
      option B: No â€” Enterprise edition uses Snowflake-managed encryption only. Tri-Secret Secure (customer-managed keys) requires Business Critical edition.
      option C: Yes, but only for tables, not for stages or pipes
      option D: No â€” Snowflake never supports external key management
    correct Answer: option B
    explanation: >
      Tri-Secret Secure (the ability to use a customer-managed key from AWS KMS, Azure Key Vault, or GCP KMS alongside Snowflake's key) is a Business Critical feature, not Enterprise. 
      Enterprise uses Snowflake-managed hierarchical key encryption. 
    difficulty level: Medium
    topic: Editions
    exam: Core

  - Question No: "055"
    question: A company uses Standard edition with DATA_RETENTION_TIME_IN_DAYS = 1 on all tables. A critical table is accidentally dropped at 10:00 AM. The error is discovered at 11:30 AM. Can the table be recovered?
    their options:
      option A: No â€” 1 hour 30 minutes exceeds the 1-day retention for Standard edition
      option B: Yes â€” UNDROP TABLE will restore the table since only 1.5 hours have passed (within the 1-day Time Travel window)
      option C: Yes â€” but only the last committed transaction before the drop can be recovered
      option D: No â€” dropped tables can only be recovered through Fail-Safe (7-day window), which requires Snowflake Support
    correct Answer: option B
    explanation: >
      With DATA_RETENTION_TIME_IN_DAYS = 1, dropped objects are recoverable for up to 24 hours using UNDROP TABLE. 
      Since only 1.5 hours have passed, UNDROP TABLE my_table will restore the table with all its data. 
      This works even on Standard edition within the 1-day window. 
    difficulty level: Hard
    topic: Editions
    exam: Core

  - Question No: "056"
    question: A company is designing a Snowflake architecture for cross-region failover. Which minimum edition is required to enable database failover and failback?
    their options:
      option A: Standard
      option B: Enterprise
      option C: Business Critical
      option D: Virtual Private
    correct Answer: option C
    explanation: >
      Database Failover/Failback for high availability and business continuity is a Business Critical (and Virtual Private) feature. 
      Enterprise edition supports database replication for read-only workloads but not the full failover/failback capability required for active DR scenarios. 
    difficulty level: Easy
    topic: Editions
    exam: Architect

  - Question No: "057"
    question: A financial institution requires: 90-day Time Travel, column-level security, private network connectivity (no public internet), and PCI-DSS compliance. Which edition meets ALL requirements?
    their options:
      option A: Enterprise
      option B: Business Critical
      option C: Virtual Private
      option D: Standard with security add-ons
    correct Answer: option B
    explanation: >
      Mapping requirements: 90-day Time Travel (Enterprise+), column-level security (Enterprise+), private network connectivity via PrivateLink (Business Critical+), PCI-DSS compliance (Business Critical+). 
      Business Critical meets all requirements. 
      Virtual Private would also work but is more expensive than needed. 
    difficulty level: Medium
    topic: Editions
    exam: Architect

  - Question No: "058"
    question: An organization is evaluating cost/benefit of upgrading from Enterprise to Business Critical. Their primary drivers are: existing Snowflake encryption is acceptable, they have no compliance requirements, but they want dedicated hardware. What is the CORRECT edition recommendation?
    their options:
      option A: Business Critical â€” it provides dedicated hardware
      option B: Virtual Private Snowflake â€” dedicated hardware is a VPS-only feature, not Business Critical
      option C: Enterprise is sufficient â€” dedicated hardware is not a Snowflake feature
      option D: Business Critical with Tri-Secret Secure for dedicated hardware
    correct Answer: option B
    explanation: >
      Dedicated virtual network infrastructure (isolated, dedicated hardware) is a Virtual Private Snowflake (VPS) feature, not Business Critical. 
      Business Critical provides enhanced compliance, encryption, and private connectivity on shared Snowflake infrastructure. 
      If the sole requirement is dedicated hardware, VPS is the correct (though most expensive) choice. 
    difficulty level: Hard
    topic: Editions
    exam: Architect

  - Question No: "059"
    question: Which of the following is an account-level object in Snowflake (NOT contained within a database)?
    their options:
      option A: Table
      option B: View
      option C: Virtual Warehouse
      option D: Stage
    correct Answer: option C
    explanation: >
      Virtual Warehouses exist at the account level. 
      They are not contained in any database or schema. 
      Other account-level objects include: Roles, Users, Resource Monitors, Network Policies, Shares, and Integrations. 
      Tables, Views, and Stages are schema-level objects. 
    difficulty level: Easy
    topic: Object Hierarchy
    exam: Core

  - Question No: "060"
    question: What is the default schema created automatically when a new database is created in Snowflake?
    their options:
      option A: MAIN
      option B: DEFAULT
      option C: PUBLIC
      option D: SYSTEM
    correct Answer: option C
    explanation: >
      When a new database is created in Snowflake, the PUBLIC schema is automatically created as a default schema. 
      The INFORMATION_SCHEMA schema is also created automatically and contains metadata views. 
      Users can create additional schemas as needed. 
    difficulty level: Easy
    topic: Object Hierarchy
    exam: Core

  - Question No: "061"
    question: What is the highest level of the Snowflake object hierarchy?
    their options:
      option A: Account
      option B: Organization
      option C: Database
      option D: Cloud Services
    correct Answer: option B
    explanation: >
      Organization is the highest level in Snowflake's object hierarchy. 
      An Organization groups multiple Snowflake accounts and enables cross-account features like replication, data sharing governance, and consolidated billing. 
      Not all customers have an Organization â€” it's used primarily in multi-account setups. 
    difficulty level: Easy
    topic: Object Hierarchy
    exam: Core

  - Question No: "062"
    question: A developer runs DROP DATABASE prod_db. Which objects are ALSO deleted as a result?
    their options:
      option A: Only the database record â€” all tables are retained in their original state
      option B: All schemas, tables, views, stages, functions, and stored procedures within prod_db
      option C: All schemas and tables, but stages are not deleted (they reference external storage)
      option D: The database and schemas, but all tables are moved to PUBLIC schema in the default database
    correct Answer: option B
    explanation: >
      Dropping a database in Snowflake is a cascading operation. 
      ALL child objects are dropped: schemas, tables, views, stages, file formats, sequences, functions, stored procedures, streams, tasks, and pipes within the database. 
      Account-level objects (warehouses, roles, users) are NOT affected. 
      Within Time Travel retention, the database can be restored with UNDROP DATABASE. 
    difficulty level: Medium
    topic: Object Hierarchy
    exam: Core

  - Question No: "063"
    question: A Snowflake account has a parameter TIMEZONE = 'UTC' set at the account level. A specific database DB1 has TIMEZONE = 'America/New_York' set. A session using DB1 has no timezone set. What timezone applies to queries in this session?
    their options:
      option A: UTC (account-level always wins)
      option B: America/New_York (database-level overrides account-level)
      option C: The system default timezone of the cloud provider
      option D: UTC/America/New_York hybrid depending on the query type
    correct Answer: option B
    explanation: >
      Snowflake parameter inheritance follows a hierarchy where more specific settings override broader ones. 
      Database-level overrides Account-level. 
      Session-level (highest priority) overrides Database-level. 
      Since the session has no explicit timezone, the database-level setting (America/New_York) applies. 
    difficulty level: Medium
    topic: Object Hierarchy
    exam: Core

  - Question No: "064"
    question: A Snowflake database named ANALYTICS has a schema named SALES_DATA. Inside this schema, there is a table named ORDERS. What is the fully qualified name of this table?
    their options:
      option A: ORDERS.SALES_DATA.ANALYTICS
      option B: ANALYTICS/SALES_DATA/ORDERS
      option C: ANALYTICS.SALES_DATA.ORDERS
      option D: SALES_DATA.ORDERS in database ANALYTICS
    correct Answer: option C
    explanation: >
      Snowflake uses dot notation for fully qualified object names: DATABASE.SCHEMA.OBJECT. 
      So the fully qualified name is ANALYTICS.SALES_DATA.ORDERS. 
      This notation is used in SQL queries when referencing objects across databases or schemas, and in GRANT statements. 
    difficulty level: Hard
    topic: Object Hierarchy
    exam: Core

  - Question No: "065"
    question: An architect wants to enforce consistent data retention policies across all tables in a production database. What is the MOST efficient approach?
    their options:
      option A: Set DATA_RETENTION_TIME_IN_DAYS on every table individually
      option B: Set DATA_RETENTION_TIME_IN_DAYS at the database level â€” all tables inherit this setting unless overridden at the schema or table level
      option C: Create a stored procedure that sets retention on all tables weekly
      option D: Use Resource Monitors to control Time Travel storage costs
    correct Answer: option B
    explanation: >
      Setting parameters at the database level propagates to all schemas and tables within it via inheritance. 
      Individual schemas or tables can override this with their own setting. 
      This is far more efficient than setting retention on every table. 
      For example: ALTER DATABASE prod SET DATA_RETENTION_TIME_IN_DAYS = 30; 
    difficulty level: Medium
    topic: Object Hierarchy
    exam: Architect

  - Question No: "066"
    question: What SQL clause is used to query historical data in Snowflake using a specific timestamp?
    their options:
      option A: HISTORICAL AS OF TIMESTAMP
      option B: AS OF DATETIME
      option C: AT (TIMESTAMP => ...)
      option D: BEFORE TIME
    correct Answer: option C
    explanation: >
      The AT clause with TIMESTAMP => syntax is used for Time Travel: SELECT * FROM table AT (TIMESTAMP => '2024-01-15 10:00:00'::timestamp). 
      BEFORE is also valid for seeing data just before a specific event. 
      There is no HISTORICAL AS OF syntax in Snowflake. 
    difficulty level: Easy
    topic: Time Travel
    exam: Core

  - Question No: "067"
    question: A table has been accidentally dropped. Which command restores it using Time Travel?
    their options:
      option A: RESTORE TABLE my_table
      option B: ROLLBACK DROP TABLE my_table
      option C: UNDROP TABLE my_table
      option D: CREATE TABLE my_table FROM BACKUP
    correct Answer: option C
    explanation: >
      UNDROP TABLE table_name is the command to restore a dropped table within the Time Travel retention period. 
      Similarly, UNDROP SCHEMA and UNDROP DATABASE can restore accidentally dropped parent objects. 
      This works as long as the object was dropped within the configured DATA_RETENTION_TIME_IN_DAYS window. 
    difficulty level: Easy
    topic: Time Travel
    exam: Core

  - Question No: "068"
    question: What is Fail-Safe, and who can access data in Fail-Safe?
    their options:
      option A: A user-accessible backup that extends Time Travel by 7 days
      option B: A Snowflake-managed disaster recovery layer, accessible only by Snowflake Support, that exists for 7 days after Time Travel expires
      option C: A feature to create standby databases in different regions
      option D: An automated backup to customer-owned cloud storage
    correct Answer: option B
    explanation: >
      Fail-Safe is a 7-day (fixed, non-configurable) disaster recovery layer that begins after Time Travel expires. 
      Unlike Time Travel (user-accessible via SQL), Fail-Safe data can only be recovered by Snowflake Support upon request. 
      It protects against catastrophic scenarios, not routine user errors. 
    difficulty level: Easy
    topic: Time Travel
    exam: Core

  - Question No: "069"
    question: A table has DATA_RETENTION_TIME_IN_DAYS = 30. A row was deleted 35 days ago. Can the row be recovered?
    their options:
      option A: Yes â€” using Time Travel with AT (OFFSET => -35 * 86400)
      option B: Yes â€” using Fail-Safe access within the 7-day Fail-Safe window (days 30â€“37)
      option C: No â€” Time Travel (30 days) has expired, and Fail-Safe (days 31â€“37) has also expired at day 35... wait, 35 < 37, so Snowflake Support could potentially recover via Fail-Safe
      option D: No â€” once Time Travel expires, data is permanently deleted
    correct Answer: option C
    explanation: >
      With 30-day Time Travel, the row's deletion was 35 days ago. 
      Time Travel expired at day 30. 
      Fail-Safe begins at day 30 and runs for 7 more days (days 30â€“37). 
      Since we're at day 35, we are within the Fail-Safe window. 
      Snowflake Support could potentially recover this data. 
      However, Fail-Safe recovery is not guaranteed â€” it is a last-resort measure. 
    difficulty level: Medium
    topic: Time Travel
    exam: Core

  - Question No: "070"
    question: Setting DATA_RETENTION_TIME_IN_DAYS = 0 on a table has what effect?
    their options:
      option A: Time Travel is disabled for that table â€” historical data is deleted immediately when rows change
      option B: Time Travel uses the default (1 day) when set to 0
      option C: Only Fail-Safe protection applies when Time Travel is 0
      option D: Setting to 0 is not allowed â€” minimum is 1 day
    correct Answer: option A
    explanation: >
      DATA_RETENTION_TIME_IN_DAYS = 0 explicitly disables Time Travel for that table. 
      Historical data is deleted as soon as rows are modified or deleted â€” no AT/BEFORE queries are possible. 
      Fail-Safe also does not apply to tables with retention = 0. 
      This reduces storage costs for transient/staging tables. 
    difficulty level: Medium
    topic: Time Travel
    exam: Core

  - Question No: "071"
    question: A table's DATA_RETENTION_TIME_IN_DAYS is changed from 90 to 1. What happens to the historical data that was retained for the previous 90-day period?
    their options:
      option A: Historical data is immediately purged when retention is reduced
      option B: Historical data retention is maintained for the old 90-day period; new changes use the 1-day retention
      option C: Historical data is retained for a grace period of 7 days before deletion
      option D: Historical data is moved to Fail-Safe automatically
    correct Answer: option A
    explanation: >
      When TIME TRAVEL retention is reduced, Snowflake does NOT immediately purge existing historical data â€” but the metadata governing its availability changes. 
      Snowflake's background processes will eventually purge data older than the new retention window (1 day) through normal garbage collection. 
      The exact timing may have a short delay but the data becomes inaccessible via Time Travel immediately upon the setting change. 
    difficulty level: Hard
    topic: Time Travel
    exam: Core

  - Question No: "072"
    question: An architect wants to implement a system where any data corruption can be recovered going back 60 days. The organization uses Standard edition. What is the issue?
    their options:
      option A: Standard edition supports only 1-day Time Travel â€” 60 days requires Enterprise edition
      option B: Standard edition supports Time Travel up to 30 days
      option C: 60-day Time Travel requires Business Critical edition
      option D: Standard edition does not support Time Travel at all
    correct Answer: option A
    explanation: >
      Standard edition is limited to a maximum of 1 day of Time Travel. 
      Enterprise edition supports up to 90 days. 
      To achieve 60-day recovery capability, the organization must upgrade to Enterprise edition and set DATA_RETENTION_TIME_IN_DAYS = 60 on the relevant tables. 
    difficulty level: Medium
    topic: Time Travel
    exam: Architect

  - Question No: "073"
    question: A Data Engineer wants to create a daily audit comparison report showing what changed in table ORDERS between yesterday and today. What is the MOST efficient Snowflake-native approach?
    their options:
      option A: Create two snapshots daily by INSERT INTO archive_table SELECT * FROM orders
      option B: Use Snowflake Streams on the ORDERS table to capture CDC (Change Data Capture) changes automatically
      option C: Run a complex self-join using Time Travel: compare ORDERS AT (OFFSET => -86400) with current ORDERS
      option D: Export data to S3 daily and compare files in an external system
    correct Answer: option B
    explanation: >
      Snowflake Streams are specifically designed for CDC tracking. 
      A Stream on ORDERS captures all INSERT, UPDATE, and DELETE changes since the last time the stream was consumed. 
      This is more efficient than periodic snapshots (no duplicated data) and more elegant than self-joining with Time Travel. 
    difficulty level: Medium
    topic: Time Travel
    exam: Data Engineer

  - Question No: "074"
    question: What is the initial storage cost of creating a zero-copy clone of a 10 TB table?
    their options:
      option A: 10 TB (full copy of all data)
      option B: 5 TB (half the data is copied initially)
      option C: $0 extra (no data is physically copied)
      option D: Only metadata storage cost (negligible)
    correct Answer: option C
    explanation: >
      Zero-copy cloning creates a metadata-only snapshot pointing to the same micro-partitions as the source. 
      No data is physically copied at clone creation time. 
      Additional storage is only consumed when either the clone or source is modified (creating new, non-shared micro-partitions). 
    difficulty level: Easy
    topic: Zero-Copy Cloning
    exam: Core

  - Question No: "075"
    question: Which objects CAN be cloned in Snowflake?
    their options:
      option A: Tables, schemas, databases
      option B: Only individual tables
      option C: Only databases
      option D: Tables and schemas only â€” databases cannot be cloned
    correct Answer: option A
    explanation: >
      In Snowflake, you can clone Tables, Schemas, and entire Databases. 
      Cloning a Schema clones all objects within it. 
      Cloning a Database clones all schemas and objects within it. 
      Streams and Tasks can also be cloned with some limitations. 
    difficulty level: Easy
    topic: Zero-Copy Cloning
    exam: Core

  - Question No: "076"
    question: After cloning a production table, a developer inserts 1 million rows into the clone. What is the storage impact?
    their options:
      option A: 1 million rows of storage allocated, shared between clone and source
      option B: New micro-partitions are created only for the clone, containing the new 1 million rows. The original shared micro-partitions remain shared.
      option C: The entire clone is copied at first write (copy-on-write) â€” the developer now has two full copies
      option D: All 1 million new rows are written to the source table's micro-partitions to maintain linkage
    correct Answer: option B
    explanation: >
      Zero-copy cloning uses a copy-on-write mechanism at the micro-partition level. 
      When the clone is modified, only the new or changed micro-partitions are created for the clone. 
      The original partitions continue to be shared. 
      Storage cost increases only for the new micro-partitions containing the 1 million inserted rows. 
    difficulty level: Medium
    topic: Zero-Copy Cloning
    exam: Core

  - Question No: "077"
    question: A team clones production database PROD to create DEV. Which of the following is NOT cloned?
    their options:
      option A: Tables and their data
      option B: Views and stored procedures
      option C: Grants (privileges) on objects within the database
      option D: Schemas and file formats
    correct Answer: option C
    explanation: >
      Zero-copy cloning does NOT copy grants (privilege assignments). 
      The cloned objects exist in the new database but none of the GRANT statements that applied to the source are replicated. 
      Access must be re-granted separately on the clone. 
      This is a common exam trap and a real operational consideration for dev/test environments. 
    difficulty level: Medium
    topic: Zero-Copy Cloning
    exam: Core

  - Question No: "078"
    question: A database is cloned using: CREATE DATABASE test CLONE prod AT (TIMESTAMP => '2024-01-01 00:00:00'). What does this create?
    their options:
      option A: A clone of the current state of prod, ignoring the timestamp
      option B: A clone reflecting prod's state at midnight on January 1, 2024 â€” a point-in-time historical snapshot
      option C: A scheduled clone that will be created on January 1, 2024
      option D: A clone that automatically resets to the January 1 state every day
    correct Answer: option B
    explanation: >
      You can combine zero-copy cloning with Time Travel to create a historical clone. 
      The AT clause specifies the exact point-in-time for the clone. 
      The resulting database reflects PROD's state at that moment. 
      This is useful for debugging (what did data look like before a bad load?) or creating reproducible test environments. 
    difficulty level: Hard
    topic: Zero-Copy Cloning
    exam: Core

  - Question No: "079"
    question: An architect wants to enable developers to test data pipeline changes safely without affecting production. Data must be realistic (not synthetic). What is the recommended Snowflake pattern?
    their options:
      option A: Replicate the production database to a development Snowflake account weekly
      option B: Use zero-copy cloning to create a development database from production â€” instant, free at creation time, fully isolated once modified
      option C: Export production data to S3 and load it into a development schema
      option D: Use Snowflake Data Sharing to give developers read-only access to production
    correct Answer: option B
    explanation: >
      Zero-copy cloning is the definitive answer for dev/test environments. 
      It's instant (regardless of data size), free at creation time, and completely isolated â€” changes in dev never affect production. 
      Developers get full read-write access to realistic production-scale data. 
      This is a core Snowflake best practice. 
    difficulty level: Medium
    topic: Zero-Copy Cloning
    exam: Architect

  - Question No: "080"
    question: A Data Engineer runs: CREATE TABLE orders_clone CLONE orders AT (OFFSET => -7200). The current time is 3:00 PM. They then insert rows into orders. Later, they query orders_clone. What do they see?
    their options:
      option A: The current state of orders, including the newly inserted rows
      option B: The state of orders as it was at 1:00 PM (2 hours ago), before the new inserts. The clone is isolated from changes to the source.
      option C: An error because orders was modified after the clone was created
      option D: The state of orders at 1:00 PM plus any changes to orders since the clone was created
    correct Answer: option B
    explanation: >
      Zero-copy cloning creates an isolated snapshot. 
      The clone reflects orders' state at 1:00 PM (3:00 PM minus 7,200 seconds = 2 hours). 
      Subsequent inserts to the source table (orders) create new micro-partitions for orders only â€” orders_clone still references the original 1:00 PM micro-partitions. 
      The clone is completely isolated from source changes. 
    difficulty level: Hard
    topic: Zero-Copy Cloning
    exam: Data Engineer

  - Question No: "081"
    question: Multi-cluster warehouses are designed to solve which problem?
    their options:
      option A: Individual queries taking too long (complex, memory-intensive queries)
      option B: Too many concurrent users causing queries to queue
      option C: Insufficient storage capacity
      option D: High cost of running large warehouses
    correct Answer: option B
    explanation: >
      Multi-cluster warehouses solve the concurrency problem â€” too many simultaneous users causing queries to wait in queue. 
      When the primary cluster is busy, additional clusters spin up to handle the extra load. 
      They do NOT help with individual query complexity â€” that requires a larger warehouse size. 
    difficulty level: Easy
    topic: Multi-Cluster Warehouses
    exam: Core

  - Question No: "082"
    question: With a multi-cluster warehouse configured as Min=1, Max=4, Scaling Policy=Standard, what triggers a second cluster to start?
    their options:
      option A: CPU utilization on cluster 1 exceeds 80%
      option B: Any query submitted when cluster 1 is already processing queries
      option C: A query would have to queue (wait) due to cluster 1 being at MAX_CONCURRENCY_LEVEL
      option D: The number of active sessions exceeds 100
    correct Answer: option C
    explanation: >
      Standard scaling policy adds a new cluster as soon as a query would have to wait in queue (i.e., the current cluster is at full concurrency). 
      Economy policy waits longer to ensure sustained queue buildup before adding a cluster. 
      CPU utilization and session count don't directly trigger cluster scaling. 
    difficulty level: Medium
    topic: Multi-Cluster Warehouses
    exam: Core

  - Question No: "083"
    question: A multi-cluster warehouse is set to Min=3, Max=6. There are no active queries. How many clusters are running?
    their options:
      option A: 0 â€” all clusters suspend when idle
      option B: 3 â€” Min clusters always run
      option C: 6 â€” Max clusters are pre-started for readiness
      option D: 1 â€” only one cluster runs until needed
    correct Answer: option B
    explanation: >
      Min clusters is the guaranteed minimum number of always-running clusters. 
      With Min=3, at least 3 clusters are always active even with zero queries. 
      Additional clusters (up to Max=6) spin up based on concurrency demand. 
      Setting Min > 1 increases cost but ensures instant availability without any cold-start latency. 
    difficulty level: Medium
    topic: Multi-Cluster Warehouses
    exam: Architect

  - Question No: "084"
    question: A warehouse has Min=1, Max=8, Scaling=Economy. During peak hour, 200 concurrent queries arrive. Each query takes 30 seconds. MAX_CONCURRENCY_LEVEL = 8. How many clusters does Snowflake need to handle this without any queuing?
    their options:
      option A: 8 clusters
      option B: 25 clusters
      option C: More than 8 â€” Snowflake adds up to 200/8 = 25 clusters, but Max=8 limits it
      option D: 200 clusters (one per query)
    correct Answer: option C
    explanation: >
      With 200 concurrent queries and 8 concurrent queries per cluster: 200 / 8 = 25 clusters needed. 
      However, Max=8. 
      So at maximum scaling, only 64 queries (8 clusters Ã— 8 concurrent) can run simultaneously â€” the remaining 136 will queue. 
      Economy scaling may further delay cluster addition. 
      The solution is to increase Max Clusters or reduce concurrency. 
    difficulty level: Hard
    topic: Multi-Cluster Warehouses
    exam: Architect

  - Question No: "085"
    question: What is the purpose of a Resource Monitor in Snowflake?
    their options:
      option A: Monitor query performance and automatically optimize slow queries
      option B: Set credit usage limits on virtual warehouses or the account to control costs
      option C: Monitor data quality and alert on anomalies
      option D: Track user login activity and suspicious access patterns
    correct Answer: option B
    explanation: >
      Resource Monitors are a cost governance tool. 
      They set credit quotas on virtual warehouses (or the entire account) and can notify administrators and/or suspend warehouses when thresholds are exceeded. 
      They do not affect query performance optimization or security. 
    difficulty level: Easy
    topic: Resource Monitors
    exam: Core

  - Question No: "086"
    question: Which role is required to create a Resource Monitor?
    their options:
      option A: SYSADMIN
      option B: SECURITYADMIN
      option C: ACCOUNTADMIN
      option D: Any role with MONITOR WAREHOUSE privilege
    correct Answer: option C
    explanation: >
      Creating Resource Monitors requires the ACCOUNTADMIN role (or a role with MANAGE GRANTS privilege, which is effectively ACCOUNTADMIN-level). 
      This ensures that only top-level administrators can control account-wide cost governance. 
    difficulty level: Easy
    topic: Resource Monitors
    exam: Core

  - Question No: "087"
    question: A Resource Monitor is set to: Credit Quota = 100, Trigger at 90% = Notify, Trigger at 100% = Notify & Suspend. The warehouse has consumed 89 credits. What happens when it consumes credit #90?
    their options:
      option A: The warehouse is suspended immediately
      option B: An alert notification is sent (email to monitor administrators), but the warehouse continues running
      option C: The warehouse is suspended after currently-running queries complete
      option D: Nothing â€” the 90% trigger only applies to the account-level monitor, not individual warehouses
    correct Answer: option B
    explanation: >
      At the 90% threshold (90 credits out of 100), the configured action is 'Notify' â€” an email notification is sent to the users/roles assigned to receive alerts for this monitor. 
      The warehouse continues running normally. 
      The suspend action only triggers at the 100% threshold (100 credits). 
    difficulty level: Medium
    topic: Resource Monitors
    exam: Core

  - Question No: "088"
    question: Resource Monitor RM1 is applied to the ACCOUNT level with a monthly quota of 1,000 credits. Warehouse WH1 also has its own Resource Monitor RM2 with a daily quota of 50 credits. WH1 has consumed 50 credits today (RM2 at 100%) and 200 credits this month (RM1 at 20%). What happens?
    their options:
      option A: RM1 wins â€” WH1 continues running because account-level quota is not exceeded
      option B: RM2 triggers WH1 to suspend because its daily quota is reached, regardless of RM1
      option C: Both monitors must be at 100% before any action is taken
      option D: RM1 and RM2 average their thresholds â€” WH1 suspends when both reach 60%
    correct Answer: option B
    explanation: >
      Resource Monitors trigger independently. 
      RM2 (warehouse-level, daily 50 credits) has reached 100% and its action (e.g., Suspend) applies to WH1. 
      The account-level RM1 being at only 20% does not prevent RM2 from acting. 
      Multiple monitors can apply simultaneously â€” the most restrictive action takes effect. 
    difficulty level: Hard
    topic: Resource Monitors
    exam: Core

  - Question No: "089"
    question: What is SnowSQL?
    their options:
      option A: Snowflake's proprietary SQL dialect
      option B: A command-line interface (CLI) tool for connecting to Snowflake
      option C: The web-based UI for Snowflake (formerly called Snowsight)
      option D: A Java library for building Snowflake applications
    correct Answer: option B
    explanation: >
      SnowSQL is Snowflake's command-line interface. 
      It allows users to execute SQL queries, run scripts, load/unload data, and perform administrative tasks from the terminal. 
      It supports batch mode (script execution) and interactive mode. 
    difficulty level: Easy
    topic: Connectivity
    exam: Core

  - Question No: "090"
    question: Which Snowflake interface requires NO software installation?
    their options:
      option A: SnowSQL
      option B: JDBC Driver
      option C: Snowsight (Web UI)
      option D: Python Connector
    correct Answer: option C
    explanation: >
      Snowsight (formerly the Classic Web UI) is Snowflake's browser-based interface. 
      It requires no local installation â€” users access it via a web browser at app.snowflake.com or their account URL. 
      SnowSQL, JDBC, and Python Connector all require local installation. 
    difficulty level: Easy
    topic: Connectivity
    exam: Core

  - Question No: "091"
    question: A BI tool needs to connect to Snowflake using a standard database connectivity protocol supported by most analytics software. Which connection method is MOST appropriate?
    their options:
      option A: SnowSQL (CLI)
      option B: Snowpark Python API
      option C: ODBC or JDBC driver
      option D: Snowflake REST API
    correct Answer: option C
    explanation: >
      ODBC (Open Database Connectivity) and JDBC (Java Database Connectivity) are the universal standard protocols that virtually all BI tools (Tableau, Power BI, MicroStrategy, Looker, etc.) support. 
      Snowflake provides both drivers. 
      SnowSQL is for command-line use, Snowpark is for data engineering, and REST API is for programmatic access. 
    difficulty level: Medium
    topic: Connectivity
    exam: Core

  - Question No: "092"
    question: A data team writes a Snowpark Python application to transform data within Snowflake. What is the key advantage of Snowpark compared to extracting data to a Pandas-based Python script?
    their options:
      option A: Snowpark is faster because it runs on the Cloud Services layer
      option B: Snowpark pushes computation into Snowflake's engine â€” data doesn't move to the client. Transformations run on the virtual warehouse with full parallelism.
      option C: Snowpark automatically generates Snowflake SQL, which the developer can then review
      option D: Snowpark only works with structured data, so it's simpler than Pandas
    correct Answer: option B
    explanation: >
      With traditional Python (Pandas), you extract data to the client machine, transform it locally, and write it back â€” moving potentially terabytes of data over the network. 
      Snowpark runs transformation code directly on the virtual warehouse inside Snowflake. 
      The data never leaves Snowflake's infrastructure, and transformations leverage Snowflake's distributed compute. 
    difficulty level: Medium
    topic: Connectivity
    exam: Data Engineer

  - Question No: "093"
    question: What is a Snowflake Stream?
    their options:
      option A: A real-time data streaming service that replaces Kafka
      option B: A CDC (Change Data Capture) object that records DML changes (INSERT, UPDATE, DELETE) made to a table
      option C: A scheduled job that runs SQL on a defined interval
      option D: A continuous query that runs indefinitely on incoming data
    correct Answer: option B
    explanation: >
      A Snowflake Stream is a CDC object that tracks row-level changes (INSERT, UPDATE, DELETE) on a source table. 
      It records what changed, when, and what type of change occurred. 
      Streams do NOT move data in real-time like Kafka â€” they are a changelog capture mechanism used with batch processing. 
    difficulty level: Easy
    topic: Streams & Tasks
    exam: Data Engineer

  - Question No: "094"
    question: What SQL object in Snowflake is used to schedule recurring SQL execution?
    their options:
      option A: Procedure
      option B: Schedule
      option C: Task
      option D: Trigger
    correct Answer: option C
    explanation: >
      A Task is Snowflake's scheduled job mechanism. 
      Tasks run SQL statements (including CALL for stored procedures) on a defined schedule (cron syntax or interval) or in response to a predecessor Task completing (task graphs/DAGs). 
      Tasks require a virtual warehouse to execute. 
    difficulty level: Easy
    topic: Streams & Tasks
    exam: Data Engineer

  - Question No: "095"
    question: A Stream is created on table CUSTOMERS. What columns does the stream add to each change record?
    their options:
      option A: CHANGE_TYPE, CHANGE_TIME, CHANGED_BY
      option B: METADATA$ACTION, METADATA$ISUPDATE, METADATA$ROW_ID
      option C: CHANGE_ID, BEFORE_IMAGE, AFTER_IMAGE
      option D: STREAM_ID, DML_TYPE, TRANSACTION_ID
    correct Answer: option B
    explanation: >
      Snowflake Streams add three metadata columns to each record: METADATA$ACTION (INSERT or DELETE), METADATA$ISUPDATE (TRUE if the change resulted from an UPDATE), and METADATA$ROW_ID (unique identifier for the row). 
      An UPDATE shows as DELETE (old row) + INSERT (new row) both with METADATA$ISUPDATE = TRUE. 
    difficulty level: Medium
    topic: Streams & Tasks
    exam: Data Engineer

  - Question No: "096"
    question: A Serverless Task is configured with no virtual warehouse specified. How is compute billed for this task?
    their options:
      option A: No compute is billed â€” serverless tasks run on Cloud Services for free
      option B: Compute is billed using Snowflake-managed compute (serverless), charged per compute-second consumed by the task
      option C: A default X-Small warehouse is automatically used and billed
      option D: The task cannot run without a warehouse â€” it will fail
    correct Answer: option B
    explanation: >
      Serverless Tasks use Snowflake-managed compute resources (not a user-provisioned warehouse). 
      Billing is per compute-second actually consumed by the task â€” there's no minimum billing period. 
      This is more cost-efficient for short, infrequent tasks because you avoid the 60-second minimum warehouse billing. 
    difficulty level: Medium
    topic: Streams & Tasks
    exam: Data Engineer

  - Question No: "097"
    question: A task graph has Task A â†’ Task B â†’ Task C. Task B fails. What happens to Task C?
    their options:
      option A: Task C runs anyway, ignoring Task B's failure
      option B: Task C is skipped automatically â€” dependent tasks do not run if their predecessor fails
      option C: Task C is retried along with Task B
      option D: Task C runs, but with an error flag in its execution metadata
    correct Answer: option B
    explanation: >
      In a Snowflake task graph (DAG), tasks only run if their predecessor completes successfully. 
      If Task B fails, Task C will not execute. 
      The failure can be monitored via the TASK_HISTORY view and alerts can be configured. 
      Task A â†’ Task B failure suspends all downstream tasks. 
    difficulty level: Hard
    topic: Streams & Tasks
    exam: Data Engineer

  - Question No: "098"
    question: Snowpipe differs from COPY INTO in which fundamental way?
    their options:
      option A: Snowpipe supports more file formats than COPY INTO
      option B: Snowpipe automatically loads files as they arrive in a stage (event-driven); COPY INTO is a manually triggered batch load
      option C: Snowpipe is faster for large file loads
      option D: Snowpipe can load from external tables; COPY INTO cannot
    correct Answer: option B
    explanation: >
      The fundamental difference is the trigger mechanism. 
      COPY INTO is a manually executed SQL command (or scheduled via Tasks). 
      Snowpipe is event-driven â€” it monitors a stage and automatically triggers when new files arrive, via cloud event notifications (SQS for S3, Event Grid for Azure Blob) or REST API calls. 
    difficulty level: Easy
    topic: Snowpipe
    exam: Data Engineer

  - Question No: "099"
    question: A company uses Snowpipe to load from S3. Files arrive in the stage but are not being loaded. What is the MOST likely cause?
    their options:
      option A: Snowpipe requires a running virtual warehouse to process files
      option B: The SQS event notification is misconfigured or not connected to the Snowpipe definition
      option C: Snowpipe only works with internal stages, not S3 external stages
      option D: The files are too small for Snowpipe to process
    correct Answer: option B
    explanation: >
      Snowpipe with S3 uses AWS SQS (Simple Queue Service) event notifications to detect new files. 
      If SQS is misconfigured, Snowpipe never receives the 'new file' notification and doesn't process the files. 
      The file size is irrelevant (though large numbers of small files are inefficient). 
      Snowpipe uses Snowflake-managed serverless compute, not a user warehouse. 
    difficulty level: Medium
    topic: Snowpipe
    exam: Data Engineer

  - Question No: "100"
    question: A company uses Snowpipe REST API to push file loading notifications. The application sends a notification for the same file twice. What is the expected behavior?
    their options:
      option A: The file is loaded twice, causing duplicate data
      option B: Snowpipe deduplicates based on file name and stage â€” the second notification is ignored if the file was already processed within the deduplication window (24 hours)
      option C: The second notification fails with a 'file already loaded' error
      option D: Both notifications are processed, but Snowflake's internal deduplication merges the duplicate rows
    correct Answer: option B
    explanation: >
      Snowpipe provides at-least-once delivery semantics. 
      It maintains a deduplication token based on file path and stage, and within a 24-hour window, duplicate notifications for the same file are typically ignored. 
      However, this deduplication is not guaranteed beyond 24 hours. 
      For absolute idempotency, applications should implement their own deduplication logic downstream. 
    difficulty level: Hard
    topic: Snowpipe
    exam: Data Engineer

  - Question No: "101"
    question: Snowflake Secure Data Sharing allows data to be shared between accounts without which of the following?
    their options:
      option A: Copying or moving any data
      option B: Using SQL to query shared data
      option C: The consumer needing a Snowflake account
      option D: Setting up network connectivity between accounts
    correct Answer: option A
    explanation: >
      Secure Data Sharing shares live data without copying or moving it. 
      The consumer account reads directly from the provider's storage. 
      No ETL, no file exports, no data duplication. 
      The consumer does need a Snowflake account (or can use a Reader Account provisioned by the provider). 
    difficulty level: Easy
    topic: Data Sharing
    exam: Architect

  - Question No: "102"
    question: A data provider wants to share data with an external partner who does not have a Snowflake account. What is the solution?
    their options:
      option A: The partner must sign up for a Snowflake account before data can be shared
      option B: The provider creates a Reader Account (managed Snowflake account) for the partner â€” the provider pays for the partner's compute
      option C: Data can be exported to S3 and shared via pre-signed URLs
      option D: Cross-organization sharing requires Snowflake Marketplace listing
    correct Answer: option B
    explanation: >
      Reader Accounts are Snowflake accounts created and managed by a data provider specifically for external consumers who don't have their own Snowflake account. 
      The provider controls the Reader Account and pays for its compute resources. 
      The consumer can query shared data but cannot load their own data. 
    difficulty level: Medium
    topic: Data Sharing
    exam: Architect

  - Question No: "103"
    question: A Snowflake data provider shares a database with a consumer in the same AWS region. The consumer reports that querying the shared database is slow. What is the MOST likely cause?
    their options:
      option A: The shared database is locked by the provider's warehouse
      option B: The consumer's virtual warehouse is too small for the shared data volume â€” the consumer must use their own warehouse resources to query shared data
      option C: Data sharing has a latency penalty for same-region sharing
      option D: The share needs to be re-created to refresh performance metadata
    correct Answer: option B
    explanation: >
      When consuming shared data, the consumer's own virtual warehouse performs the computation. 
      The consumer's warehouse size directly impacts query performance. 
      The provider's warehouse is irrelevant â€” providers only serve the data storage. 
      If the consumer has an X-Small warehouse querying a 10 TB shared table, performance will be poor. 
    difficulty level: Hard
    topic: Data Sharing
    exam: Architect

  - Question No: "104"
    question: Snowflake database replication creates a secondary database that is:
    their options:
      option A: An exact real-time mirror updated synchronously
      option B: A read-only replica updated asynchronously on a defined schedule or on demand
      option C: A writable copy that synchronizes changes back to primary every hour
      option D: An independent copy with no automatic synchronization
    correct Answer: option B
    explanation: >
      Secondary databases created via replication are read-only replicas. 
      They are updated asynchronously â€” either on a schedule or when a REFRESH is manually triggered. 
      There is no real-time synchronous replication in Snowflake (as of standard replication). 
      This means the secondary may be slightly behind the primary. 
    difficulty level: Easy
    topic: Replication
    exam: Architect

  - Question No: "105"
    question: A company replicates a database from Account A (AWS us-east-1) to Account B (AWS eu-west-1). The refresh schedule is every 6 hours. An outage occurs in us-east-1. The company fails over to Account B. What is the potential data loss?
    their options:
      option A: No data loss â€” replication is synchronous
      option B: Up to 6 hours of data (since the last successful refresh)
      option C: Exactly 6 hours (replication always stays exactly on schedule)
      option D: No data loss because Fail-Safe covers the gap
    correct Answer: option B
    explanation: >
      With asynchronous replication on a 6-hour schedule, the secondary (Account B) may be up to 6 hours behind the primary at any given time. 
      In a failover, data written to Account A in the last 0-6 hours (since the last refresh) is not present in Account B. 
      This RPO (Recovery Point Objective) of up to 6 hours must be acceptable for the business. 
    difficulty level: Hard
    topic: Replication
    exam: Architect

  - Question No: "106"
    question: A query scans a 5 TB table but only needs 3 columns out of 200. How does Snowflake's columnar storage format help?
    their options:
      option A: Snowflake reads all 200 columns but ignores 197 during processing
      option B: Snowflake reads only the 3 requested columns' data from storage â€” 197 columns are not read at all
      option C: Columnar format compresses the 3 columns to read faster
      option D: Columnar format creates indexes on each column for faster access
    correct Answer: option B
    explanation: >
      Columnar storage stores each column separately. 
      A SELECT requesting only 3 of 200 columns means Snowflake only reads those 3 columns' micro-partition data from storage. 
      In row-based storage, every row's full 200-column record would need to be read. 
      This is why columnar storage dramatically reduces I/O for analytics workloads. 
    difficulty level: Medium
    topic: Performance
    exam: Core

  - Question No: "107"
    question: A query joins two large tables: ORDERS (1 billion rows) and CUSTOMERS (10 million rows). Performance is poor. The developer tries adding a clustering key to ORDERS on customer_id. Will this help?
    their options:
      option A: Yes â€” clustering ORDERS on customer_id will always speed up join performance
      option B: It depends: if queries filter on customer_id before the join, clustering helps pruning. If the join is between all rows without selective filtering, clustering alone won't significantly help the join itself.
      option C: No â€” clustering keys only help for selective range queries, not joins
      option D: Yes â€” clustering ensures ORDERS rows with the same customer_id are on the same nodes as CUSTOMERS rows
    correct Answer: option B
    explanation: >
      Clustering helps if the query filters on customer_id before (or as part of) the join â€” fewer ORDERS partitions are scanned. 
      If the join processes all rows (no selective filter), clustering doesn't reduce the join work. 
      For large joins, the optimizer's choice of join type (hash join vs. merge join) and the sizes of both tables matter more. 
    difficulty level: Hard
    topic: Performance
    exam: Core

  - Question No: "108"
    question: An architect reviews slow dashboard queries. The Query Profile shows 'Remote Disk I/O' as the dominant operation. What does this indicate, and what should the architect recommend?
    their options:
      option A: Remote Disk I/O is normal â€” it means data is being read from the storage layer. This is expected behavior.
      option B: Remote Disk I/O indicates the warehouse's local SSD cache is full or cold, forcing reads from cloud storage. Recommendation: increase warehouse size (more SSD cache), avoid frequent warehouse suspension to preserve warm cache, or consider a dedicated warehouse for this workload.
      option C: Remote Disk I/O means the virtual warehouse is undersized â€” upgrade to a larger warehouse
      option D: Remote Disk I/O indicates network congestion â€” contact Snowflake support
    correct Answer: option B
    explanation: >
      Remote Disk I/O (reading from S3/Blob/GCS) is slower than local SSD reads. 
      High Remote Disk I/O means the local warehouse cache is cold (data hasn't been cached yet) or the dataset is larger than the cache capacity. 
      Solutions: warm the cache by running typical queries before peak hours, avoid warehouse suspension for active workloads, and consider larger warehouse sizes for larger cache capacity. 
    difficulty level: Medium
    topic: Performance
    exam: Architect

  - Question No: "109"
    question: A COPY INTO command loads files from S3 but takes 4x longer than expected. Files are 5 MB each, and there are 10,000 of them. What is the performance problem and solution?
    their options:
      option A: 10,000 files is too many for COPY INTO â€” use Snowpipe instead
      option B: 5 MB files are too small; Snowflake processes each file with overhead that outweighs the data. Consolidate files to 100â€“250 MB before staging for optimal COPY INTO performance.
      option C: COPY INTO doesn't support large numbers of files â€” batch into groups of 100
      option D: The warehouse needs to be at least X-Large for 10,000 file loads
    correct Answer: option B
    explanation: >
      5 MB files are the classic small file problem. 
      COPY INTO (and Snowpipe) open and process each file individually. 
      With 10,000 files at 5 MB, the file-processing overhead dominates. 
      Optimal file size for Snowflake is 100â€“250 MB per file. 
      Consolidating 10,000 Ã— 5 MB files into ~200 Ã— 250 MB files would dramatically improve throughput. 
    difficulty level: Medium
    topic: Performance
    exam: Data Engineer

  - Question No: "110"
    question: What is Snowflake's primary access control model?
    their options:
      option A: DAC (Discretionary Access Control) only
      option B: MAC (Mandatory Access Control)
      option C: RBAC (Role-Based Access Control)
      option D: ABAC (Attribute-Based Access Control)
    correct Answer: option C
    explanation: >
      Snowflake uses RBAC (Role-Based Access Control). 
      Privileges are granted to roles, and roles are granted to users. 
      Users inherit all privileges of their active role. 
      Snowflake also supports DAC elements (object owners control their objects), but RBAC is the foundational model. 
    difficulty level: Easy
    topic: Security
    exam: Core

  - Question No: "111"
    question: What is the purpose of a Network Policy in Snowflake?
    their options:
      option A: Encrypt data in transit between Snowflake and clients
      option B: Restrict which IP addresses can connect to the Snowflake account
      option C: Control which cloud services Snowflake can access
      option D: Monitor network traffic and detect intrusions
    correct Answer: option B
    explanation: >
      Network Policies restrict access to a Snowflake account or user based on IP address ranges (IP allowlisting). 
      You can specify allowed IP ranges and blocked IP ranges. 
      Network Policies can be applied at the account level (affects all users) or at the individual user level. 
    difficulty level: Easy
    topic: Security
    exam: Core

  - Question No: "112"
    question: A column contains credit card numbers. A masking policy is applied so that analysts see 'XXXX-XXXX-XXXX-1234' (only last 4 digits visible). Data engineers see the full number. What Snowflake feature enables this?
    their options:
      option A: Row Access Policies
      option B: Column-level Security (Dynamic Data Masking)
      option C: Data Classification
      option D: Object Tagging
    correct Answer: option B
    explanation: >
      Dynamic Data Masking (part of Column-level Security) allows defining masking policies that return different values based on the querying user's role. 
      Analysts with a lower-privileged role see the masked value; data engineers with a higher-privileged role see the full value. 
      This requires Enterprise edition. 
    difficulty level: Medium
    topic: Security
    exam: Core

  - Question No: "113"
    question: An organization uses Tri-Secret Secure. The customer's CMK (Customer Managed Key) in AWS KMS is revoked. What happens to Snowflake data access?
    their options:
      option A: Data remains accessible because Snowflake maintains a copy of the key
      option B: All data encrypted with Tri-Secret Secure becomes immediately inaccessible to everyone, including Snowflake itself â€” both keys are required for decryption
      option C: Only external shares and replication are affected; internal queries continue
      option D: Snowflake Support can generate an emergency key to maintain access for 72 hours
    correct Answer: option B
    explanation: >
      Tri-Secret Secure requires both Snowflake's key AND the customer's CMK to decrypt data. 
      If the CMK is revoked, decryption is impossible for everyone â€” including Snowflake employees. 
      This provides the highest level of data sovereignty: a customer can 'destroy' their data immediately by revoking their key. 
      This is by design, not a bug. 
    difficulty level: Hard
    topic: Security
    exam: Architect

  - Question No: "114"
    question: A new employee asks why Snowflake doesn't require vacuuming or manual statistics updates like PostgreSQL. What is the correct explanation?
    their options:
      option A: Snowflake uses a much simpler storage engine that doesn't need maintenance
      option B: Snowflake's immutable micro-partitions never accumulate dead rows, and micro-partition metadata (statistics) is maintained automatically by the Cloud Services layer on every write
      option C: Vacuuming is done automatically every Sunday at midnight
      option D: Snowflake users can run VACUUM if needed, but it's optional
    correct Answer: option B
    explanation: >
      In PostgreSQL, UPDATE/DELETE marks rows as 'dead' in the same page â€” VACUUM cleans them up. 
      Snowflake uses immutable micro-partitions: changes create new partitions; old ones are retired automatically by garbage collection. 
      Statistics are maintained per-partition on every DML. 
      No manual maintenance is needed. 
    difficulty level: Medium
    topic: Architecture Overview
    exam: Core

  - Question No: "115"
    question: A Snowflake account has 5 running virtual warehouses of various sizes. A sixth warehouse is created but immediately suspended. How many sets of data cache (local SSD cache) exist?
    their options:
      option A: 6 â€” one per warehouse including the suspended one
      option B: 5 â€” only running warehouses have active SSD cache
      option C: 1 â€” all warehouses share a single cache pool
      option D: 0 â€” local cache only exists during active query execution
    correct Answer: option B
    explanation: >
      Local SSD cache only exists on running virtual warehouses. 
      A suspended warehouse has no active compute nodes and therefore no local cache. 
      When the sixth warehouse starts, it begins with an empty cache. 
      Only the 5 active warehouses maintain their own independent local caches. 
    difficulty level: Hard
    topic: Architecture Overview
    exam: Core

  - Question No: "116"
    question: An architect must design a Snowflake solution where the total monthly compute budget is strictly capped at $10,000 with zero tolerance for overrun. The solution must also ensure BI queries are never queued. Which combination achieves both goals?
    their options:
      option A: Multi-cluster warehouse (no budget cap possible) + Resource Monitor on account
      option B: Dedicated BI warehouse with Resource Monitor (Notify & Suspend Immediately at $10,000) + separate batch warehouse with its own monitor; accept that queuing may occur when BI monitor is suspended
      option C: Set a single warehouse to MAX_CONCURRENCY_LEVEL=200 with a $10,000 resource monitor
      option D: These two requirements are contradictory â€” guaranteeing no queuing requires unlimited scaling, which conflicts with a strict budget cap
    correct Answer: option D
    explanation: >
      Strict zero-tolerance budget cap AND zero queuing are fundamentally contradictory in a usage-based billing model. 
      When the resource monitor suspends the warehouse at budget limit, queries WILL queue (or fail). 
      Any solution must acknowledge this trade-off. 
      The architect must present this conflict to stakeholders and find an acceptable compromise (e.g., alert at 80%, hard limit only at month boundary). 
    difficulty level: Hard
    topic: Architecture Overview
    exam: Architect

  - Question No: "117"
    question: What is the Snowflake storage layer built on top of for an AWS-based account?
    their options:
      option A: AWS EFS (Elastic File System)
      option B: AWS S3 (Simple Storage Service)
      option C: AWS EBS (Elastic Block Store)
      option D: AWS Glacier
    correct Answer: option B
    explanation: >
      For AWS-based Snowflake accounts, the storage layer is built on Amazon S3. 
      However, customers cannot access this S3 bucket directly â€” it is owned and managed by Snowflake. 
      For Azure accounts, it's Azure Blob Storage; for GCP, it's Google Cloud Storage. 
    difficulty level: Easy
    topic: Architecture Overview
    exam: Data Engineer

  - Question No: "118"
    question: A table in Snowflake is described as having 'high clustering depth.' What does this indicate?
    their options:
      option A: The table has many nested/complex data types
      option B: The table's data is NOT well-sorted relative to its clustering key â€” many partitions have overlapping value ranges, reducing pruning effectiveness
      option C: The table has been clustered too many times and needs reclustering
      option D: The table has too many rows per partition
    correct Answer: option B
    explanation: >
      Clustering depth measures how many micro-partitions overlap for a given value range. 
      High clustering depth means many partitions could contain the same range of values â€” pruning eliminates fewer partitions and query performance suffers. 
      Low clustering depth (well-clustered data) means few partitions per value range, enabling aggressive pruning. 
    difficulty level: Easy
    topic: Micro-Partitions
    exam: Core

  - Question No: "119"
    question: Which SQL command provides information about the clustering state of a Snowflake table?
    their options:
      option A: DESCRIBE TABLE ... CLUSTERING
      option B: SHOW CLUSTERING KEYS IN TABLE
      option C: SYSTEM$CLUSTERING_INFORMATION('table_name')
      option D: SELECT * FROM INFORMATION_SCHEMA.CLUSTERING_STATS
    correct Answer: option C
    explanation: >
      SYSTEM$CLUSTERING_INFORMATION('table_name') returns a JSON object with clustering metrics including average overlaps, average depth, and the percentage of partitions in various clustering states. 
      This is the primary tool for assessing whether a clustering key is effective. 
    difficulty level: Medium
    topic: Micro-Partitions
    exam: Core

  - Question No: "120"
    question: A table has a clustering key on (COUNTRY, ORDER_DATE). A query filters WHERE COUNTRY = 'US' AND ORDER_DATE BETWEEN '2024-01-01' AND '2024-03-31'. How effective is the clustering key?
    their options:
      option A: Not effective â€” compound clustering keys never work well
      option B: Very effective â€” both filter columns match the clustering key columns in the correct order
      option C: Partially effective â€” COUNTRY filter benefits from clustering, but ORDER_DATE filtering within US rows may have limited benefit
      option D: Only ORDER_DATE clustering is used â€” the COUNTRY column is ignored
    correct Answer: option B
    explanation: >
      The compound clustering key (COUNTRY, ORDER_DATE) means data is primarily sorted by COUNTRY, then by ORDER_DATE within each country. 
      A query filtering on both columns in the same order as the key benefits from aggressive pruning: first, all non-US partitions are eliminated; then within US partitions, those outside the date range are eliminated. 
      This is highly effective. 
    difficulty level: Medium
    topic: Micro-Partitions
    exam: Architect

  - Question No: "121"
    question: A Data Engineer loads data daily in batches. Each batch contains records for the past 90 days (backfill pattern). After 6 months, query performance is poor. The table has a clustering key on ORDER_DATE. What is the issue?
    their options:
      option A: The clustering key needs to be on LOAD_DATE instead of ORDER_DATE
      option B: Daily backfill loads create heavily overlapping micro-partitions for old dates â€” each new load adds partitions for all 90 days, causing partition overlap across the entire 90-day range. Automatic Clustering will reorganize this but may lag behind the load pattern.
      option C: The table is too large for clustering â€” remove the clustering key
      option D: Clustering keys only work for the most recent 30 days of data
    correct Answer: option B
    explanation: >
      Backfill patterns (loading historical date ranges repeatedly) are the enemy of date-based clustering. 
      Each daily load creates new micro-partitions containing rows for the past 90 days, leading to many overlapping partitions for any given date. 
      Automatic Clustering will attempt to reorganize, but aggressive backfill may consume significant clustering credits. 
      Consider loading in date-order batches or using a MERGE approach. 
    difficulty level: Hard
    topic: Micro-Partitions
    exam: Data Engineer

  - Question No: "122"
    question: What parameter controls how long a query can wait in the warehouse queue before failing?
    their options:
      option A: QUERY_TIMEOUT
      option B: STATEMENT_QUEUED_TIMEOUT_IN_SECONDS
      option C: MAX_QUEUE_WAIT_SECONDS
      option D: CONCURRENCY_QUEUE_TIMEOUT
    correct Answer: option B
    explanation: >
      STATEMENT_QUEUED_TIMEOUT_IN_SECONDS defines the maximum time (in seconds) a query can wait in the warehouse's concurrency queue. 
      If set to 60, a query that hasn't started executing within 60 seconds of being submitted will fail with a timeout error. 
      Default is 0 (no timeout â€” queries queue indefinitely). 
    difficulty level: Easy
    topic: Virtual Warehouses
    exam: Core

  - Question No: "123"
    question: A developer wants to ensure their long-running ETL query cannot run for more than 2 hours to prevent runaway jobs. What parameter should be set?
    their options:
      option A: AUTO_SUSPEND = 7200
      option B: STATEMENT_TIMEOUT_IN_SECONDS = 7200 on the warehouse or session
      option C: MAX_RUNTIME_SECONDS = 7200 on the query
      option D: QUERY_MAX_SECONDS = 7200 in the account settings
    correct Answer: option B
    explanation: >
      STATEMENT_TIMEOUT_IN_SECONDS defines the maximum execution time for a single query/statement. 
      Setting it to 7200 (2 hours = 7200 seconds) will cancel any query that exceeds this runtime. 
      It can be set at the account, warehouse, user, or session level. 
      The most specific level takes precedence. 
    difficulty level: Medium
    topic: Virtual Warehouses
    exam: Core

  - Question No: "124"
    question: Warehouse WH1 has MAX_CONCURRENCY_LEVEL = 8. User A runs a query that takes 60 seconds. 5 seconds after User A's query starts, 8 more users each submit a query simultaneously. What happens?
    their options:
      option A: All 9 queries (A + 8 new) run immediately on the warehouse
      option B: User A's query continues; 7 new queries run (filling to MAX_CONCURRENCY_LEVEL=8 total); 1 new query queues
      option C: User A's query continues; all 8 new queries queue because A is using one slot (7 slots available... but 8 queries fill them + 1 queues)
      option D: All 8 new queries queue behind User A's query
    correct Answer: option B
    explanation: >
      MAX_CONCURRENCY_LEVEL = 8 means up to 8 queries run simultaneously. 
      User A is query #1 (slot 1). 
      When 8 more arrive, slots 2â€“8 are filled by 7 of them (total = 8 running). 
      The 8th new query queues. 
      Running: A + 7 others (8 total). 
      Queued: 1. 
    difficulty level: Hard
    topic: Virtual Warehouses
    exam: Core

  - Question No: "125"
    question: A production multi-cluster warehouse is configured with Min=2, Max=8, Economy scaling. During normal business hours, 3 clusters run. Outside business hours (nights and weekends), query volume drops to near zero. What is the compute cost during off-hours?
    their options:
      option A: $0 â€” clusters suspend when no queries run
      option B: The cost of running 2 clusters continuously (Min=2 always keeps 2 running)
      option C: The cost of running 1 cluster (the minimum collapses to 1 at night)
      option D: The cost of all 8 clusters (Max stays provisioned for readiness)
    correct Answer: option B
    explanation: >
      With Min=2, at least 2 clusters always run regardless of query volume. 
      Outside business hours, clusters 3+ will shut down as concurrency drops, but clusters 1 and 2 remain running. 
      This represents continuous cost for the minimum clusters. 
      Organizations with no off-hours workloads may prefer Min=0 if they can tolerate startup latency for the first morning queries. 
    difficulty level: Medium
    topic: Virtual Warehouses
    exam: Architect

  - Question No: "126"
    question: A data pipeline needs a warehouse to load data, immediately transform it, and write results. The pipeline runs for exactly 45 seconds. How many seconds is the warehouse billed for?
    their options:
      option A: 45 seconds
      option B: 60 seconds (minimum billing period)
      option C: 90 seconds (minimum period Ã— 2 for load and transform phases)
      option D: 120 seconds (safety margin applied automatically)
    correct Answer: option B
    explanation: >
      Minimum billing per warehouse start is 60 seconds regardless of actual usage. 
      Even though the pipeline completes in 45 seconds, 60 seconds are billed. 
      If the pipeline ran for 61 seconds, 61 seconds would be billed. 
      Design: if tasks take under 60 seconds, try to batch them together to amortize the per-start minimum. 
    difficulty level: Medium
    topic: Virtual Warehouses
    exam: Data Engineer

  - Question No: "127"
    question: A user adds a comment to a query: /* department: finance */ SELECT SUM(revenue) FROM sales. The exact same query without the comment was run 5 minutes ago and is in the Result Cache. Will the cached result be used?
    their options:
      option A: Yes â€” Snowflake ignores comments when matching Result Cache entries
      option B: No â€” the comment changes the SQL text, so it's treated as a different query and the cache is bypassed
      option C: Yes â€” Snowflake normalizes whitespace and comments before cache lookup
      option D: It depends on the CACHE_COMMENT_SENSITIVE account setting
    correct Answer: option B
    explanation: >
      Result Cache matching is based on the exact SQL text. 
      A comment (/* department: finance */) changes the text, so Snowflake treats it as a different query. 
      The cache from the comment-free version is not used. 
      This is why standardizing query text in BI tools is important for maximizing Result Cache effectiveness. 
    difficulty level: Hard
    topic: Caching
    exam: Core

  - Question No: "128"
    question: An architect wants maximum query performance for a fixed set of executive dashboards that run every morning. The dashboards use the same SQL every day but query data that was updated overnight. What is the BEST design?
    their options:
      option A: Enable a permanent Result Cache for the dashboard queries
      option B: Accept that Result Cache is invalidated by overnight data updates â€” performance optimization should focus on clustering, warehouse sizing, and materialized views to pre-aggregate results
      option C: Create a static snapshot table that dashboards query instead of live data
      option D: Schedule dashboard queries to run at 2 AM before business users arrive to pre-warm the cache
    correct Answer: option B
    explanation: >
      Since overnight data updates invalidate Result Cache, pre-warming alone doesn't work (stale cache before data loads, valid cache only after load). 
      Materialized Views can pre-aggregate expensive computations and are refreshed automatically when base data changes. 
      Combined with proper clustering, this is the most robust solution for recurring executive dashboards. 
    difficulty level: Hard
    topic: Caching
    exam: Architect

  - Question No: "129"
    question: A company runs Standard edition and wants to set DATA_RETENTION_TIME_IN_DAYS = 7 on a critical table. What happens?
    their options:
      option A: The command succeeds and 7-day retention is applied
      option B: The command fails with an error â€” Standard edition maximum is 1 day. Enterprise edition is required for retention > 1 day.
      option C: The command succeeds but is silently capped at 1 day
      option D: The command requires ACCOUNTADMIN role which can override edition limits
    correct Answer: option B
    explanation: >
      Standard edition hard caps DATA_RETENTION_TIME_IN_DAYS at 1. 
      Running ALTER TABLE t SET DATA_RETENTION_TIME_IN_DAYS = 7 on Standard edition returns an error: 'Time Travel retention for table T is too high for account edition Standard.' Enterprise edition must be purchased to use retention > 1 day. 
    difficulty level: Hard
    topic: Editions
    exam: Core

  - Question No: "130"
    question: Which statement is TRUE about Standard edition Fail-Safe?
    their options:
      option A: Standard edition does not have Fail-Safe â€” it's an Enterprise feature
      option B: Standard edition includes 7-day Fail-Safe for all permanent tables, same as all editions
      option C: Standard edition includes only 1-day Fail-Safe (matching its Time Travel period)
      option D: Fail-Safe in Standard edition requires additional purchase
    correct Answer: option B
    explanation: >
      Fail-Safe is available in ALL Snowflake editions, always 7 days (fixed). 
      The edition difference is only in Time Travel duration (Standard: 1 day max, Enterprise+: 90 days max). 
      A Standard edition table with 1-day retention still has 7-day Fail-Safe after Time Travel expires. 
    difficulty level: Medium
    topic: Editions
    exam: Core

  - Question No: "131"
    question: What type of object is a STAGE in Snowflake?
    their options:
      option A: An account-level object like a warehouse
      option B: A schema-level object â€” it lives inside a database and schema
      option C: A database-level object â€” it lives directly inside a database
      option D: A special cross-database object
    correct Answer: option B
    explanation: >
      Stages (both internal and external) are schema-level objects. 
      They live inside a database.schema namespace. 
      For example: CREATE STAGE mydb.myschema.my_stage. 
      Warehouses, on the other hand, are account-level objects. 
    difficulty level: Easy
    topic: Object Hierarchy
    exam: Core

  - Question No: "132"
    question: A user queries table A in database DB1. They are currently connected to database DB2. Which query syntax is CORRECT?
    their options:
      option A: SELECT * FROM A (Snowflake auto-detects the database)
      option B: SELECT * FROM DB1..A (double dot notation)
      option C: SELECT * FROM DB1.SCHEMA1.A (fully qualified name)
      option D: USE DATABASE DB1 must be run â€” cross-database queries are not allowed in Snowflake
    correct Answer: option C
    explanation: >
      Cross-database queries are fully supported in Snowflake using fully qualified three-part names: DATABASE.SCHEMA.OBJECT. 
      The current USE DATABASE setting doesn't restrict access to objects in other databases â€” you simply reference them by their full path. 
      Two-dot notation (DB1..A) is also sometimes used, defaulting to PUBLIC schema. 
    difficulty level: Medium
    topic: Object Hierarchy
    exam: Core

  - Question No: "133"
    question: A developer creates a stored procedure in schema MY_SCHEMA that uses CREATE TABLE to build a result table. The procedure owner is role ENGINEER_ROLE. The procedure is called by an analyst with role ANALYST_ROLE. ANALYST_ROLE does NOT have CREATE TABLE privilege on MY_SCHEMA. Will the CREATE TABLE inside the procedure succeed?
    their options:
      option A: No â€” the analyst's role (ANALYST_ROLE) is checked for CREATE TABLE privilege
      option B: Yes â€” stored procedures in Snowflake run with owner's rights by default (EXECUTE AS OWNER), so ENGINEER_ROLE's privileges apply
      option C: Only if the procedure has EXECUTE AS CALLER specified
      option D: Yes â€” analyst privilege doesn't matter inside stored procedures
    correct Answer: option B
    explanation: >
      Snowflake stored procedures have two execution modes: EXECUTE AS OWNER (default) uses the procedure creator/owner's role privileges, and EXECUTE AS CALLER uses the calling user's role privileges. 
      By default (owner's rights), ENGINEER_ROLE's CREATE TABLE privilege applies, so the table creation succeeds even though ANALYST_ROLE lacks that privilege. 
    difficulty level: Hard
    topic: Object Hierarchy
    exam: Core

  - Question No: "134"
    question: A user wants to undo all changes made to table ORDERS in the last 10 minutes. What is the correct approach?
    their options:
      option A: ROLLBACK LAST 10 MINUTES ON ORDERS
      option B: CREATE OR REPLACE TABLE ORDERS AS SELECT * FROM ORDERS AT (OFFSET => -600)
      option C: RESTORE TABLE ORDERS TO 10 MINUTES AGO
      option D: ALTER TABLE ORDERS RESET TO OFFSET -600
    correct Answer: option B
    explanation: >
      To restore a table to a previous state using Time Travel, the pattern is: CREATE OR REPLACE TABLE t AS SELECT * FROM t AT (OFFSET => -600) â€” this overwrites the current table with its state from 600 seconds (10 minutes) ago. 
      Alternatively, create a new table with the historical data and swap. 
    difficulty level: Medium
    topic: Time Travel
    exam: Core

  - Question No: "135"
    question: A schema has DATA_RETENTION_TIME_IN_DAYS = 14. A table within the schema has DATA_RETENTION_TIME_IN_DAYS = 3. What retention period applies to the table?
    their options:
      option A: 14 days (schema setting overrides table setting)
      option B: 3 days (table-level setting overrides schema-level setting)
      option C: 17 days (settings are additive)
      option D: 3 days for Time Travel; 14 days for Fail-Safe
    correct Answer: option B
    explanation: >
      Lower-level (more specific) parameter settings override higher-level settings. 
      The table-level DATA_RETENTION_TIME_IN_DAYS = 3 overrides the schema-level setting of 14 days. 
      The table has only 3 days of Time Travel, not 14. 
      This allows fine-grained control: set a default at schema level, override for specific tables. 
    difficulty level: Hard
    topic: Time Travel
    exam: Core

  - Question No: "136"
    question: A Data Engineer accidentally runs UPDATE orders SET amount = 0 WHERE 1=1 (updating all rows). The error is discovered 2 hours later. The table has 90-day Time Travel. What is the FASTEST recovery?
    their options:
      option A: Contact Snowflake Support for Fail-Safe recovery
      option B: CREATE OR REPLACE TABLE orders AS SELECT * FROM orders AT (OFFSET => -7200) â€” restore from Time Travel snapshot 2 hours ago
      option C: Run UNDROP TABLE orders (doesn't apply to updates, only drops)
      option D: Restore from an external backup in S3
    correct Answer: option B
    explanation: >
      UNDROP only recovers dropped tables. 
      For a mass UPDATE, Time Travel is used: the query SELECT * FROM orders AT (OFFSET => -7200) returns the table state 2 hours ago (before the bad UPDATE). 
      Wrapping in CREATE OR REPLACE TABLE replaces the current corrupted table with the clean historical version. 
      With 90-day retention and only 2 hours elapsed, this is straightforward. 
    difficulty level: Medium
    topic: Time Travel
    exam: Data Engineer

  - Question No: "137"
    question: Which of the following objects CANNOT be zero-copy cloned in Snowflake?
    their options:
      option A: Tables
      option B: Schemas
      option C: Databases
      option D: External Tables (read-only metadata objects pointing to external files)
    correct Answer: option D
    explanation: >
      External tables are metadata-only objects pointing to files in external storage â€” there's no Snowflake-managed data to clone. 
      Other limitations: pipes (Snowpipe) and some integrations are not cloned. 
      Tables, schemas, databases, and their contents (views, functions, stages) CAN be cloned. 
    difficulty level: Medium
    topic: Zero-Copy Cloning
    exam: Core

  - Question No: "138"
    question: A production database is cloned daily for a data science team. The clone is deleted each evening and re-created the next morning. After 30 days, the team complains that the clone creation is increasingly slow. Why is this happening?
    their options:
      option A: More data has been added to production, making clone creation slower
      option B: Clone creation time is proportional to data volume â€” this is expected behavior as data grows
      option C: This should NOT be happening â€” zero-copy clones are always instantaneous regardless of data size. If clone creation is taking longer, it might indicate underlying infrastructure issues or excessive metadata complexity from extremely large schema objects (millions of tables/partitions), not data volume.
      option D: Clones get slower each time because Snowflake tracks clone lineage
    correct Answer: option C
    explanation: >
      Zero-copy clone creation is a metadata operation and should be nearly instantaneous regardless of data volume. 
      If clone creation of a database is slowing down over 30 days, the issue is NOT the data volume but potentially: increasing schema complexity (many new tables/objects), metadata management overhead, or a potential Snowflake platform issue. 
      The developer's premise may be incorrect â€” they should measure actual clone creation time to verify. 
    difficulty level: Hard
    topic: Zero-Copy Cloning
    exam: Architect

  - Question No: "139"
    question: What is an Internal Stage in Snowflake?
    their options:
      option A: A staging area inside a user's own S3 bucket
      option B: A Snowflake-managed storage location for staging files before loading into tables
      option C: A temporary table used as intermediate storage during ETL
      option D: A schema designated for pre-production data
    correct Answer: option B
    explanation: >
      Internal Stages are Snowflake-managed storage locations. 
      Files are uploaded to Snowflake's own cloud storage (not the customer's). 
      Three types: Named Stages (CREATE STAGE), User Stages (one per user, ~@~user/stage), and Table Stages (one per table, @%table_name). 
      External stages reference customer-owned cloud storage. 
    difficulty level: Easy
    topic: Data Loading
    exam: Data Engineer

  - Question No: "140"
    question: Which command is used to load data from a Snowflake stage into a table?
    their options:
      option A: LOAD INTO my_table FROM @my_stage
      option B: IMPORT INTO my_table FROM @my_stage
      option C: COPY INTO my_table FROM @my_stage
      option D: INSERT INTO my_table FROM @my_stage
    correct Answer: option C
    explanation: >
      COPY INTO is the primary command for bulk loading data from a stage into a table. 
      Syntax: COPY INTO table_name FROM @stage_name FILE_FORMAT = (type = 'CSV'). 
      It also supports loading from external cloud storage URLs directly. 
    difficulty level: Easy
    topic: Data Loading
    exam: Data Engineer

  - Question No: "141"
    question: A developer runs COPY INTO but wants to load only specific files from the stage, not all files. How is this done?
    their options:
      option A: Use WHERE clause: COPY INTO t FROM @s WHERE filename LIKE '2024%'
      option B: Specify files in the FILES parameter: COPY INTO t FROM @s FILES = ('file1.csv', 'file2.csv')
      option C: Move unwanted files out of the stage first
      option D: COPY INTO always loads all files â€” use SELECT statements to filter rows after loading
    correct Answer: option B
    explanation: >
      The FILES parameter in COPY INTO allows specifying a list of specific files to load. 
      Alternatively, PATTERN = '.*2024.*\\.csv' uses a regex to match file names. 
      Both provide selective loading without moving files. 
    difficulty level: Medium
    topic: Data Loading
    exam: Data Engineer

  - Question No: "142"
    question: A COPY INTO command runs with ON_ERROR = CONTINUE. 5 of 100 files have formatting errors. What is the outcome?
    their options:
      option A: All 100 files fail â€” CONTINUE only applies to row-level errors within a file
      option B: 95 files are loaded successfully; the 5 error files are skipped and their errors are recorded in the load history and accessible via VALIDATE()
      option C: All 100 files are attempted; error rows within the 5 bad files are skipped; good rows within those files are loaded
      option D: The command fails with a warning about the 5 error files
    correct Answer: option B
    explanation: >
      ON_ERROR = CONTINUE instructs Snowflake to skip files that encounter errors and continue loading the remaining files. 
      The 95 good files load successfully. 
      The 5 error files are recorded as failed in load history with their error details. 
      VALIDATE() can be used to inspect errors after the fact. 
      Note: at the row level within a file, CONTINUE skips error rows. 
    difficulty level: Hard
    topic: Data Loading
    exam: Data Engineer

  - Question No: "143"
    question: What is the recommended file size for optimal COPY INTO performance?
    their options:
      option A: As large as possible (multi-GB files are ideal)
      option B: 100â€“250 MB per file
      option C: Exactly 16 MB to match micro-partition size
      option D: Less than 1 MB for parallel loading
    correct Answer: option B
    explanation: >
      Snowflake recommends files of 100â€“250 MB for optimal COPY INTO performance. 
      Files in this range allow efficient parallelism while avoiding excessive metadata overhead from too many small files. 
      Very large files (multi-GB) can limit parallelism. 
      The 16 MB micro-partition size is the storage unit after loading, not the input file size. 
    difficulty level: Medium
    topic: Data Loading
    exam: Data Engineer

  - Question No: "144"
    question: A Stream is created on a table with the default STREAM_TYPE. What type of stream is this?
    their options:
      option A: APPEND_ONLY stream â€” captures only INSERT operations
      option B: STANDARD stream â€” captures INSERT, UPDATE, and DELETE operations
      option C: DELTA stream â€” captures net changes (before and after images merged)
      option D: FULL stream â€” captures a full snapshot of the table on each refresh
    correct Answer: option B
    explanation: >
      The default stream type is STANDARD, which captures all DML changes: INSERTs (ACTION=INSERT), DELETEs (ACTION=DELETE), and UPDATEs (as DELETE + INSERT pairs with ISUPDATE=TRUE). 
      APPEND_ONLY streams exist for tables that only receive inserts (like event log tables) â€” they're more efficient because they only track new rows. 
    difficulty level: Medium
    topic: Streams & Tasks
    exam: Data Engineer

  - Question No: "145"
    question: A Stream's STALE status is TRUE. What does this mean and what action is required?
    their options:
      option A: The stream has no unconsumed data â€” it's 'stale' because there's nothing to process. No action needed.
      option B: The stream's offset has fallen outside the Time Travel retention window of the source table â€” the historical data needed to reconstruct changes is gone. The stream must be dropped and recreated.
      option C: The stream is paused because the source table is locked
      option D: The stream needs a warehouse to refresh its data â€” submit a query to reactivate it
    correct Answer: option B
    explanation: >
      A STALE stream means the stream's offset (the point in time from which it tracks changes) has fallen outside the source table's Time Travel retention window. 
      The historical micro-partitions the stream needs to compute changes no longer exist. 
      A stale stream cannot be used â€” it must be dropped and recreated. 
      To prevent staleness: ensure streams are consumed regularly, or increase the table's Time Travel retention. 
    difficulty level: Hard
    topic: Streams & Tasks
    exam: Data Engineer

  - Question No: "146"
    question: What happens to a Stream's offset after a Task successfully executes a DML statement using the stream's data?
    their options:
      option A: The offset doesn't advance until the Task is manually reset
      option B: The offset advances to the current time (commits to the point after the DML completed) â€” consumed changes are no longer visible in the stream
      option C: The offset advances by exactly 24 hours
      option D: The offset is cleared and the stream starts capturing changes from scratch
    correct Answer: option B
    explanation: >
      When a Task executes a DML that reads a Stream within a transaction, and that transaction commits successfully, the Stream's offset advances to the current timestamp. 
      The consumed changes are no longer visible â€” the stream now only shows changes since the last successful consumption. 
      This is how Streams + Tasks implement reliable incremental processing. 
    difficulty level: Medium
    topic: Streams & Tasks
    exam: Data Engineer

  - Question No: "147"
    question: What does 'spillage to remote storage' in a Query Profile indicate?
    their options:
      option A: Data is being backed up to external storage during processing
      option B: The warehouse ran out of local SSD space and is writing intermediate results to cloud storage (S3/Blob/GCS) â€” a significant performance degradation indicator
      option C: The query is writing its final results to an external stage
      option D: Snowflake is auto-optimizing by offloading cold data
    correct Answer: option B
    explanation: >
      Spillage to remote storage means the warehouse exhausted both memory AND local SSD, and is writing intermediate query data (hash join tables, sort buffers) to cloud storage. 
      Remote storage I/O is orders of magnitude slower than memory. 
      This indicates the warehouse is too small for the query â€” increase the warehouse size. 
    difficulty level: Easy
    topic: Performance
    exam: Core

  - Question No: "148"
    question: A query runs the same JOIN 5 times with different parameter values. Each run takes 10 minutes. An analyst suggests using a materialized view to pre-compute the JOIN result. What is the benefit?
    their options:
      option A: Materialized views eliminate JOIN computation entirely, so each query is instant
      option B: Materialized views pre-compute and store the JOIN result. Queries against the materialized view read the stored result instead of re-computing the JOIN, dramatically reducing query time.
      option C: Materialized views cache the JOIN parameters, not the results
      option D: Materialized views convert the JOIN to a simpler UNION operation
    correct Answer: option B
    explanation: >
      Materialized Views pre-compute expensive transformations (JOINs, aggregations) and store the physical results. 
      Queries against the MV read the pre-computed data instead of re-running the JOIN on base tables. 
      Snowflake automatically keeps MVs fresh when base data changes. 
      MVs require Enterprise edition. 
    difficulty level: Medium
    topic: Performance
    exam: Core

  - Question No: "149"
    question: An architect notices that the same large aggregation query runs well the first time (15 seconds) but subsequent identical runs are taking 15 seconds again instead of using cache. Investigation shows the underlying table is updated with new rows every 2 minutes. What should the architect implement?
    their options:
      option A: Increase warehouse size to speed up repeated queries
      option B: Use a Materialized View to pre-aggregate the data â€” the MV is refreshed automatically when new data arrives, and queries against it benefit from MV-level caching
      option C: Disable automatic updates and only refresh data hourly to preserve Result Cache
      option D: Accept 15-second query times â€” this is expected for frequently-updated data
    correct Answer: option B
    explanation: >
      Frequent data updates (every 2 minutes) invalidate Result Cache immediately. 
      Materialized Views solve this because: (1) the MV stores pre-aggregated results, (2) MV refresh happens in the background when data changes, and (3) queries against a refreshed MV are fast because they read pre-computed data. 
      The MV itself can benefit from Result Cache between refreshes. 
    difficulty level: Medium
    topic: Performance
    exam: Architect

  - Question No: "150"
    question: A Data Engineer is optimizing a 1 TB table join. The small table has 50,000 rows (10 MB). The large table has 10 billion rows. What join optimization does Snowflake apply automatically?
    their options:
      option A: Hash join â€” both tables are hashed on the join key
      option B: Broadcast join (shuffle replicate) â€” the small 10 MB table is broadcast to all compute nodes, each of which then performs a local join with its local partition of the large table
      option C: Merge join â€” both tables are sorted and merged
      option D: Sort-merge join â€” requires a clustering key on both tables
    correct Answer: option B
    explanation: >
      Snowflake's query optimizer automatically selects a broadcast join (also called a replicated join) when one table is significantly smaller than the other. 
      The 10 MB small table is sent to every compute node. 
      Each node then performs a local hash join with its portion of the large table, eliminating the need to redistribute the large table's data across nodes. 
    difficulty level: Hard
    topic: Performance
    exam: Data Engineer

  - Question No: "151"
    question: Which built-in role has the highest level of privileges in a Snowflake account?
    their options:
      option A: SYSADMIN
      option B: SECURITYADMIN
      option C: ACCOUNTADMIN
      option D: SUPERUSER
    correct Answer: option C
    explanation: >
      ACCOUNTADMIN is the top-level system-defined role in Snowflake. 
      It can manage all aspects of the account: billing, users, roles, warehouses, databases, security policies, and more. 
      Best practice: minimize the number of users assigned to ACCOUNTADMIN and use it only for administrative tasks. 
    difficulty level: Medium
    topic: Security
    exam: Core

  - Question No: "152"
    question: A user is assigned roles ROLE_A and ROLE_B. ROLE_A has SELECT on TABLE_X. ROLE_B has no privileges on TABLE_X. The user's primary role is ROLE_B. Can the user SELECT from TABLE_X?
    their options:
      option A: Yes â€” Snowflake automatically uses the most privileged role across all assigned roles
      option B: No â€” only the primary role's privileges apply in a session. The user must explicitly use ROLE_A with USE ROLE ROLE_A to access TABLE_X.
      option C: Yes â€” secondary roles activate automatically when a primary role lacks access
      option D: No â€” users with multiple roles always use the lowest-privileged role for security
    correct Answer: option B
    explanation: >
      In Snowflake, the active session uses ONE role at a time (the primary role or a role explicitly activated with USE ROLE). 
      By default, only the primary role's privileges apply. 
      The user must run USE ROLE ROLE_A to access TABLE_X. 
      Note: Secondary roles (activated with USE SECONDARY ROLES ALL) can be explicitly enabled to combine privileges from all assigned roles. 
    difficulty level: Hard
    topic: Security
    exam: Core

  - Question No: "153"
    question: What is a Snowflake Network Policy?
    their options:
      option A: A policy that encrypts all network traffic
      option B: A policy that restricts Snowflake access to specified IP address ranges
      option C: A policy that controls which external services Snowflake can call
      option D: A policy that limits the number of concurrent network connections
    correct Answer: option B
    explanation: >
      Network Policies in Snowflake control access based on IP addresses. 
      You define allowed IP ranges (and optionally blocked IP ranges). 
      Network Policies can be applied at the account level (affects all users) or at the user level. 
      They are a first line of defense against unauthorized access from unexpected network locations. 
    difficulty level: Easy
    topic: Security
    exam: Core

  - Question No: "154"
    question: An organization wants analysts to see customer email addresses as 'j***@example.com' (partial masking) while data stewards see the full email. Snowflake's Dynamic Data Masking should be used. What determines which masked value a user sees?
    their options:
      option A: The table's owner role
      option B: The user's current active role at query time
      option C: The database where the table lives
      option D: The type of warehouse used for the query
    correct Answer: option B
    explanation: >
      Dynamic Data Masking policies use conditions based on CURRENT_ROLE() (or IS_ROLE_IN_SESSION()) to determine which value to return. 
      The masking function checks the active role: if the user's role is 'DATA_STEWARD', return the full value; otherwise return the masked version. 
      Different users see different values based on their role when running the query. 
    difficulty level: Medium
    topic: Security
    exam: Architect

  - Question No: "155"
    question: What is Snowpark?
    their options:
      option A: A new Snowflake data sharing marketplace
      option B: A developer framework for building data pipelines and ML applications entirely within Snowflake using Python, Java, or Scala
      option C: A Snowflake add-on for real-time streaming analytics
      option D: A drag-and-drop ETL tool built into Snowsight
    correct Answer: option B
    explanation: >
      Snowpark is Snowflake's developer framework that allows writing data transformations and ML pipelines using Python, Java, or Scala. 
      Code runs directly on Snowflake's compute engine â€” data doesn't move to external systems. 
      It provides a DataFrame API similar to Spark/Pandas for Snowflake-native distributed processing. 
    difficulty level: Easy
    topic: Snowpark
    exam: Data Engineer

  - Question No: "156"
    question: A Snowpark Python UDF is defined as a vectorized UDF (table function). What is the primary benefit over a scalar UDF?
    their options:
      option A: Vectorized UDFs process entire batches of rows as Apache Arrow batches, matching Snowflake's columnar processing model and dramatically improving throughput vs row-by-row scalar UDFs
      option B: Vectorized UDFs run on the Cloud Services layer, bypassing warehouse billing
      option C: Vectorized UDFs can return multiple columns while scalar UDFs return one value
      option D: Vectorized UDFs can access external APIs that scalar UDFs cannot
    correct Answer: option A
    explanation: >
      Snowflake processes data in columnar batches internally (using Apache Arrow format). 
      Vectorized UDFs receive entire batches (columns) as Arrow arrays, enabling NumPy/Pandas-style vectorized operations on entire column arrays at once. 
      Scalar UDFs process one row at a time, invoking Python for every row â€” the per-row function call overhead is massive for large datasets. 
      Vectorized UDFs can achieve 10â€“100x better performance. 
    difficulty level: Medium
    topic: Snowpark
    exam: Data Engineer

  - Question No: "157"
    question: A Snowpark ML feature store pipeline needs to maintain a real-time feature table updated within 1 minute of source changes. Which Snowflake components form the most robust solution?
    their options:
      option A: Hourly Tasks + COPY INTO
      option B: Streams on source tables + Tasks with SYSTEM$STREAM_HAS_DATA + Serverless Tasks for near-real-time processing (sub-minute scheduling)
      option C: Snowpipe for source loading + a View joining the source data
      option D: Materialized Views with automatic refresh + a 1-minute Task
    correct Answer: option B
    explanation: >
      For near-real-time (< 1 minute) feature updates: (1) Streams capture CDC changes on source tables instantly, (2) Tasks can be scheduled as frequently as 1 minute using SCHEDULE='1 MINUTE' or triggered as children of loading tasks, (3) SYSTEM$STREAM_HAS_DATA prevents unnecessary runs. 
      Serverless Tasks avoid warehouse start/stop overhead for frequent short runs. 
    difficulty level: Hard
    topic: Snowpark
    exam: Data Engineer

  - Question No: "158"
    question: What is the recommended warehouse strategy for isolating a reporting workload from an ETL workload?
    their options:
      option A: Use one warehouse with separate schemas for each workload
      option B: Create separate virtual warehouses for reporting and ETL â€” each gets its own isolated compute resources
      option C: Use the SYSADMIN role for ETL and ANALYST role for reporting on the same warehouse
      option D: Use resource monitors to limit each workload's credit usage on a shared warehouse
    correct Answer: option B
    explanation: >
      Separate warehouses provide complete compute isolation. 
      An ETL job consuming all resources on one warehouse has zero impact on a reporting warehouse because each warehouse has its own independent compute pool. 
      This is one of Snowflake's core architectural advantages â€” unlimited virtual warehouses with no resource contention. 
    difficulty level: Easy
    topic: Architecture Design
    exam: Architect

  - Question No: "159"
    question: A company's data architecture includes Kafka for event streaming and Snowflake for analytics. What is the recommended pattern for getting Kafka events into Snowflake?
    their options:
      option A: Use Snowflake's native Kafka connector with Snowpipe or Kafka Snowflake Connector (Confluent/Snowflake)
      option B: Export Kafka events to S3 every hour and use COPY INTO
      option C: Use Snowflake's built-in Kafka listener (LISTEN TO KAFKA command)
      option D: Install Kafka inside a Snowflake virtual warehouse
    correct Answer: option A
    explanation: >
      The Snowflake Kafka Connector (available for Confluent and Apache Kafka) is the recommended integration. 
      It uses Snowflake internal stages and Snowpipe to automatically ingest Kafka topic messages into Snowflake tables with low latency. 
      There is no native LISTEN TO KAFKA command in Snowflake SQL. 
    difficulty level: Medium
    topic: Architecture Design
    exam: Architect

  - Question No: "160"
    question: An organization wants to implement a 'data mesh' architecture where multiple business domains own their own Snowflake data products. Cross-domain data access must be governed, audited, and discoverable. Which Snowflake features support this architecture?
    their options:
      option A: Multiple databases with cross-database joins for all sharing
      option B: Separate Snowflake accounts per domain (Organization), Secure Data Sharing for cross-domain access, Snowflake Data Marketplace for discovery, and Access History for auditing
      option C: A single account with separate schemas per domain, using Row Access Policies for isolation
      option D: External tables pointing to a shared data lake accessible by all domains
    correct Answer: option B
    explanation: >
      Data mesh on Snowflake maps naturally to: separate accounts per domain (Organization-level management), Secure Data Sharing for governed cross-domain data products (no copies, governed by provider), Snowflake Marketplace/Data Exchange for discovery of domain data products, and Access History/Query History for auditing cross-domain consumption. 
      This provides true data mesh autonomy with platform-level governance. 
    difficulty level: Hard
    topic: Architecture Design
    exam: Architect

  - Question No: "161"
    question: In a Snowflake medallion architecture (Bronze/Silver/Gold layers), how are layers typically implemented?
    their options:
      option A: Three separate Snowflake accounts (Bronze, Silver, Gold)
      option B: Three separate databases (or schemas) within the same Snowflake account: Bronze for raw data, Silver for cleaned/conformed, Gold for aggregated/business-ready data
      option C: Three separate warehouses that each manage their own data layer
      option D: Bronze uses external tables, Silver uses regular tables, Gold uses materialized views
    correct Answer: option B
    explanation: >
      The medallion architecture in Snowflake is typically implemented as separate databases (e.g., BRONZE_DB, SILVER_DB, GOLD_DB) or schemas (e.g., RAW, CLEAN, MART) within the same account. 
      Bronze stores raw data as-landed. 
      Silver applies cleaning and conforming. 
      Gold provides business-ready aggregations and star/snowflake schemas for BI consumption. 
      Cross-database queries connect the layers. 
    difficulty level: Medium
    topic: Architecture Design
    exam: Architect

  - Question No: "162"
    question: A table column is defined as VARIANT. What type of data can it store?
    their options:
      option A: Only JSON data
      option B: Any semi-structured data format: JSON, XML, Avro, Parquet, ORC
      option C: Any data type including structured (VARCHAR, NUMBER) and semi-structured
      option D: Only unstructured binary data
    correct Answer: option B
    explanation: >
      VARIANT is Snowflake's universal semi-structured data type. 
      It can store JSON, Avro, ORC, Parquet, and XML data natively. 
      VARIANT columns can hold up to 16 MB per value. 
      Snowflake provides dot and bracket notation for navigating VARIANT data: my_variant:address:city, my_variant['items'][0]. 
    difficulty level: Medium
    topic: Data Types
    exam: Core

  - Question No: "163"
    question: Which Snowflake data type is used to store semi-structured data like JSON?
    their options:
      option A: TEXT
      option B: JSON
      option C: VARIANT
      option D: OBJECT
    correct Answer: option C
    explanation: >
      VARIANT is the primary data type for semi-structured data. 
      OBJECT is a VARIANT subtype for JSON objects specifically. 
      ARRAY is a VARIANT subtype for arrays. 
      In practice, VARIANT is used for columns that receive JSON, Avro, Parquet, or XML data. 
    difficulty level: Easy
    topic: Data Types
    exam: Core

  - Question No: "164"
    question: A VARIANT column contains JSON: {'user': {'id': 123, 'name': 'Alice'}}. How is the user's name accessed in a SQL query?
    their options:
      option A: json_column.user.name
      option B: json_column['user']['name'] OR json_column:user:name
      option C: EXTRACT(name FROM json_column.user)
      option D: JSON_VALUE(json_column, '$.user.name')
    correct Answer: option B
    explanation: >
      Snowflake supports two equivalent notations for navigating VARIANT data: colon notation (column:key:subkey) and bracket notation (column['key']['subkey']). 
      Both access the same nested values. 
      So my_col:user:name and my_col['user']['name'] both return 'Alice'. 
      The result is a VARIANT type that can be cast: my_col:user:name::VARCHAR. 
    difficulty level: Medium
    topic: Data Types
    exam: Data Engineer

  - Question No: "165"
    question: Snowflake supports ACID transactions. What does ACID stand for?
    their options:
      option A: Authentication, Compression, Integrity, Distribution
      option B: Atomicity, Consistency, Isolation, Durability
      option C: Asynchronous, Concurrent, Independent, Distributed
      option D: Automatic, Clustered, Indexed, Deduplicated
    correct Answer: option B
    explanation: >
      ACID stands for: Atomicity (all operations in a transaction succeed or all fail), Consistency (database goes from one valid state to another), Isolation (concurrent transactions don't interfere with each other), Durability (committed data survives system failures). 
      Snowflake supports full ACID transactions including multi-statement transactions with BEGIN/COMMIT/ROLLBACK. 
    difficulty level: Medium
    topic: Transactions
    exam: Core

  - Question No: "166"
    question: Two sessions run simultaneously: Session 1 starts a transaction and INSERTs 1,000 rows but has NOT committed. Session 2 runs SELECT COUNT(*) on the same table. What count does Session 2 see?
    their options:
      option A: The count including Session 1's 1,000 uncommitted rows (dirty read)
      option B: The count excluding Session 1's uncommitted rows â€” Snowflake uses snapshot isolation, so Session 2 sees only committed data
      option C: An error because the table is locked by Session 1's transaction
      option D: It depends on the ISOLATION_LEVEL setting of Session 2's transaction
    correct Answer: option B
    explanation: >
      Snowflake uses multi-version concurrency control (MVCC) with snapshot isolation. 
      Session 2 sees the snapshot of the table as of when its transaction/statement began â€” uncommitted changes from other sessions are invisible. 
      There are no dirty reads in Snowflake. 
      Table locks don't block reads. 
      This is enabled by Snowflake's immutable micro-partition model. 
    difficulty level: Hard
    topic: Transactions
    exam: Core

  - Question No: "167"
    question: Which of the following is TRUE about Snowflake's compute layer?
    their options:
      option A: Virtual warehouses share compute resources from a common pool
      option B: Each virtual warehouse has its own dedicated compute resources (CPU, memory, SSD)
      option C: Compute is managed by the user's cloud provider (AWS/Azure/GCP) directly
      option D: Compute resources are shared between storage and processing
    correct Answer: option B
    explanation: >
      Each virtual warehouse has its own dedicated compute cluster â€” its own set of CPUs, memory, and local SSD. 
      Warehouses do not share compute resources with each other. 
      This is what enables true workload isolation: one warehouse's resource usage cannot impact another. 
    difficulty level: Easy
    topic: Architecture Overview
    exam: Core

  - Question No: "168"
    question: Snowflake charges for Cloud Services when they exceed 10% of daily compute. Which operations primarily consume Cloud Services credits?
    their options:
      option A: COPY INTO and data loading operations
      option B: SHOW commands, query compilation/optimization, metadata queries, authentication, and infrastructure management
      option C: SELECT queries and data scanning
      option D: Storage operations like compression and garbage collection
    correct Answer: option B
    explanation: >
      Cloud Services handles: authentication (every connection), query optimization/compilation (every query), metadata operations (SHOW, DESCRIBE, INFORMATION_SCHEMA queries), infrastructure management, and transaction coordination. 
      Heavy use of SHOW commands or metadata queries without much compute can push Cloud Services above the 10% threshold. 
    difficulty level: Medium
    topic: Architecture Overview
    exam: Core

  - Question No: "169"
    question: A Snowflake pipeline needs to process real-time CDC events from a relational database. What is the recommended architecture?
    their options:
      option A: COPY INTO from S3 every 5 minutes using a scheduled Task
      option B: Use a CDC tool (Debezium, Fivetran, Attunity) to capture changes from the source, publish to Kafka or S3, then use Snowflake Kafka Connector or Snowpipe to ingest into Snowflake
      option C: Connect directly from the source DB to Snowflake using the JDBC driver
      option D: Use Snowflake external tables pointing to the source database
    correct Answer: option B
    explanation: >
      Snowflake doesn't natively connect to source databases for CDC. 
      The standard pattern is: (1) CDC tool extracts changes from source DB transaction logs, (2) events published to Kafka or cloud storage, (3) Snowflake Kafka Connector or Snowpipe ingests into Snowflake. 
      This decoupled architecture handles backpressure and provides fault tolerance. 
    difficulty level: Medium
    topic: Architecture Overview
    exam: Data Engineer

  - Question No: "170"
    question: What is the relationship between warehouse size and query execution parallelism?
    their options:
      option A: Larger warehouses enable higher parallelism because they have more compute nodes â€” queries can process more data partitions simultaneously
      option B: Parallelism is fixed regardless of warehouse size â€” only query complexity affects runtime
      option C: Larger warehouses only help with memory, not parallelism
      option D: Parallelism is controlled by MAX_CONCURRENCY_LEVEL, not warehouse size
    correct Answer: option A
    explanation: >
      Virtual warehouse size directly controls parallelism. 
      An X-Small has 1 compute node. 
      A Small has 2. 
      Each size up doubles the nodes. 
      More nodes = more micro-partitions processed in parallel = faster scans, joins, and aggregations. 
      This is why sizing up helps complex, large-data queries: the work is divided across more parallel workers. 
    difficulty level: Easy
    topic: Virtual Warehouses
    exam: Core

  - Question No: "171"
    question: A company needs to comply with SOC 2 Type II. Which Snowflake edition is required?
    their options:
      option A: Standard
      option B: Enterprise
      option C: Business Critical
      option D: All editions are SOC 2 Type II compliant â€” it's a platform-level certification
    correct Answer: option D
    explanation: >
      SOC 2 Type II is a Snowflake platform-level certification that applies to ALL editions, not just specific tiers. 
      HIPAA, PCI-DSS, and FedRAMP require Business Critical (or above). 
      SOC 2, ISO 27001, and similar certifications are Snowflake-wide. 
      This is a common exam trap â€” candidates assume all compliance requires Business Critical. 
    difficulty level: Hard
    topic: Editions
    exam: Core

  - Question No: "172"
    question: A Snowflake administrator wants to ensure that all data in a specific database is encrypted with a key that can be rotated on demand. What Snowflake feature enables this?
    their options:
      option A: Automatic key rotation (built into all editions, no configuration needed)
      option B: Tri-Secret Secure with customer-managed keys (Business Critical)
      option C: TLS encryption in transit (applies to all editions automatically)
      option D: Column-level encryption using UDFs
    correct Answer: option B
    explanation: >
      Tri-Secret Secure (Business Critical) allows customers to use their own Customer Managed Key (CMK) from AWS KMS, Azure Key Vault, or GCP KMS alongside Snowflake's key for encrypting data. 
      The customer can rotate their CMK on demand. 
      Without Tri-Secret Secure, Snowflake's automatic key rotation occurs but is managed by Snowflake, not the customer. 
    difficulty level: Medium
    topic: Security
    exam: Architect

  - Question No: "173"
    question: A MERGE statement runs nightly to upsert 100K rows into a 1 billion row table. Performance degrades over time. What is the MOST likely cause and solution?
    their options:
      option A: The 100K rows overwhelm the table â€” use INSERT instead of MERGE
      option B: As the target table grows, MERGE scans more data to find matching rows. Solution: ensure the join key is a clustering key on the target table to minimize partition scans during the match phase.
      option C: MERGE is deprecated â€” use streams and tasks instead
      option D: Nightly runs cause cache invalidation â€” schedule MERGE in the morning
    correct Answer: option B
    explanation: >
      MERGE performance degrades as the target table grows because finding matching rows requires scanning more data. 
      If the join key (e.g., ORDER_ID) is a clustering key, Snowflake can prune most partitions and find matches in only the relevant micro-partitions. 
      Without clustering, every MERGE scans the entire 1 billion row table to find the 100K matching rows. 
    difficulty level: Medium
    topic: Performance
    exam: Data Engineer
`;

function autoLoadBundled() {
  try {
    const qs = parseYAML(BUNDLED_YAML_CONTENT);
    if (qs && qs.length > 0) {
      const label = 'Snowflake Architecture';
      staged.push({ name: BUNDLED_YAML_FILENAME, label, questions: qs, ok: true });
      renderStaged();
      // Auto-launch immediately
      launchApp();
    }
  } catch(e) {
    console.warn('Auto-load failed:', e);
  }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  PARSERS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
function parseMD(text) {
  const lines = text.split(/\r?\n/);
  let hi = lines.findIndex(l => /^\|.+\|/.test(l.trim()));
  if (hi === -1) return null;
  const headers = lines[hi].split('|').map(h => h.trim().toLowerCase()).filter(Boolean);
  let di = hi + 1;
  if (lines[di] && /^\|[\s\-:|]+\|$/.test(lines[di].trim())) di++;
  const rows = [];
  for (let i = di; i < lines.length; i++) {
    const ln = lines[i].trim();
    if (!ln.startsWith('|')) continue;
    const cells = splitRow(ln);
    if (cells.length < 2) continue;
    const obj = {};
    headers.forEach((h, idx) => { obj[h] = (cells[idx]||'').replace(/\\\|/g,'|').trim(); });
    rows.push(obj);
  }
  return rows.length ? rows : null;
}

function splitRow(line) {
  const inner = line.replace(/^\|/,'').replace(/\|$/,'');
  const parts = []; let cur='', i=0;
  while (i < inner.length) {
    if (inner[i]==='\\' && inner[i+1]==='|') { cur+='|'; i+=2; }
    else if (inner[i]==='|') { parts.push(cur); cur=''; i++; }
    else cur += inner[i++];
  }
  parts.push(cur);
  return parts;
}

function parseCSV(text) {
  const lines = text.replace(/\r\n|\r/g,'\n').split('\n').filter(l=>l.trim());
  if (lines.length < 2) return null;
  const pr = line => {
    const cells=[]; let cur='',inQ=false;
    for (let i=0;i<line.length;i++) {
      const c=line[i];
      if(c==='"'){if(inQ&&line[i+1]==='"'){cur+='"';i++;}else inQ=!inQ;}
      else if(c===','&&!inQ){cells.push(cur.trim());cur='';}
      else cur+=c;
    }
    cells.push(cur.trim()); return cells;
  };
  const headers = pr(lines[0]).map(h=>h.toLowerCase().trim());
  return lines.slice(1).map(l=>{
    const cells=pr(l),obj={};
    headers.forEach((h,i)=>{obj[h]=(cells[i]||'').trim();});
    return obj;
  }).filter(r=>Object.values(r).some(Boolean));
}

function norm(r, sourceName) {
  const get = (...keys) => {
    for (const k of keys) {
      const rk = Object.keys(r).find(x=>x.replace(/[\s_]/g,'').toLowerCase()===k.replace(/[\s_]/g,'').toLowerCase());
      if (rk!==undefined && r[rk]) return r[rk];
    }
    return '';
  };
  const opts = ['a','b','c','d','e'].map(l=>get(`option${l}`,`opt${l}`,`option ${l}`)).filter(Boolean);
  const ansRaw = get('answer','correct','ans');
  const ansIdx = {A:0,B:1,C:2,D:3,E:4,'1':0,'2':1,'3':2,'4':3}[(ansRaw||'').toUpperCase().trim()] ?? 0;
  return {
    question:    get('question','q') || '',
    options:     opts,
    answer:      ansIdx,
    explanation: get('explanation','explain','reason') || '',
    level:       get('level','difficulty') || 'Medium',
    exam:        get('exam','certification') || 'General',
    topic:       get('topic','category','subject') || 'General',
    source:      sourceName,   // â† which file it came from
  };
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  STAGED FILES (before launch)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
let staged = [];   // [{ name, questions[] }]

document.getElementById('fi').addEventListener('change', e => {
  if (e.target.files.length) readFiles([...e.target.files]);
});

const dz = document.getElementById('dz');
dz.addEventListener('dragover',  e => { e.preventDefault(); dz.classList.add('over'); });
dz.addEventListener('dragleave', () => dz.classList.remove('over'));
dz.addEventListener('drop', e => {
  e.preventDefault(); dz.classList.remove('over');
  if (e.dataTransfer.files.length) readFiles([...e.dataTransfer.files]);
});

function readFiles(files) {
  // Deduplicate by name â€” re-adding same file replaces it
  let pending = files.length;
  files.forEach(file => {
    const reader = new FileReader();
    reader.onload = e => {
      try {
        const text = e.target.result;
        const ext  = file.name.split('.').pop().toLowerCase();
        const raw  = ext==='csv' ? parseCSV(text) : (ext==='yml'||ext==='yaml') ? parseYAML(text) : parseMD(text);
        let qs;
        if (ext==='yml' || ext==='yaml') {
          const src = labelFromFilename(file.name);
          qs = raw ? raw.map(q=>({...q, source: src})).filter(q=>q.question&&q.options.length>=2) : [];
        } else {
          qs = raw ? raw.map(r=>norm(r, labelFromFilename(file.name))).filter(q=>q.question&&q.options.length>=2) : [];
        }
        // replace if already staged
        const idx = staged.findIndex(s=>s.name===file.name);
        if (idx>=0) staged[idx] = { name:file.name, label:labelFromFilename(file.name), questions:qs, ok:qs.length>0 };
        else staged.push({ name:file.name, label:labelFromFilename(file.name), questions:qs, ok:qs.length>0 });
      } catch(err) {
        staged.push({ name:file.name, label:file.name, questions:[], ok:false, errMsg:err.message });
      }
      if (--pending === 0) renderStaged();
    };
    reader.readAsText(file,'UTF-8');
  });
}

function labelFromFilename(name) {
  // "knowledgebase_virtual_warehouses.md" â†’ "Virtual Warehouses"
  return name
    .replace(/\.(md|csv|txt)$/i,'')
    .replace(/^knowledgebase[_\-]?/i,'')
    .replace(/[_\-]/g,' ')
    .replace(/\b\w/g,c=>c.toUpperCase())
    .trim() || name;
}

function renderStaged() {
  const list = document.getElementById('files-list');
  const rows = document.getElementById('files-rows');
  const btn  = document.getElementById('load-btn');
  if (!staged.length) { list.style.display='none'; btn.style.display='none'; return; }
  list.style.display = 'block';
  const totalQ = staged.reduce((s,f)=>s+f.questions.length,0);
  document.getElementById('staged-count').textContent = `${totalQ} questions from ${staged.length} file${staged.length>1?'s':''}`;
  rows.innerHTML = staged.map(f => `
    <div class="file-row">
      <span class="file-icon">${f.ok?'ğŸ“‹':'âš ï¸'}</span>
      <span class="file-name">${f.name}</span>
      ${f.ok
        ? `<span class="file-count">${f.questions.length} questions</span>`
        : `<span class="file-err">parse error</span>`}
    </div>`).join('');
  const hasValid = staged.some(f=>f.ok&&f.questions.length>0);
  btn.style.display = hasValid ? 'block' : 'none';
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  STATE
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
let DB=[], filtered=[], answered={}, bookmarks=new Set();
let quizQ=[], qPos=0, mode='browse', special=null;
const AF = { src:new Set(['*']), exam:new Set(['*']), level:new Set(['*']), topic:new Set(['*']) };

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  LAUNCH
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
function launchApp() {
  DB = staged.filter(f=>f.ok).flatMap(f=>f.questions);
  if (!DB.length) { document.getElementById('err').textContent='âš  No valid questions found.'; document.getElementById('err').style.display='block'; return; }
  document.getElementById('ls').classList.add('gone');
  document.getElementById('app').classList.add('on');

  // topbar file tags
  const tbf = document.getElementById('tb-files');
  const names = staged.filter(f=>f.ok).map(f=>f.label);
  const show  = names.slice(0,3);
  const rest  = names.length - show.length;
  tbf.innerHTML = show.map(n=>`<span class="tfile-tag">${n}</span>`).join('')
    + (rest>0?`<span class="tfile-tag more">+${rest} more</span>`:'');

  document.getElementById('tb-n').textContent = DB.length;
  buildChips(); applyFilters(); updateStats(); showBrowse();
}

function chgFile() {
  DB=[]; filtered=[]; answered={}; bookmarks=new Set(); staged=[];
  quizQ=[]; qPos=0; special=null;
  AF.src=new Set(['*']); AF.exam=new Set(['*']); AF.level=new Set(['*']); AF.topic=new Set(['*']);
  document.getElementById('ls').classList.remove('gone');
  document.getElementById('app').classList.remove('on');
  document.getElementById('fi').value='';
  document.getElementById('files-list').style.display='none';
  document.getElementById('load-btn').style.display='none';
  document.getElementById('err').style.display='none';
  staged=[];
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  CHIPS â€” built dynamically from data
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
function buildChips() {
  const sources = [...new Set(DB.map(q=>q.source))].sort();
  const exams   = [...new Set(DB.map(q=>q.exam))].sort();
  const levels  = ['Easy','Medium','Hard'].filter(l=>DB.some(q=>q.level===l));
  const topics  = [...new Set(DB.map(q=>q.topic))].sort();

  // source chips get purple accent to distinguish from content filters
  mkChips('c-src',   ['All',...sources], 'src',   'src');
  mkChips('c-exam',  ['All',...exams],   'exam',  'exam');
  mkChips('c-lvl',   ['All',...levels],  'level', 'lvl');
  mkChips('c-topic', ['All',...topics],  'topic', 'topic');

  // hide source block if only 1 file
  document.getElementById('sb-src').style.display = sources.length > 1 ? '' : 'none';
}

function mkChips(elId, items, key, afKey) {
  const el = document.getElementById(elId); el.innerHTML='';
  items.forEach(item => {
    const isAll=item==='All', val=isAll?'*':item;
    const cnt = isAll ? DB.length : DB.filter(q=>q[key===afKey?key:'source']===item||q[key]===item).length;
    // colour dot for difficulty
    const dot = key==='level'&&!isAll
      ? `<span class="dot" style="background:${item==='Easy'?'var(--easy)':item==='Medium'?'var(--med)':'var(--hard)'}"></span>` : '';
    const b=document.createElement('button');
    b.className='chip'+(isAll?' on':'');
    b.dataset.af=afKey; b.dataset.val=val;
    b.innerHTML=`${dot}${item} <span class="n">${cnt}</span>`;
    b.onclick=()=>toggleChip(afKey,val,b,elId);
    el.appendChild(b);
  });
}

function toggleChip(afKey, val, btn, grpId) {
  const grp=document.getElementById(grpId);
  const af=AF[afKey==='lvl'?'level':afKey];
  if (val==='*') {
    af.clear(); af.add('*');
    grp.querySelectorAll('.chip').forEach(c=>c.classList.toggle('on',c.dataset.val==='*'));
  } else {
    af.delete('*');
    af.has(val)?af.delete(val):af.add(val);
    if(!af.size) af.add('*');
    grp.querySelectorAll('.chip').forEach(c=>c.classList.toggle('on',af.has(c.dataset.val)));
  }
  special=null; document.querySelectorAll('.qf-btn').forEach(b=>b.classList.remove('on'));
  applyFilters();
}

function toggleSp(type) {
  special=special===type?null:type;
  ['bm','wr','ua'].forEach(t=>document.getElementById('qf-'+t).classList.toggle('on',special===t));
  applyFilters();
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  FILTERING
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
function applyFilters() {
  const s=(document.getElementById('srch').value||'').toLowerCase();
  filtered=DB.map((_,i)=>i).filter(i=>{
    const q=DB[i];
    if(!AF.src.has('*')  && !AF.src.has(q.source))  return false;
    if(!AF.exam.has('*') && !AF.exam.has(q.exam))    return false;
    if(!AF.level.has('*')&& !AF.level.has(q.level))  return false;
    if(!AF.topic.has('*')&& !AF.topic.has(q.topic))  return false;
    if(special==='bm'&&!bookmarks.has(i))                       return false;
    if(special==='wr'&&(!answered[i]||answered[i].ok))          return false;
    if(special==='ua'&&answered[i])                             return false;
    if(s&&!q.question.toLowerCase().includes(s)&&!q.topic.toLowerCase().includes(s)) return false;
    return true;
  });
  document.getElementById('rc').textContent=`${filtered.length} question${filtered.length!==1?'s':''}`;
  if(mode==='browse') renderBrowse();
}

function doShuffle(){filtered.sort(()=>Math.random()-.5);renderBrowse();}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  BROWSE
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
function showBrowse(){
  mode='browse';
  document.getElementById('qv').classList.remove('on');
  document.getElementById('bv').classList.remove('hide');
  renderBrowse();
}

function renderBrowse(){
  const el=document.getElementById('ql');
  if(!filtered.length){
    el.innerHTML='<div class="empty"><div class="ico">â„ï¸</div><h3>No questions match</h3><p>Adjust your filters or search term.</p></div>';
    return;
  }
  const lc=l=>l==='Easy'?'be':l==='Medium'?'bm':'bh';
  el.innerHTML=filtered.map((qi,pos)=>{
    const q=DB[qi],a=answered[qi],bm=bookmarks.has(qi);
    return`<div class="qcard${a?(a.ok?' ok':' miss'):''}" onclick="startAt(${pos})">
      <span class="qcard-num">#${qi+1}</span>
      <div class="qcard-meta">
        <span class="badge bsrc">${q.source}</span>
        <span class="badge bx">${q.exam}</span>
        <span class="badge ${lc(q.level)}">${q.level}</span>
        <span class="badge bt">${q.topic}</span>
        ${a?`<span class="badge ${a.ok?'bok':'bno'}">${a.ok?'âœ“ Correct':'âœ— Wrong'}</span>`:''}
        ${bm?`<span class="badge" style="background:var(--med-bg);border:1px solid var(--med-dim);color:var(--med)">â˜…</span>`:''}
      </div>
      <div class="qcard-text">${q.question}</div>
    </div>`;
  }).join('');
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  QUIZ
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
function startQuiz(){if(!filtered.length)return;quizQ=[...filtered];qPos=0;enterQuiz();}
function startAt(pos){quizQ=[...filtered];qPos=pos;enterQuiz();}
function enterQuiz(){mode='quiz';document.getElementById('qv').classList.add('on');document.getElementById('bv').classList.add('hide');renderQ();}

function renderQ(){
  const qi=quizQ[qPos],q=DB[qi],a=answered[qi],tot=quizQ.length;
  document.getElementById('qfill').style.width=((qPos+1)/tot*100)+'%';
  document.getElementById('qcnt').textContent=`${qPos+1} / ${tot}`;
  const lc=q.level==='Easy'?'be':q.level==='Medium'?'bm':'bh';
  document.getElementById('qmeta').innerHTML=
    `<span class="badge bsrc">${q.source}</span>
     <span class="badge bx">${q.exam}</span>
     <span class="badge ${lc}">${q.level}</span>
     <span class="badge bt">${q.topic}</span>`;
  document.getElementById('qidx').textContent=`Question #${qi+1} of ${DB.length}`;
  document.getElementById('qtxt').textContent=q.question;
  const L=['A','B','C','D','E'];
  document.getElementById('qopts').innerHTML=q.options.map((o,i)=>{
    let c='';if(a){if(i===q.answer)c='cor';else if(i===a.chosen)c='wrg';}
    return`<button class="opt ${c}" onclick="pick(${i})" ${a?'disabled':''}><span class="ol">${L[i]}</span><span>${o}</span></button>`;
  }).join('');
  document.getElementById('qexpl').innerHTML=a?`<div class="expl"><strong>Explanation: </strong>${q.explanation}</div>`:'';
  const bm=bookmarks.has(qi);
  document.getElementById('bmbtn').textContent=bm?'â˜…':'â˜†';
  document.getElementById('bmbtn').classList.toggle('on',bm);
  document.getElementById('btn-p').disabled=qPos===0;
  const last=qPos===tot-1;
  document.getElementById('btn-n').style.display=last?'none':'';
  document.getElementById('btn-f').style.display=last?'':'none';
}

function pick(chosen){
  const qi=quizQ[qPos];if(answered[qi])return;
  answered[qi]={ok:chosen===DB[qi].answer,chosen};
  updateStats();renderQ();
}
function qNext(){if(qPos<quizQ.length-1){qPos++;renderQ();}}
function qPrev(){if(qPos>0){qPos--;renderQ();}}
function toggleBm(){const qi=quizQ[qPos];bookmarks.has(qi)?bookmarks.delete(qi):bookmarks.add(qi);updateStats();renderQ();}

function finishQuiz(){
  const qs=quizQ.filter(i=>answered[i]);
  const c=qs.filter(i=>answered[i].ok).length,w=qs.length-c;
  const p=qs.length?Math.round(c/qs.length*100):0;
  document.getElementById('m-c').textContent=c;
  document.getElementById('m-w').textContent=w;
  document.getElementById('m-p').textContent=p+'%';
  const col=p>=80?'var(--easy)':p>=60?'var(--med)':'var(--hard)';
  const r=document.getElementById('mring');r.style.stroke=col;
  setTimeout(()=>{r.style.strokeDashoffset=289.03*(1-p/100);},50);
  document.getElementById('m-t').textContent=p>=80?'ğŸ‰ Excellent!':p>=60?'ğŸ“š Good Effort!':'ğŸ’ª Keep Going!';
  document.getElementById('modal').classList.add('on');
}
function closeModal(){document.getElementById('modal').classList.remove('on');}
function revWrong(){closeModal();special='wr';document.getElementById('qf-wr').classList.add('on');applyFilters();showBrowse();}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//  STATS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
function updateStats(){
  const keys=Object.keys(answered),tot=DB.length;
  const c=keys.filter(k=>answered[k].ok).length,w=keys.length-c;
  const p=keys.length?Math.round(c/keys.length*100):null,acc=p!==null?p+'%':'â€”';
  document.getElementById('tb-a').textContent=keys.length;
  document.getElementById('tb-p').textContent=acc;
  document.getElementById('sc-c').textContent=c;
  document.getElementById('sc-w').textContent=w;
  document.getElementById('sc-s').textContent=tot-keys.length;
  document.getElementById('sc-a').textContent=acc;
  const prog=tot?keys.length/tot:0;
  document.getElementById('ring').style.strokeDashoffset=238.76*(1-prog);
  document.getElementById('rpct').textContent=Math.round(prog*100)+'%';
  document.getElementById('rlbl').textContent=`${keys.length} of ${tot}`;
  document.getElementById('n-bm').textContent=bookmarks.size;
  document.getElementById('n-wr').textContent=w;
  document.getElementById('n-ua').textContent=tot-keys.length;
}

function doReset(){if(!confirm('Reset all progress?'))return;answered={};bookmarks=new Set();updateStats();applyFilters();}

// Auto-load bundled YAML on page start
document.addEventListener('DOMContentLoaded', autoLoadBundled);

document.addEventListener('keydown',e=>{
  if(mode!=='quiz')return;
  if(e.key==='ArrowRight')qNext();
  if(e.key==='ArrowLeft') qPrev();
  if(['1','2','3','4'].includes(e.key)){const qi=quizQ[qPos];if(!answered[qi])pick(+e.key-1);}
  if(e.key.toLowerCase()==='b')toggleBm();
});
</script>
</body>
</html>
