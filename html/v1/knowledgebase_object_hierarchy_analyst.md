# Snowflake Object Hierarchy — Advanced Analyst

> **Exam:** Advanced Analyst (AAA-C01)  
> **Topics:** Tables (Permanent, Temporary, Transient, Iceberg, External, Dynamic) · Views (Standard, Materialized, Secure)  
> **Total Questions:** 23

---

| No | Question | Option A | Option B | Option C | Option D | Answer | Explanation | Level | Exam | Topic |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | An analyst accidentally runs UPDATE customers SET email = NULL WHERE 1=1. The transaction committed 2 hours ago. The table has 7-day Time Travel. How can the analyst recover the original emails? | ROLLBACK — undoes committed transactions | CREATE TABLE recovery AS SELECT email FROM customers AT (OFFSET => -7200) — then update the production table from recovery | Contact Snowflake Support — committed changes cannot be reversed | Run UNDROP TABLE customers | B | Time Travel (AT OFFSET => -7200 seconds = 2 hours ago) returns the table as it was before the UPDATE. The analyst can: (1) SELECT the original emails from AT(...), (2) update the production table using this recovered data. The original UPDATE was committed so ROLLBACK doesn't work — only Time Travel enables this recovery. | Medium | Data Analyst | Permanent Tables |
| 2 | An analyst queries a permanent table and sees data from yesterday, not today. Queries on the same table from a colleague return today's data. What is the most likely cause? | The analyst's warehouse is smaller — less data is loaded | The analyst accidentally has a Time Travel AT clause in a saved query or BI tool configuration | The analyst's role has data masking applied to today's data | Different warehouses return different data | B | A saved Time Travel query (AT TIMESTAMP or OFFSET) in a BI tool or query template is the most common cause of analysts seeing stale data while others see current data. All warehouses serve the same consistent data — the issue is a Time Travel qualifier in the query definition. | Hard | Data Analyst | Permanent Tables |
| 3 | What does SHOW TABLES display about a permanent table that helps an analyst understand its recovery capabilities? | The table's last backup date | retention_time column shows Time Travel days; created_on shows creation date | SHOW TABLES does not show recovery information | The Fail-Safe expiry date | B | SHOW TABLES includes a retention_time column showing the current DATA_RETENTION_TIME_IN_DAYS value for each table. This tells analysts how many days of Time Travel recovery are available. Combined with the dropped_on column (for dropped tables in SHOW TABLES HISTORY), analysts can determine recovery feasibility. | Medium | Data Analyst | Permanent Tables |
| 4 | An analyst creates a temporary table in session A to experiment with a dataset subset. They open a new Snowflake browser tab (session B) and try to query the temp table. What do they see? | The same temporary table — Snowflake shares temp tables across browser tabs for the same user | Table not found — each browser tab is a separate session with its own isolated temporary table namespace | A read-only version of the temp table | The temp table but with no data — empty clone | B | Each browser tab in Snowflake (Snowsight) creates a separate session. Temporary tables are session-isolated — session B cannot see session A's temporary tables. This is true even for the same user. The analyst must recreate the temp table in session B or use a transient/permanent table for cross-session sharing. | Easy | Data Analyst | Temporary Tables |
| 5 | An analyst wants to create a working copy of a large table to experiment with data transformations without affecting production. The copy only needs to last for their current analysis session. What is the most cost-efficient approach? | CREATE TABLE analysis_copy AS SELECT * FROM prod_table — full copy | CREATE TEMPORARY TABLE analysis_copy CLONE prod_table — zero-copy clone, session-scoped, no Fail-Safe overhead | SELECT * INTO #analysis_copy FROM prod_table — temp table shorthand | CREATE TRANSIENT TABLE analysis_copy CLONE prod_table | B | CREATE TEMPORARY TABLE ... CLONE is optimal: zero-copy (no immediate storage cost), session-scoped (auto-drops when session ends, no cleanup needed), and no Fail-Safe overhead. CTAS creates a full data copy. Transient clone would persist after the session ends requiring manual cleanup. | Medium | Data Analyst | Temporary Tables |
| 6 | An analyst's team creates many ad-hoc analysis tables in a shared schema. After 6 months, storage costs are high due to Fail-Safe overhead. What table type change would reduce costs? | Switch to temporary tables — they have no storage cost | Use transient tables for ad-hoc analysis — no Fail-Safe, same SQL interface as permanent tables | Use external tables — data stays in S3 with no Snowflake storage cost | Reduce DATA_RETENTION_TIME_IN_DAYS to 0 on all permanent tables | B | Ad-hoc analysis tables rarely need Fail-Safe recovery (they can be recreated from source data). Switching to transient tables eliminates 7-day Fail-Safe storage overhead while providing the same SQL functionality. This is a common cost-reduction recommendation for analytics teams with many short-lived analysis tables. | Easy | Data Analyst | Transient Tables |
| 7 | An analyst drops a transient table that was created 6 hours ago (DATA_RETENTION=1 day). Can it be recovered? | No — transient tables cannot be recovered once dropped | Yes — UNDROP TABLE works within the 1-day Time Travel window | Yes — Snowflake Support can recover via Fail-Safe | Yes — restore from the 7-day Fail-Safe | B | Transient tables DO support Time Travel (up to 1 day). UNDROP TABLE or CREATE TABLE recovery CLONE dropped_table BEFORE (STATEMENT => drop_query_id) will work within the retention window. The critical difference from permanent tables: once the 1-day Time Travel window expires, there is NO Fail-Safe fallback for recovery. | Medium | Data Analyst | Transient Tables |
| 8 | An analyst queries an Iceberg table in Snowflake. Can they use standard SQL SELECT statements? | No — Iceberg tables require special Iceberg SQL syntax | Yes — Snowflake Iceberg tables support standard SQL SELECT, with full filter pushdown and Snowflake query optimisation | Only basic SELECT with no JOINs — Iceberg tables have SQL restrictions | Only if the Iceberg table is in Snowflake-as-catalog mode | B | From an analyst's perspective, Iceberg tables in Snowflake look and behave identically to native tables — standard SQL SELECT, JOINs, aggregations, window functions all work. The Iceberg format is transparent to the analyst. Snowflake translates SQL operations to Iceberg file reads internally. | Easy | Data Analyst | Iceberg Tables |
| 9 | An analyst needs to query data from an Iceberg table that was modified by a Spark job 30 minutes ago. The Iceberg table uses an external catalog (AWS Glue). They run a query but get old data. What is needed? | Wait 1 hour for Snowflake to auto-sync with Glue | Ask the data engineer to run ALTER ICEBERG TABLE ... REFRESH to sync Snowflake's metadata with the latest Glue catalog snapshot | The analyst must query the Parquet files directly in S3 | Add a filter WHERE snapshot_id = (SELECT MAX(snapshot_id) FROM ...) | B | For external-catalog Iceberg tables, Snowflake does not auto-detect changes from other engines. A manual REFRESH (or AUTO_REFRESH if configured) is required to sync with new snapshots added by Spark. Without refresh, Snowflake serves the data as of the last refresh point, not the latest Spark write. | Medium | Data Analyst | Iceberg Tables |
| 10 | What is the primary reason an analyst might encounter an Iceberg table in Snowflake rather than a native permanent table? | Iceberg tables have better query performance than native tables | The company uses a multi-engine data platform (Spark, Snowflake, Trino) and needs data accessible to all engines without duplication or ETL | Iceberg tables are cheaper to store in Snowflake | Iceberg tables have longer Time Travel (unlimited snapshots) than permanent tables | B | The primary reason for Iceberg in a Snowflake environment is multi-engine data access. Data engineering teams may write data via Spark (for ML pipelines) and expose the same data to analysts via Snowflake — using Iceberg as the interoperability layer avoids duplicating data. Analysts see it as a regular table. | Medium | Data Analyst | Iceberg Tables |
| 11 | An analyst queries an external table and notices queries are very slow compared to native tables. What is the most likely technical reason? | External tables use a slower SQL engine | External tables lack Snowflake's micro-partition statistics and columnar caching — every query reads raw files from object storage with full file scans | The analyst's warehouse is too small for external table queries | External tables have row-level security that slows queries | B | External tables trade Snowflake's storage optimisations (micro-partition statistics, local disk cache, columnar pruning within files) for keeping data in external storage. Without these optimisations, queries must read more raw data from S3/Azure/GCS. For frequently-queried data, loading into native tables dramatically improves performance. | Medium | Data Analyst | External Tables |
| 12 | An analyst wants to run aggregations on an external table with 10TB of Parquet files in S3. The team does not want to load the data into Snowflake due to storage costs. What can be done to improve query performance? | Nothing — external tables always have the same performance | Ask the data engineer to create a Materialized View on the external table — pre-computed aggregations stored in Snowflake, queries read from MV not S3 | Convert to an Iceberg table — Iceberg is faster than external tables | Increase the warehouse size — more compute overcomes external table limitations | B | Materialized Views on external tables is a powerful pattern: aggregation results are pre-computed and stored in Snowflake native storage (fast), while the source data remains in S3 (no duplication cost). The MV auto-maintains as new files are detected by AUTO_REFRESH. Analysts query the MV at native speed. | Hard | Data Analyst | External Tables |
| 13 | An analyst is told their report table is a 'Dynamic Table' with TARGET_LAG='15 minutes'. What does this mean for their analysis? | The table schema changes dynamically based on query patterns | The table's data is refreshed automatically and is at most 15 minutes behind the source data — no manual refresh needed | The table is a temporary copy that expires after 15 minutes | The table refreshes only when queried — lazy evaluation | B | TARGET_LAG='15 minutes' means the Dynamic Table is automatically refreshed and guaranteed to be within 15 minutes of the source data. Analysts can rely on the data being near-real-time without any manual intervention. This is the key characteristic that makes Dynamic Tables different from Materialized Views (which also auto-refresh but have SQL restrictions). | Easy | Data Analyst | Dynamic Tables |
| 14 | An analyst queries a Dynamic Table and gets results immediately. Can they trust the data is fully up to date? | Yes — Dynamic Tables always contain real-time data | Data is at most TARGET_LAG stale — the analyst should check SHOW DYNAMIC TABLES to see data_timestamp (last successful refresh time) and compare to their needs | Dynamic Tables cache results for 1 hour — data may be older than TARGET_LAG | Dynamic Tables require manual refresh before each query for accuracy | B | Dynamic Tables are near-real-time, not real-time. The maximum staleness is TARGET_LAG, but the actual freshness depends on the last successful refresh. SHOW DYNAMIC TABLES includes data_timestamp showing when data was last refreshed. For time-sensitive analysis, checking this timestamp contextualises the data's currency. | Medium | Data Analyst | Dynamic Tables |
| 15 | An analyst is granted SELECT on a view called SALES_SUMMARY but not on the underlying ORDERS table. Can they query SALES_SUMMARY? | No — SELECT on underlying tables is always required | Yes — SELECT on the view is sufficient; Snowflake uses the view owner's access rights to query the underlying table | Only if they have USAGE on the schema | Yes but only for SELECT * — specific column queries require table access | B | Snowflake views provide privilege isolation via owner's rights — the view owner's access to underlying tables is used when a user queries the view. The analyst with SELECT on the view can query it without needing direct table access. This is the fundamental access-control use case for views. | Easy | Data Analyst | Standard Views |
| 16 | An analyst notices that running SELECT * FROM sales_view returns different columns than SELECT * FROM sales_table. What explains the difference? | The view is outdated — it was created from an older version of the table | The view selects specific columns from the table (column subsetting) — it may hide sensitive or irrelevant columns from the analyst | Views always add extra metadata columns | The view applies masking to all columns, changing their names | B | Views can expose a subset of columns from the base table. The view may exclude sensitive columns (like salary, SSN) or add computed columns (derived metrics). The analyst sees only what the view exposes — not the full table definition. This is a common data access control pattern. | Easy | Data Analyst | Standard Views |
| 17 | An analyst wants to understand why a view returns no data for their region despite knowing data exists. What is the most likely cause? | The view has a caching issue — clear cache and retry | The view includes a WHERE clause filtering by region, and the analyst's current role or session variable does not match the region filter condition | The analyst's warehouse is too small to return all rows | Views only return data inserted after their creation date | B | Dynamic WHERE clauses in views (e.g., WHERE region = CURRENT_ROLE() or filtering based on a lookup table) can return zero rows if the current session context doesn't match. The analyst should check the view definition (DESCRIBE VIEW or SHOW VIEWS if not secure) to understand the filter logic applied. | Medium | Data Analyst | Standard Views |
| 18 | An analyst's dashboard query suddenly becomes much faster without any changes to the query or warehouse. A materialized view was recently created by the data team. What happened? | The data team increased the warehouse size | Snowflake's optimizer transparently used the materialized view to answer the dashboard query — no query changes needed | The result cache was pre-warmed by the data team | The dashboard tool is caching results locally | B | Snowflake can transparently rewrite queries on a base table to use a relevant materialized view without the analyst changing anything. If the MV pre-computes the aggregations needed by the dashboard query, the optimizer uses the MV result — dramatically reducing query time. This is automatic and invisible to the analyst. | Easy | Data Analyst | Materialized Views |
| 19 | An analyst queries a Materialized View during a period when the base table is being heavily loaded. Is the MV data consistent? | No — MVs become inconsistent during heavy loads | Yes — Snowflake ensures MVs always reflect committed data; they are transactionally consistent with the base table | Only if the analyst uses AT (STATEMENT => ...) | MVs are locked during base table DML — the query queues until loading finishes | B | Snowflake guarantees transactional consistency for materialized views. A query on an MV always returns data consistent with committed transactions on the base table. The MV maintenance happens asynchronously but never serves partially-applied transaction data. Analysts can rely on MV data being fully consistent. | Medium | Data Analyst | Materialized Views |
| 20 | An analyst with SELECT on a secure view runs DESCRIBE VIEW to understand the view's SQL logic. What do they see? | The full SQL definition — DESCRIBE works regardless of view type | The column names and types but NOT the SQL definition — the definition is hidden for secure views | An error — DESCRIBE is not permitted on secure views | The SQL definition but with sensitive column names redacted | B | For secure views, DESCRIBE VIEW shows column metadata (names, types, nullability) but hides the SQL definition. Non-owners cannot reverse-engineer the view's logic. This protects business rules, calculation methodology, and knowledge of underlying table structures from view consumers. | Easy | Data Analyst | Secure Views |
| 21 | An analyst is consuming data from a data provider via Snowflake Data Sharing. The shared object is a secure view. The analyst tries to look at the view definition. What can they see? | Full definition — Data Sharing consumers have elevated privileges | Column names and types only — the definition is hidden; they cannot see the provider's underlying tables, SQL logic, or join conditions | Nothing — shared secure views return no metadata | The definition but only if they have IMPORTED PRIVILEGES grant | B | Data Sharing via secure views is specifically designed to hide the provider's data model. The consumer sees column names and types (necessary to write queries) but cannot see the underlying SQL, the provider's table names, or the business logic. This is the standard data sharing governance model. | Medium | Data Analyst | Secure Views |
| 22 | Why might an analyst notice that identical queries against a secure view take longer than against a standard view with similar logic? | Secure views use a different storage format | Secure views disable query optimisations to prevent data inference attacks — filter push-down and short-circuit evaluation are limited, causing more data to be scanned | Secure views always read from the slowest storage tier | The analyst's role has lower compute priority for secure views | B | Secure views intentionally disable certain optimisations to prevent analysts from inferring filtered data through query execution time differences. For example, a WHERE clause that eliminates 99% of rows still requires more scanning in a secure view context. This performance difference is expected and is the security-performance trade-off of secure views. | Hard | Data Analyst | Secure Views |
| 23 | An analyst needs to share their analysis results with a business partner who has a Snowflake account. The results contain confidential calculation logic. What should the analyst ask their Snowflake admin to create? | A permanent table with the results — grant SELECT to the partner | A secure view over the results table — add to an outbound share for the partner's account | A standard view — shares automatically hide view definitions | Export to CSV and share via email | B | The correct pattern: create a secure view exposing the results (hiding the calculation logic), add the secure view to a Snowflake Data Share, grant the share to the partner's account. The partner queries the secure view — seeing results without understanding the methodology. Standard views in shares expose their definitions to consumers. | Medium | Data Analyst | Secure Views |

---
*23 questions · Object Hierarchy: Tables & Views · Advanced Analyst (AAA-C01)*