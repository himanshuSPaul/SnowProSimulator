# Snowflake Object Hierarchy — Advanced Architect

> **Exam:** Advanced Architect (ADA-C01)  
> **Topics:** Tables (Permanent, Temporary, Transient, Iceberg, External, Dynamic) · Views (Standard, Materialized, Secure)  
> **Total Questions:** 35

---

| No | Question | Option A | Option B | Option C | Option D | Answer | Explanation | Level | Exam | Topic |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | An architect needs to design a table strategy for a financial system with: production data needing 90-day audit trail, a dev copy needing 1-day recovery, and a staging table needing no long-term recovery. What table types should be used respectively? | All permanent tables with different retention values | Permanent (90-day TT), Transient (1-day TT), Transient (0-day TT) | Permanent, Temporary, Temporary | All transient to reduce storage costs | B | Permanent with 90-day Time Travel for production. Transient with 1-day for dev (no Fail-Safe overhead). Transient with 0-day retention for staging (maximum cost efficiency, no recovery needed). Temporary would auto-drop when session ends — not suitable for persistent dev tables. | Hard | Architect | Permanent Tables |
| 2 | An architect observes that a 10TB permanent table with DATA_RETENTION=90 is costing 3x its raw data size in storage. What is the cause? | Snowflake over-provisions storage for large tables | 90-day Time Travel retains old micro-partitions for 90 days plus 7-day Fail-Safe — a high-churn table accumulates many historical micro-partition versions | Permanent tables have a 3x storage multiplier by design | The clustering key is causing data triplication | B | High-churn tables with long Time Travel retention accumulate many micro-partition versions. Each UPDATE or DELETE creates new micro-partitions while retaining the old ones for the retention window. 90-day retention on a daily-updated table can mean 90x copies of changed partitions. The architect should evaluate whether all 90 days of retention are truly needed. | Hard | Architect | Permanent Tables |
| 3 | What is the correct DDL pattern to perform a safe table migration in Snowflake without downtime? | Drop old table, create new table with new schema | CREATE TABLE new_t AS SELECT ... from old_t; validate; ALTER TABLE old_t RENAME TO old_t_backup; ALTER TABLE new_t RENAME TO old_t | Use TRUNCATE TABLE and reload | Directly ALTER TABLE to modify column types | B | The zero-downtime migration pattern: CTAS creates new_t with new schema from old_t data. After validation, atomic RENAME operations swap names. Applications transparently connect to the new schema. The old_t_backup clone is retained for rollback. Snowflake RENAME is atomic within a schema. | Hard | Architect | Permanent Tables |
| 4 | An architect needs to ensure that dropping a critical production table is impossible without explicit approval. What mechanism achieves this? | Set DATA_RETENTION_TIME_IN_DAYS=90 — prevents dropping | Revoke DROP privilege from all non-ACCOUNTADMIN roles on the table | Enable Time Lock through Business Critical Edition | Create a Resource Monitor on the table schema | B | Revoking DROP TABLE privilege on critical tables from all roles except designated DBA roles prevents accidental drops. Even with OWNERSHIP, DROP can be revoked via REVOKE OWNERSHIP strategies or strict RBAC. Time Travel/Fail-Safe allows recovery but doesn't prevent the drop. | Hard | Architect | Permanent Tables |
| 5 | A permanent table uses CLUSTER BY (customer_id, order_date). Queries always filter on order_date only. What is the architecture recommendation? | The current clustering is fine — multi-column keys always help | Change the cluster key to CLUSTER BY (order_date) — the leading column must match the most common filter predicate for effective pruning | Add a second clustering key for customer_id separately | Remove clustering entirely — it adds overhead without benefit here | B | Micro-partition pruning uses the CLUSTER BY key's leading column first. If queries filter only on order_date but it is the second column, pruning is ineffective. Reordering to CLUSTER BY (order_date) (or CLUSTER BY (order_date, customer_id)) ensures Snowflake can skip micro-partitions based on order_date predicates. | Hard | Architect | Permanent Tables |
| 6 | An ETL pipeline uses temporary tables for intermediate joins across 5 transformation steps. The pipeline runs in a single session. What is the storage lifecycle of these temp tables? | Temp tables persist for 24 hours after session end — cleanup job needed | Temp tables are automatically dropped when the session ends — no cleanup required | Temp tables persist until explicitly dropped or the warehouse suspends | Temp tables are dropped at each COMMIT within the session | B | Temporary tables live exactly as long as the session. When the ETL session terminates (successfully or on failure), all temporary tables created within it are automatically dropped. This eliminates the need for explicit cleanup code in ETL pipelines — a significant operational simplification. | Medium | Architect | Temporary Tables |
| 7 | An architect designs a pipeline where a stored procedure creates a temporary table, populates it, and calls another stored procedure that reads from it. Will the second stored procedure see the temporary table? | No — each stored procedure has its own session scope | Yes — temporary tables are session-scoped, and stored procedures execute within the caller's session (when called with EXECUTE AS CALLER) | Only if the stored procedures are in the same schema | Yes, but only if the temporary table was created with GLOBAL scope | B | Stored procedures called with EXECUTE AS CALLER run in the caller's session context and can access temporary tables created in that session. If EXECUTE AS OWNER is used, the procedure runs in the owner's session context and cannot see the caller's temporary tables — an important design consideration. | Hard | Architect | Temporary Tables |
| 8 | Why might an architect choose a transient table over a temporary table for a staging layer that persists between pipeline runs? | Transient tables support clustering; temporary tables do not | Transient tables persist between sessions — a staging table shared across multiple pipeline sessions needs to survive session boundaries; temporary tables would be dropped | Transient tables have better query performance | Temporary tables cannot be used in COPY INTO statements | B | Temporary tables are session-scoped — if the staging table must be visible to multiple sessions or persist between pipeline runs, a transient table is required. Transient tables persist until explicitly dropped (like permanent tables) but without Fail-Safe overhead. | Medium | Architect | Temporary Tables |
| 9 | A data lake architecture stores 100TB of raw event data that is reloaded from S3 every 24 hours. The only recovery requirement is within the same day. What table strategy minimises total storage cost? | Permanent tables with 1-day retention | Transient tables with 1-day retention — no Fail-Safe, 1-day TT for same-day recovery | External tables pointing to S3 directly | Temporary tables refreshed each session | B | Transient with 1-day retention: live data + 1-day Time Travel versions, NO 7-day Fail-Safe. For data reloaded from S3 (external source of truth), Fail-Safe is redundant — S3 is the backup. Transient tables eliminate the 7-day Fail-Safe overhead, potentially saving significant storage cost at 100TB scale. | Hard | Architect | Transient Tables |
| 10 | An architect receives a requirement: all objects in the DEV environment must have no Fail-Safe to reduce costs. What is the most efficient implementation? | Run ALTER TABLE SET NO_FAILSAFE on every table after creation | Create all schemas as TRANSIENT — all tables created within inherit transient properties | Set FAILSAFE=FALSE at the account level for the dev account | Create a Resource Monitor that suspends warehouses when Fail-Safe triggers | B | CREATE TRANSIENT SCHEMA (or CREATE TRANSIENT DATABASE) ensures all tables created within are automatically transient — no Fail-Safe on any object without requiring per-table configuration. This is the scalable architectural approach for cost-optimised dev environments. | Hard | Architect | Transient Tables |
| 11 | What is the storage cost implication when an architect clones a permanent production table as transient for a data science sandbox? | Full duplication cost — transient clones are not zero-copy | Zero-copy at clone creation; subsequent storage cost is only for data modified in the transient clone — no Fail-Safe overhead on the clone itself | Transient clones cannot be created from permanent tables | 2x storage — clone has transient overhead despite zero-copy creation | B | CREATE TRANSIENT TABLE ds_sandbox CLONE prod_table: zero-copy at creation (shares micro-partitions with prod). Modified data in the clone creates new micro-partitions — storage charged only for changes. No Fail-Safe on the clone (it's transient). This is a cost-efficient pattern for creating data science sandboxes. | Hard | Architect | Transient Tables |
| 12 | A transient table has DATA_RETENTION_TIME_IN_DAYS=1. A critical row is deleted 23 hours ago. What is the recovery window remaining? | No recovery — transient tables have no recovery options | 1 hour of Time Travel window remains — use INSERT INTO ... SELECT FROM table AT (OFFSET => -82800) | 7 days of Fail-Safe begin after Time Travel expires | Recovery requires contacting Snowflake Support | B | With 1-day retention and deletion 23 hours ago, 1 hour of Time Travel remains. Use Time Travel (AT OFFSET => -82800 seconds, or AT TIMESTAMP) to query and recover the deleted row. There is NO Fail-Safe fallback — once the 1-day window expires, the data is permanently unrecoverable. | Hard | Architect | Transient Tables |
| 13 | An architect designs a multi-engine data platform: Snowflake for SQL analytics, Apache Spark for ML training, and Trino for ad-hoc queries — all on the same data. What table format and catalog strategy achieves this? | Native Snowflake tables shared to other engines via JDBC | Apache Iceberg tables with an external catalog (AWS Glue or Polaris) as the shared metadata layer — all engines read/write through the same catalog | Snowflake external tables with Parquet format — other engines read the same S3 files | Delta Lake format with Delta sharing protocol | B | The multi-engine lakehouse pattern: Iceberg as the open table format ensures interoperability. An external catalog (Glue, Polaris) acts as the single metadata authority — Snowflake, Spark, and Trino all register reads/writes through it. This prevents metadata divergence and enables true multi-engine data sharing. | Hard | Architect | Iceberg Tables |
| 14 | An Iceberg table uses Snowflake as catalog. The table receives 1 million row updates daily. What is the implication for Iceberg metadata files? | Iceberg metadata is stored in Snowflake — no external metadata file growth | Each transaction creates new Iceberg snapshot, manifest list, and manifest files in external storage — over time, metadata file accumulation requires compaction | Snowflake compresses all Iceberg metadata into a single file daily | Iceberg metadata is automatically archived after 7 days | B | Iceberg's architecture creates new metadata files (snapshots, manifests) for each transaction. High-frequency DML generates many small metadata files over time, degrading query planning performance. Regular metadata compaction (snapshot expiry, orphan file removal) is required for operational health of high-churn Iceberg tables. | Hard | Architect | Iceberg Tables |
| 15 | An architect evaluates Iceberg tables vs permanent tables for a workload that: runs exclusively in Snowflake, has no multi-engine requirement, needs maximum query performance, and requires 90-day Time Travel. What should the architect recommend? | Iceberg tables — they are always better than permanent tables | Permanent tables — they provide better Snowflake-native performance (micro-partition statistics, clustering, caching) without the external storage overhead of Iceberg | Iceberg tables — only Iceberg supports 90-day Time Travel in Snowflake | External tables with Parquet format — equivalent to Iceberg but simpler | B | If the workload is Snowflake-exclusive, permanent tables provide better performance (native micro-partitioning, columnar caching, seamless clustering). Iceberg's value is multi-engine interoperability. Adding Iceberg overhead (external storage, metadata management) for a Snowflake-only workload adds complexity without benefit. | Hard | Architect | Iceberg Tables |
| 16 | What is an EXTERNAL VOLUME and what minimum privileges are required to create an Iceberg table on it? | An EXTERNAL VOLUME is a warehouse type — CREATE WAREHOUSE privilege required | EXTERNAL VOLUME is a Snowflake object defining external storage location — CREATE ICEBERG TABLE on the schema and USAGE on the external volume are required | CREATE STAGE privilege on the database is sufficient | ACCOUNTADMIN role is always required to use external volumes | B | To create an Iceberg table: USAGE privilege on the external volume (to use the defined storage location) plus CREATE ICEBERG TABLE privilege on the target schema. ACCOUNTADMIN creates the external volume (requires cloud IAM setup) but is not required for table creation by other roles once the volume is shared. | Hard | Architect | Iceberg Tables |
| 17 | What is Snowflake Polaris and how does it relate to Iceberg tables? | Polaris is Snowflake's proprietary table format competing with Iceberg | Polaris (Snowflake Open Catalog) is an Apache Iceberg REST catalog — open-source catalog that any Iceberg-compatible engine (Snowflake, Spark, Flink) can use as the shared metadata service | Polaris is a security layer for Iceberg tables requiring Business Critical Edition | Polaris is a Snowflake UI for managing Iceberg table schemas | B | Snowflake Polaris (now Apache Polaris, open-sourced) is an Apache Iceberg REST catalog specification implementation. It serves as an open, interoperable catalog that multiple engines can use. Snowflake can use Polaris as an external catalog for Iceberg tables, enabling true multi-engine lakehouse architectures. | Hard | Architect | Iceberg Tables |
| 18 | An Iceberg table on Snowflake uses AWS Glue as the external catalog. A Spark job writes new data to the table. What must happen before Snowflake can query the new data? | Nothing — Snowflake auto-detects changes via S3 event notifications | ALTER ICEBERG TABLE ... REFRESH must be run to sync Snowflake's view of the Glue catalog metadata | Snowflake automatically polls Glue every 5 minutes for changes | The Iceberg table must be dropped and recreated after each Spark write | B | For external catalog Iceberg tables, Snowflake does not automatically detect catalog changes made by other engines. ALTER ICEBERG TABLE table_name REFRESH synchronises Snowflake's metadata cache with the latest Glue catalog state. Without refresh, Snowflake queries the pre-Spark-write snapshot. | Hard | Architect | Iceberg Tables |
| 19 | A company stores financial data in Iceberg tables with Snowflake as catalog. They need column-level masking for analysts. Is this possible? | No — Iceberg tables do not support Snowflake governance features | Yes — Snowflake Dynamic Data Masking policies can be applied to Iceberg table columns just like permanent tables | Only if the data is in Parquet format with specific column encryption | Only in Business Critical Edition with Tri-Secret Secure | B | When Snowflake manages the Iceberg catalog, the full Snowflake governance suite applies — including Dynamic Data Masking, Row Access Policies, Object Tagging, and RBAC. This is a key architectural differentiator: Iceberg tables get open format interoperability WITHOUT sacrificing governance. | Medium | Architect | Iceberg Tables |
| 20 | An architect designs a data lake query pattern where 90% of data is historical (never changes) and 10% is recent (changes frequently). What is the optimal table strategy? | All data in Snowflake permanent tables | Historical data in external tables (query S3 in place), recent data in native Snowflake tables — join them via views | All data in external tables — Snowflake queries S3 directly | Use Iceberg for historical, permanent for recent | B | This is the 'hot/cold' data architecture. Cold historical data stays in S3 as external tables (no ingestion cost, no storage duplication). Hot recent data is in native tables for performance. Views or queries JOIN them as needed. This minimises Snowflake storage cost while maintaining SQL access to all data. | Hard | Architect | External Tables |
| 21 | What is the performance impact of querying an external table with no partition definition vs one with a well-defined PARTITION BY clause? | No difference — partition definitions are advisory hints only | Without partitions: full file scan of all S3 objects in the stage. With PARTITION BY: Snowflake prunes files based on partition predicates, scanning only relevant files — dramatically reducing cost and time | Partition definitions slow queries due to partition resolution overhead | Partition definitions only affect COPY INTO — not external table queries | B | Without partitions, every query against an external table scans ALL files in the stage — O(n) file overhead. With PARTITION BY expressions matching query predicates (e.g., date-based directory paths), Snowflake evaluates which files to include before scanning — enabling file-level pruning equivalent to micro-partition pruning on native tables. | Hard | Architect | External Tables |
| 22 | An architect needs to provide Snowflake SQL access to 500TB of historical logs in S3 that are updated by a Spark pipeline hourly. Loading into Snowflake is not feasible. What design maximises query performance? | Create an external table with no partitioning — Snowflake handles optimization automatically | Create a partitioned external table matching the S3 directory structure (year/month/day/hour), set AUTO_REFRESH=TRUE with SQS event notifications, create a materialized view over the external table for common aggregations | Use Snowpipe to load all new files as they arrive — external tables are too slow | Create a Snowflake permanent table with CLUSTER BY (log_date) | B | Optimal external table design for large-scale S3 data: partition definition matching S3 structure enables file pruning, AUTO_REFRESH keeps metadata current via SQS (no manual refresh), materialized views over external tables pre-compute common aggregations. This is the maximum performance achievable for external table workloads. | Hard | Architect | External Tables |
| 23 | Can materialized views be created on external tables in Snowflake? | No — MVs require native Snowflake tables | Yes — materialized views can be created on external tables to pre-compute results and improve query performance | Yes but only in Business Critical Edition | Only for Parquet-format external tables | B | Snowflake supports CREATE MATERIALIZED VIEW on external tables. This is a powerful pattern — the MV pre-computes aggregations or transformations from the external data and stores results in Snowflake native storage. Queries against the MV are fast (native storage) while the source remains in external S3/Azure/GCS. | Hard | Architect | External Tables |
| 24 | An architect compares three pipeline patterns for transforming raw events into analytics tables: Streams+Tasks, Materialized Views, and Dynamic Tables. The transformation involves multi-table joins, complex aggregations, and 5-minute freshness SLA. What is the recommended pattern? | Materialized Views — most performant | Streams + Tasks — maximum control and flexibility | Dynamic Tables with TARGET_LAG='5 minutes' — declarative, handles multi-table joins, self-manages refresh | All three achieve the same result — choose based on team preference | C | Dynamic Tables are designed for exactly this use case: multi-table transformation with a freshness SLA. MVs cannot handle complex multi-table joins with aggregations. Streams+Tasks require manual orchestration code. Dynamic Tables are declarative — define the SQL and TARGET_LAG, Snowflake manages the rest including incremental refresh where possible. | Hard | Architect | Dynamic Tables |
| 25 | A Dynamic Table DAG has 4 layers: raw → clean → aggregated → reporting. Each layer has TARGET_LAG='DOWNSTREAM' except reporting which has TARGET_LAG='10 minutes'. Describe the refresh behavior. | Each layer refreshes independently every 10 minutes | Reporting triggers its own 10-minute refresh, which triggers aggregated, which triggers clean, which triggers raw — all propagate backward through the DAG on demand | Only the reporting layer refreshes — upstream layers are static | All layers refresh simultaneously every 10 minutes | B | DOWNSTREAM lag means 'refresh when my downstream needs me.' When reporting's 10-minute cycle triggers, it needs fresh aggregated data, which needs fresh clean, which needs fresh raw. Snowflake propagates the refresh need backward through the DAG, refreshing each DOWNSTREAM layer just-in-time. This ensures end-to-end freshness without independent scheduling on each layer. | Hard | Architect | Dynamic Tables |
| 26 | An architect wants to migrate a Streams+Tasks pipeline to Dynamic Tables. The Task uses a complex MERGE statement with custom deduplication logic. What is the key consideration? | Dynamic Tables cannot replace Tasks — they are complementary | Dynamic Tables execute the MERGE logic automatically — no changes needed | Dynamic Tables use declarative SQL (SELECT-based transformation) — MERGE logic must be rewritten as a SELECT query that Snowflake can manage incrementally; not all MERGE patterns translate directly | Dynamic Tables are always more expensive than Tasks — migration is not recommended | C | Dynamic Tables are driven by SELECT queries, not MERGE statements. Complex deduplication MERGE logic must be rethought as a declarative SELECT with QUALIFY, DISTINCT, or ROW_NUMBER() patterns. If the deduplication logic cannot be expressed declaratively, Streams+Tasks may remain the right tool. Not all pipeline patterns are Dynamic Table-compatible. | Hard | Architect | Dynamic Tables |
| 27 | What is the 'initialize' behavior of a Dynamic Table when first created? | The table is empty until the first scheduled refresh | Snowflake immediately performs an initial full refresh — populating the table with the result of the defining query on current source data | The table is populated from the most recent snapshot of source tables | Initialization requires a manual REFRESH call | B | When a Dynamic Table is created, Snowflake immediately performs an initial full refresh to populate it with the current state of the source data. Subsequent refreshes are incremental (where possible). This means the table is immediately queryable after creation — no waiting for the first scheduled refresh. | Medium | Architect | Dynamic Tables |
| 28 | An architect needs to ensure a Dynamic Table pipeline stops processing if costs exceed a budget. What mechanism achieves this? | Dynamic Tables have a built-in CREDIT_LIMIT parameter | Assign a Resource Monitor to the warehouse used by Dynamic Tables — when credits are exhausted, the warehouse suspends and Dynamic Table refreshes fail | Set TARGET_LAG='NEVER' when the budget is exceeded | Dynamic Tables use serverless compute — Resource Monitors do not apply | B | Dynamic Tables run refreshes on a specified virtual warehouse. A Resource Monitor on that warehouse enforces credit limits. When the monitor triggers SUSPEND, the warehouse stops and Dynamic Table refreshes fail (retrying on next cycle). This is the budget control mechanism for Dynamic Table compute costs. | Hard | Architect | Dynamic Tables |
| 29 | An architect benchmarks a dashboard query that takes 45 seconds on a Large warehouse scanning a 5TB table. The same query runs 200 times daily. What is the TCO comparison between adding a Materialized View vs upgrading to 2XL warehouse? | 2XL warehouse is always cheaper — no MV maintenance overhead | MV: one-time maintenance compute + storage for MV result (queried from pre-computed data, minimal warehouse time); 2XL: 2x credit rate for ALL 200 daily queries — MV likely cheaper for frequent identical queries | They cost the same — Snowflake normalises per-query cost | MV is always cheaper regardless of query frequency | B | Cost model: With MV, the 200 daily queries hit pre-computed data (minimal compute, milliseconds each). MV maintenance runs once per base table change (serverless). With 2XL, all 200 queries run at 2x credit rate. For frequently-run stable queries on large tables, MVs typically have far lower TCO than warehouse upsizing. | Hard | Architect | Materialized Views |
| 30 | A Materialized View is defined on a table in database A. A query in database B references a view in database C that references the same table in database A. Can the optimizer use the MV to answer the database B query? | Yes — MV rewriting works across all database boundaries | No — transparent MV rewriting only applies to queries that directly reference the same base table the MV is defined on | Yes but only in Business Critical Edition | Only if all objects are in the same schema | B | Transparent MV rewriting applies when a query directly references the base table the MV is defined on. Complex indirection (views referencing views across databases) may prevent the optimizer from identifying the MV rewrite opportunity. Direct queries on the base table benefit most reliably from MV rewriting. | Hard | Architect | Materialized Views |
| 31 | When should an architect choose Dynamic Tables over Materialized Views? | Always — Dynamic Tables are strictly superior | When the transformation requires multi-table joins, complex SQL not supported by MVs, or a pipeline DAG of transformations — MVs are better for simple single-table aggregations with low SQL complexity | When query latency must be under 1 second — MVs are too slow | When the source data changes more than once per minute | B | MVs excel at: simple aggregations on single tables, transparent query rewriting, Enterprise Edition performance optimisation. Dynamic Tables excel at: multi-table joins, complex SQL transformations, pipeline DAGs, more flexible SQL support. The choice depends on transformation complexity and whether transparent rewriting is needed. | Hard | Architect | Materialized Views |
| 32 | A data mesh architecture has domain teams sharing data via Snowflake Data Sharing. The finance domain wants partners to query revenue metrics but not see the calculation methodology. What is the architectural design? | Share the base revenue table with column masking | Create secure views exposing calculated metrics — share these views in the outbound share, hiding the SQL definition from consumers | Create standard views and rely on RBAC to prevent DESCRIBE VIEW | Use Dynamic Data Masking on the formula columns | B | Secure views are the standard pattern for governed data sharing. The finance domain's secure view: (1) exposes only relevant metrics, (2) hides calculation formulas from consuming domains, (3) can include row filters for multi-tenancy. Sharing secure views is a Snowflake best practice for data mesh outbound sharing. | Hard | Architect | Secure Views |
| 33 | An architect implements a multi-tenant SaaS platform on Snowflake. Each customer should only see their own data from a shared table. What is the recommended design combining secure views and dynamic filtering? | Create one table and one view per customer | Create one table, one secure view with CURRENT_ROLE() or a lookup-based WHERE clause filtering to customer's data — all customers share the same view object | Use Row Access Policies on the base table — secure views are not needed | Create separate databases per customer — secure views are not multi-tenant capable | B | Multi-tenant SaaS pattern: one base table, one secure view per tenant tier (or one view with CURRENT_ROLE/user lookup). The secure view's WHERE clause dynamically filters to the current user's tenant. The secure keyword prevents tenant A from reverse-engineering the filter to attempt accessing tenant B's data. This scales to thousands of tenants without schema proliferation. | Hard | Architect | Secure Views |
| 34 | What is the difference between SECURE VIEW and a view protected by Row Access Policies from a data sharing perspective? | Row Access Policies are shareable; secure views are not | Secure views can be included in Data Shares directly; Row Access Policies on base tables also work in shares but the table owner must grant policy access to the share — secure views are simpler for sharing use cases | There is no difference — both work identically in data shares | Only secure views work in cross-cloud shares | B | For Data Sharing: secure views are cleanly shareable — add the view to the share, consumer queries it directly. Row Access Policies on base tables also work in shares, but the setup is more complex (share access to the policy function, etc.). For most sharing scenarios, secure views are the simpler architectural choice. | Hard | Architect | Secure Views |
| 35 | An architect needs to prevent a sophisticated analyst from inferring masked salary values through repeated queries with different filter conditions against a salary view. What additional protection does SECURE VIEW provide over a standard masked view? | Secure views add row-level encryption preventing all inference | Secure views disable query optimisations that could leak information through query execution patterns (e.g., short-circuit evaluation) — preventing timing and optimisation-based inference attacks | Secure views hash all query results before returning them | There is no additional protection — both behave identically for analytical queries | B | Sophisticated inference attacks exploit query optimisation behaviors — e.g., a WHERE clause that short-circuits on certain values can reveal data through different query times. Secure views disable these optimisations, preventing timing-based inference. For truly sensitive data (salary, PII), secure views prevent statistical inference that standard views with masking do not fully prevent. | Hard | Architect | Secure Views |

---
*35 questions · Object Hierarchy: Tables & Views · Advanced Architect (ADA-C01)*