# Snowflake Object Hierarchy — Advanced Data Engineer

> **Exam:** Advanced Data Engineer (ADE-C01)  
> **Topics:** Tables (Permanent, Temporary, Transient, Iceberg, External, Dynamic) · Views (Standard, Materialized, Secure)  
> **Total Questions:** 36

---

| No | Question | Option A | Option B | Option C | Option D | Answer | Explanation | Level | Exam | Topic |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | A data engineer runs COPY INTO on a permanent table. Some files fail to load. After fixing the files, the engineer re-runs COPY INTO. What happens to the already-loaded files? | All files reload — COPY INTO always reloads everything | Successfully loaded files are skipped (tracked in load metadata for 64 days) — only previously failed or new files are loaded | An error is thrown — COPY INTO cannot be run twice on the same stage | The engineer must use FORCE=TRUE to reload all files | B | COPY INTO tracks a 64-day load history per stage/table combination. Files successfully loaded within this window are automatically skipped on re-run, preventing duplicates. Only files not in the load history (new files or previously failed ones) are loaded. Use FORCE=TRUE to override and reload already-loaded files. | Medium | Data Engineer | Permanent Tables |
| 2 | A data engineer needs to add a NOT NULL column to a large permanent table with 10 billion rows. What is the recommended approach? | ALTER TABLE ADD COLUMN col VARCHAR NOT NULL — Snowflake handles it instantly | ALTER TABLE ADD COLUMN col VARCHAR, then UPDATE the column, then ALTER TABLE ALTER COLUMN SET NOT NULL — avoids table lock during update | Recreate the entire table with the new column schema | NOT NULL constraints cannot be added to existing columns in Snowflake | A | Snowflake's ALTER TABLE ADD COLUMN is metadata-only — it completes instantly regardless of table size (no full table rewrite). For adding NOT NULL columns with a default value, ALTER TABLE ADD COLUMN col VARCHAR NOT NULL DEFAULT 'value' also completes as a metadata operation. Snowflake handles constraint enforcement lazily. | Hard | Data Engineer | Permanent Tables |
| 3 | A permanent table receives MERGE statements from a CDC pipeline every 5 minutes. After 3 months, storage usage is 15x the raw data size. What is the most likely cause? | CDC MERGE causes table fragmentation requiring manual VACUUM | With 90-day Time Travel retention, each MERGE creates new micro-partitions while retaining old ones — 3 months × 6 merges/hour = ~13,000 micro-partition generations retained | MERGE statements duplicate data by design | The clustering key is causing excessive micro-partition overlap | B | High-frequency DML with long Time Travel retention is the leading cause of storage amplification. Each MERGE cycle creates new micro-partitions (for changed rows) while old micro-partitions are retained for the retention window. Solution: reduce DATA_RETENTION_TIME_IN_DAYS on the target table or switch to transient table type for high-churn staging tables. | Hard | Data Engineer | Permanent Tables |
| 4 | What is the purpose of COPY INTO ... PURGE=TRUE? | Purges old Time Travel versions of the target table after loading | Deletes the source files from the stage after successful loading | Purges the result cache for the loaded table | Removes duplicate rows from the loaded data | B | PURGE=TRUE in COPY INTO deletes source files from the stage after successful loading. This is useful for preventing re-loading in manual pipelines and reducing stage storage costs. Note: files that fail to load are NOT purged even with PURGE=TRUE. | Medium | Data Engineer | Permanent Tables |
| 5 | A data engineer uses CREATE TABLE ... CLONE to create a dev copy of a production table. They then run DELETE FROM dev_table WHERE id < 1000000. What storage is created? | The DELETE affects both the clone and the source table | New micro-partitions are created in the clone for the deleted rows' data (the original micro-partitions are modified from the clone's perspective); the source table is unaffected | Zero storage — clone deletes are metadata-only | The clone is invalidated by the DELETE and must be recreated | B | Zero-copy clones are copy-on-write. When the DELETE modifies rows, new micro-partitions are created for the clone (Snowflake rewrites affected partitions). The source table's micro-partitions are unaffected — they remain shared with the clone for unmodified data. Only modified partitions diverge and create new storage. | Hard | Data Engineer | Permanent Tables |
| 6 | A stored procedure creates a temporary table, fails mid-execution due to a DML error, and the calling session continues. Is the temporary table still accessible in the session? | No — temporary tables are dropped on any error | Yes — the temporary table exists until the session ends, regardless of the stored procedure's success or failure | Only if the procedure used EXECUTE AS CALLER | Yes, but only if the error was caught with an exception handler | B | Temporary tables are session-scoped, not procedure-scoped. A temporary table created in a stored procedure persists in the session even if the procedure fails. The table continues to exist until the session ends. This is important for debugging — you can inspect the temp table contents after a procedure failure to diagnose the issue. | Hard | Data Engineer | Temporary Tables |
| 7 | A data engineer writes an ELT script that creates 5 temporary staging tables and runs complex transformations. After testing, they run the script in production. The production session ends. What cleanup is needed? | Run DROP TABLE on each temporary table | No cleanup needed — all 5 temporary tables are automatically dropped when the session ends | Run TRUNCATE on each temporary table before closing the session | Temporary tables persist until the warehouse suspends | B | Temporary tables auto-drop on session end. No explicit cleanup code is needed. This is one of the key operational advantages of temporary tables over transient tables for intermediate ELT staging — no DROP TABLE boilerplate required at the end of scripts. | Easy | Data Engineer | Temporary Tables |
| 8 | What happens if a data engineer tries to SELECT from a temporary table in a Snowflake Task? | Tasks cannot access temporary tables — they run in isolated serverless sessions | Each Task execution runs in its own session — a temporary table created by one Task execution is not accessible in subsequent executions or other Tasks | Temporary tables are global within a Task graph (DAG) | Tasks automatically create their own temporary table namespace | B | Each Task execution creates a new isolated session. Temporary tables created in one execution are dropped at session end and are NOT available to the next execution. For state shared across Task executions, use permanent or transient tables. This is a common pitfall when migrating Stored Procedure ETL (which uses session-scoped temp tables) to Tasks. | Hard | Data Engineer | Temporary Tables |
| 9 | A data engineer's ETL pipeline has a staging table that holds data for 2 hours before it is transformed and purged. The table is shared across multiple pipeline sessions. What is the optimal table type? | Temporary — session scoped | Transient — persists between sessions, no Fail-Safe overhead for short-lived staging data | Permanent — best durability | External — data stays in S3 | B | Multi-session staging tables must be transient (not temporary — they'd drop with each session). Transient tables eliminate Fail-Safe overhead — since the staging data is temporary by nature (purged after 2 hours), the 7-day Fail-Safe on a permanent table is wasteful. DATA_RETENTION=0 on the transient table further eliminates Time Travel overhead. | Medium | Data Engineer | Transient Tables |
| 10 | A data engineer sets DATA_RETENTION_TIME_IN_DAYS=0 on a transient staging table. A wrong TRUNCATE TABLE is executed. What are the recovery options? | Use UNDROP to recover — TRUNCATE is reversible | Use Time Travel AT (OFFSET => -1) to see pre-TRUNCATE data | No recovery — with retention=0 on a transient table, TRUNCATE immediately purges data with no Time Travel or Fail-Safe | Restore from Fail-Safe via Snowflake Support | C | DATA_RETENTION=0 disables Time Travel. Transient tables have no Fail-Safe. TRUNCATE on a transient table with retention=0 immediately and permanently removes the data with no recovery path. This is intentional for staging tables where speed is prioritized over recovery — but engineers must be aware of the risk. | Hard | Data Engineer | Transient Tables |
| 11 | Why might a data engineer use a transient table for a large intermediate join result in an ELT pipeline? | Transient tables are faster for JOIN operations | To avoid Fail-Safe storage cost on large intermediate data that is only needed temporarily — combine with DATA_RETENTION=0 for minimum storage overhead | Transient tables have better compression than permanent tables | Transient tables support better concurrency for parallel pipeline reads | B | Large intermediate join results can be expensive to store as permanent tables (Fail-Safe adds 7 days of historical versions). Transient tables with DATA_RETENTION=0 store only the live data — no historical versions. For multi-session ETL where temporary tables won't work, this is the cost-optimal choice. | Medium | Data Engineer | Transient Tables |
| 12 | A data engineer creates a transient schema for a dev environment. A colleague tries to ALTER TABLE dev_schema.orders MODIFY COLUMN status VARCHAR(50) to VARCHAR(200). What happens? | Fails — transient tables are immutable | Succeeds — DDL on transient tables works identically to permanent tables; the transient property only affects storage retention | Succeeds but creates a permanent table — ALTER promotes transient to permanent | Fails — only ACCOUNTADMIN can alter transient tables | B | Transient tables support all standard DDL and DML operations — ALTER TABLE, CREATE INDEX (clustering), TRUNCATE, etc. The TRANSIENT property exclusively affects the storage behavior (no Fail-Safe, limited Time Travel). All schema evolution operations are identical to permanent tables. | Medium | Data Engineer | Transient Tables |
| 13 | A data engineer creates an Iceberg table with Snowflake as catalog using CREATE ICEBERG TABLE. What is the minimum prerequisite Snowflake object that must exist before running this DDL? | An external stage pointing to the S3 bucket | An EXTERNAL VOLUME defining the cloud storage location and credentials | A Snowflake Storage Integration | A Snowflake service account in the target cloud | B | CREATE ICEBERG TABLE requires an EXTERNAL VOLUME — a Snowflake object that encapsulates the cloud storage location (S3 bucket/prefix, Azure container, GCS bucket) and the IAM role/credentials Snowflake uses to access it. The external volume is created separately with CREATE EXTERNAL VOLUME and referenced in the Iceberg table DDL. | Medium | Data Engineer | Iceberg Tables |
| 14 | A data engineer needs to load data into an Iceberg table managed by Snowflake. Which method works? | COPY INTO only — DML is not supported on Iceberg tables | INSERT INTO, COPY INTO, MERGE, and other standard DML — Snowflake-managed Iceberg tables support full DML | Only Snowpipe can ingest into Iceberg tables | Only SELECT statements work — Iceberg tables are read-only | B | Snowflake-managed Iceberg tables support all standard DML (INSERT, UPDATE, DELETE, MERGE) and loading methods (COPY INTO, Snowpipe, INSERT INTO). This is identical to native permanent tables in terms of data manipulation capabilities. | Easy | Data Engineer | Iceberg Tables |
| 15 | A data engineer runs a COPY INTO on an Iceberg table and then queries the data immediately. Where are the actual Parquet data files stored? | In Snowflake's internal managed storage | In the external storage (S3/Azure/GCS) defined by the EXTERNAL VOLUME | In the stage used by COPY INTO | In Snowflake's cache temporarily, then moved to external storage | B | Iceberg table data files are always written to the external storage (S3/Azure/GCS) defined in the EXTERNAL VOLUME. Snowflake manages the Iceberg metadata (snapshots, manifests) but the Parquet data files reside in customer-owned cloud storage. This is fundamental to Iceberg's open format principle. | Medium | Data Engineer | Iceberg Tables |
| 16 | What is the purpose of BASE_LOCATION in a CREATE ICEBERG TABLE statement? | Specifies the Snowflake schema where the table's metadata is stored | Specifies the relative path within the external volume where the table's Parquet data files and Iceberg metadata will be written | Defines the base clustering column for the table | Specifies the base table that the Iceberg table is derived from | B | BASE_LOCATION is the relative path within the external volume's storage location where Snowflake writes the Iceberg table's data files and metadata. For example, BASE_LOCATION='iceberg/orders/' tells Snowflake to write all files under s3://bucket/iceberg/orders/. This namespaces the table's files within the external volume. | Medium | Data Engineer | Iceberg Tables |
| 17 | A data engineer adds a new column to an Iceberg table via ALTER TABLE. How does Iceberg handle schema evolution? | Schema changes require dropping and recreating the table | Iceberg tracks schema evolution in its metadata — new columns are nullable by default; old Parquet files without the column return NULL for the new column | All existing Parquet files are rewritten to include the new column | Schema changes on Iceberg tables are not supported in Snowflake | B | Apache Iceberg natively supports schema evolution. Adding a column is a metadata-only operation — Snowflake adds the column to the Iceberg schema without rewriting existing Parquet files. When old files are queried, the missing column returns NULL. This is one of Iceberg's key advantages over traditional Hive-style partitioned tables. | Hard | Data Engineer | Iceberg Tables |
| 18 | A data engineer queries an Iceberg table and needs to see data as of 3 days ago. The table uses Snowflake as catalog. How is this done? | Time Travel is not supported on Iceberg tables | SELECT * FROM iceberg_table AT (TIMESTAMP => DATEADD(day,-3,CURRENT_TIMESTAMP)) or AT (SNAPSHOT => snapshot_id) | Use UNDROP TABLE to restore the 3-day-old version | Query the raw Parquet files directly from S3 with a timestamp filter | B | For Snowflake-managed Iceberg tables, Time Travel uses Iceberg's snapshot mechanism. AT (TIMESTAMP) finds the snapshot that was current at that time. AT (SNAPSHOT => id) uses a specific Iceberg snapshot ID. The syntax mirrors native Snowflake Time Travel but leverages Iceberg snapshots under the hood. | Medium | Data Engineer | Iceberg Tables |
| 19 | A data engineer creates an external table on S3 CSV files. After creation, 1000 new files are added to the S3 bucket. AUTO_REFRESH=FALSE. The engineer runs a query — do the new files appear? | Yes — external tables automatically detect new files | No — without AUTO_REFRESH or manual REFRESH, the external table metadata still references the original file list | Only if the query includes a WHERE clause on the file metadata | Yes, but only after the warehouse cache is cleared | B | External table metadata is a snapshot of the stage contents at creation/last refresh time. New files added to S3 are not visible until ALTER EXTERNAL TABLE table_name REFRESH is run (manual) or AUTO_REFRESH triggers the refresh via SQS events. Without refresh, queries operate on the stale file list. | Medium | Data Engineer | External Tables |
| 20 | What SQL command manually refreshes an external table's file metadata? | REFRESH TABLE ext_table | ALTER EXTERNAL TABLE ext_table REFRESH | UPDATE EXTERNAL TABLE ext_table METADATA | SYNC EXTERNAL TABLE ext_table | B | ALTER EXTERNAL TABLE table_name REFRESH scans the underlying stage and updates the external table's file inventory metadata. This is the manual equivalent of AUTO_REFRESH. It can be scheduled via a Snowflake Task for controlled refresh intervals. | Easy | Data Engineer | External Tables |
| 21 | A data engineer defines an external table on Parquet files in S3 with a directory structure: s3://bucket/data/year=2024/month=01/day=15/file.parquet. How should partitions be defined to enable pruning? | Partition pruning is automatic — no definition needed | PARTITION BY (year INT AS (VALUE:year::INT), month INT AS (VALUE:month::INT), day INT AS (VALUE:day::INT)) — extracting partition values from the file path using SPLIT_PART or regex on metadata$filename | CREATE PARTITION ON (s3://bucket/data/year/month/day) | Partitions cannot be defined on Parquet external tables | B | External table partitions are defined as virtual columns derived from the file path using metadata$filename. SPLIT_PART(metadata$filename,'/',5) for year=2024 requires parsing the path. Alternatively, Snowflake supports Hive-style partition path parsing. Well-defined partitions enable significant query pruning, skipping irrelevant time-range files. | Hard | Data Engineer | External Tables |
| 22 | Can COPY INTO be used to load data from an external table into a native Snowflake table? | COPY INTO cannot read from external tables — only stages | Yes — SELECT * FROM external_table can be used in INSERT INTO native_table SELECT ... — treating the external table as a query source; or COPY INTO can read from the same stage | External tables and COPY INTO use separate loading mechanisms | Only if the external table uses CSV format | B | To load from an external table to a native table, use INSERT INTO native_table SELECT ... FROM external_table or run COPY INTO directly from the underlying stage (external tables and COPY INTO share the same stage reference). The external table provides a SQL query interface to the same data COPY INTO reads. | Medium | Data Engineer | External Tables |
| 23 | A data engineer wants to build a pipeline: raw Kafka events → cleaned events → aggregated hourly metrics. They want minimal code, automatic refresh, and 5-minute freshness. What is the recommended implementation? | 3 Streams + 3 Tasks with MERGE statements | 3 Dynamic Tables in a DAG: raw_events_clean (TARGET_LAG='DOWNSTREAM') → hourly_metrics (TARGET_LAG='5 minutes') with the cleaning as an intermediate DOWNSTREAM table | 3 Materialized Views — one per layer | A Snowpipe + single Task pipeline | B | Dynamic Table DAG is purpose-built for this: intermediate tables use DOWNSTREAM lag (refresh when needed by downstream), final table has explicit 5-minute TARGET_LAG. Snowflake manages the refresh cascade automatically. No MERGE code, no Task scheduling, no Stream offset management — fully declarative. | Medium | Data Engineer | Dynamic Tables |
| 24 | A data engineer wants to suspend a Dynamic Table to stop refreshes during a maintenance window. What command achieves this? | ALTER DYNAMIC TABLE dt_name SUSPEND | PAUSE DYNAMIC TABLE dt_name | ALTER DYNAMIC TABLE dt_name SET SUSPENDED=TRUE | STOP DYNAMIC TABLE dt_name | A | ALTER DYNAMIC TABLE table_name SUSPEND stops automatic refreshes. The table retains its last refreshed data and remains queryable — it just stops updating. ALTER DYNAMIC TABLE table_name RESUME restarts refreshes. This is the equivalent of pausing a Task for maintenance windows. | Easy | Data Engineer | Dynamic Tables |
| 25 | A Dynamic Table has TARGET_LAG='1 minute'. The refresh takes 90 seconds to complete. What happens? | The table is automatically dropped — refresh time must be under TARGET_LAG | Snowflake runs the next refresh immediately after the previous one completes, as the table is already behind its TARGET_LAG — refreshes run back-to-back until the lag is recovered | The refresh is terminated at 60 seconds to meet the lag SLA | Snowflake automatically increases the TARGET_LAG to accommodate the refresh duration | B | If refresh duration exceeds TARGET_LAG, the table continuously runs back-to-back refreshes trying to catch up. The table will always be at least 90s stale (the minimum possible given refresh time). Snowflake does not auto-adjust TARGET_LAG — the engineer must either optimise the refresh query or increase TARGET_LAG to a realistic value. | Hard | Data Engineer | Dynamic Tables |
| 26 | What does the INITIALIZE = ON_CREATE vs ON_SCHEDULE parameter control in a Dynamic Table? | ON_CREATE runs the initial full refresh immediately at creation; ON_SCHEDULE waits for the first scheduled refresh cycle before populating data | ON_SCHEDULE creates the table faster; ON_CREATE validates the query first | ON_CREATE uses serverless compute; ON_SCHEDULE uses the assigned warehouse | There is no INITIALIZE parameter — Dynamic Tables always initialize immediately | A | INITIALIZE = ON_CREATE (default) immediately performs a full refresh at creation time — table is populated right away. ON_SCHEDULE delays the initial population until the first scheduled refresh cycle based on TARGET_LAG. ON_SCHEDULE is useful when you don't need immediate data population and want to avoid the initial full-refresh cost. | Hard | Data Engineer | Dynamic Tables |
| 27 | How does a data engineer monitor whether a Dynamic Table refresh is using incremental or full refresh mode? | SHOW DYNAMIC TABLES displays the current refresh mode | DYNAMIC_TABLE_REFRESH_HISTORY in ACCOUNT_USAGE shows REFRESH_ACTION column (INCREMENTAL or FULL) for each refresh execution | Query INFORMATION_SCHEMA.REFRESH_MODE table | The refresh mode is set at creation and never changes | B | SNOWFLAKE.ACCOUNT_USAGE.DYNAMIC_TABLE_REFRESH_HISTORY contains detailed refresh history including REFRESH_ACTION (INCREMENTAL or FULL), refresh duration, bytes processed, and status. Monitoring this view reveals if the table has fallen back to FULL refresh (indicating the SQL may be too complex for incremental maintenance). | Medium | Data Engineer | Dynamic Tables |
| 28 | A Dynamic Table is created with the following SQL: CREATE DYNAMIC TABLE sales_summary TARGET_LAG='10 minutes' WAREHOUSE=compute_wh AS SELECT region, SUM(amount) total FROM raw_sales GROUP BY region. What happens if a new region is added to raw_sales? | Dynamic Table must be dropped and recreated to handle new regions | The new region automatically appears in the next refresh — Dynamic Tables are declarative and reflect the full current state of the source query result | Only regions present at creation time are included — Dynamic Tables have a fixed result schema | A MERGE must be run to add the new region to the Dynamic Table | B | Dynamic Tables are fully declarative — their content always reflects the current result of the defining SQL query on the source data. New regions added to raw_sales will appear in sales_summary after the next refresh (within 10 minutes). No DDL or pipeline changes needed. | Easy | Data Engineer | Dynamic Tables |
| 29 | A data engineer creates a view that joins 5 large tables with complex window functions. Analytics teams query this view 500 times per day. What is the performance implication? | No impact — views are cached after first execution | Each of the 500 queries fully re-executes the join and window function logic — warehouse costs are incurred 500 times | The view is compiled once and cached — only the first query is expensive | Views automatically create an MV after 10 executions | B | Standard views are executed fresh on every query — no result caching beyond the Query Result Cache (which only helps for identical queries on unchanged data). For expensive views queried frequently, consider Materialized Views or Dynamic Tables to pre-compute results. | Medium | Data Engineer | Standard Views |
| 30 | A data engineer uses a view to abstract a complex multi-step transformation. The underlying table schema changes — a column is renamed. What is the impact on the view? | The view automatically adapts to the new column name | The view breaks — referencing the old column name will cause a resolution error at query time | The view is automatically dropped and must be recreated | Only views with SCHEMA_EVOLUTION enabled adapt automatically | B | Snowflake views use late binding by default — column resolution happens at query time, not at creation time. Renaming a column breaks the view because the SQL references the old name. The view must be recreated (CREATE OR REPLACE VIEW) referencing the new column name. This is a key consideration in schema evolution management. | Medium | Data Engineer | Standard Views |
| 31 | What is a RECURSIVE view (WITH RECURSIVE CTE) used for in data engineering? | Views that reference themselves to create infinite loops | Views that use WITH RECURSIVE common table expressions to traverse hierarchical or graph-structured data (org charts, bill of materials, parent-child relationships) | Views that recursively refresh themselves like Dynamic Tables | Views that recursively apply masking policies | B | RECURSIVE CTEs (WITH RECURSIVE) in views enable traversal of hierarchical data structures — parent-child relationships, org hierarchies, graph paths. Snowflake supports RECURSIVE CTEs in view definitions, enabling SQL-based graph traversal without procedural code. | Hard | Data Engineer | Standard Views |
| 32 | A data engineer creates a Materialized View on a table that receives 100,000 row updates per hour. After a week, the DBA notices high unexpected serverless compute costs. What is the cause? | Materialized views have a flat monthly maintenance fee | Each update to the base table triggers MV maintenance using serverless compute — 100K updates/hour × 24h × 7 days generates continuous maintenance compute charges | The MV is being queried too frequently | MVs auto-replicate across regions, doubling compute | B | MV maintenance is event-driven by base table DML. High-frequency updates generate proportionally high maintenance compute costs. For very high-churn tables, Dynamic Tables with a TARGET_LAG are often more cost-predictable than MVs, which charge per-change-event for maintenance. | Hard | Data Engineer | Materialized Views |
| 33 | A Materialized View is STALE. When does this happen and how is it resolved? | An MV becomes stale after 24 hours and must be manually refreshed | An MV can become stale if: the base table schema changes in an incompatible way, the MV has a SQL error after a DDL change, or maintenance fails repeatedly. It must be dropped and recreated. | MVs never become stale — Snowflake guarantees continuous freshness | Run ALTER MATERIALIZED VIEW mv_name REFRESH to fix staleness | B | A Materialized View becomes stale (status=FAILED or SUSPENDED) when: incompatible base table schema changes, persistent maintenance failures, or certain DDL operations on dependencies. The MV remains queryable but serves stale data. Resolving requires fixing the root cause and either dropping/recreating or using ALTER MATERIALIZED VIEW ... RESUME where supported. | Hard | Data Engineer | Materialized Views |
| 34 | Can a Materialized View include a JOIN between two tables? | No — MVs are restricted to single-table queries only | Yes — MVs can include JOINs, but there are restrictions: non-deterministic joins may prevent incremental maintenance, and certain join types may force full refresh | Yes — MVs support all SQL including multi-table joins without restrictions | Only if both tables are in the same schema | B | Snowflake Materialized Views do support JOINs, but with significant restrictions. Some join patterns prevent incremental maintenance, forcing full refresh on every base table change. Complex multi-table joins with aggregations may not be maintainable at all. Dynamic Tables are generally better suited for complex multi-table transformations. | Hard | Data Engineer | Materialized Views |
| 35 | A data engineer creates a view for the analytics team that filters PII columns. Should they use a standard view or a secure view? What is the deciding factor? | Always use secure views — security is always better | Use a standard view if the team should not know the underlying tables but CAN know which columns are filtered; use a secure view if the view's definition itself must be hidden | Secure views are always required for PII data | Standard views with Dynamic Data Masking are always sufficient for PII | B | The deciding factor is whether the view definition must be hidden. If it's acceptable for the analytics team to see DESCRIBE VIEW output (showing the WHERE/CASE logic), a standard view is fine and performs better. If the filtering logic, column names, or table structure must be hidden from the consumer, use a secure view. | Medium | Data Engineer | Secure Views |
| 36 | A data engineer discovers that a secure view they created is slower than the equivalent standard view. What is the reason? | Secure views use a separate, slower execution engine | Secure views disable certain query optimisations (like filter push-down and short-circuit evaluation) to prevent data inference — this is by design and is the security-performance trade-off | The view definition is being re-compiled on every query | Secure views do not use the warehouse local disk cache | B | Snowflake disables specific optimisations for secure views to prevent timing-based inference attacks (where an analyst could determine filtered values based on query execution time differences). This is an intentional security-performance trade-off. For non-sensitive analytical views, standard views with RBAC deliver better performance. | Medium | Data Engineer | Secure Views |

---
*36 questions · Object Hierarchy: Tables & Views · Advanced Data Engineer (ADE-C01)*