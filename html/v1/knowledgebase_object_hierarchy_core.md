# Snowflake Object Hierarchy — SnowPro Core

> **Exam:** SnowPro Core (COF-C03)  
> **Topics:** Tables (Permanent, Temporary, Transient, Iceberg, External, Dynamic) · Views (Standard, Materialized, Secure)  
> **Total Questions:** 85

---

| No | Question | Option A | Option B | Option C | Option D | Answer | Explanation | Level | Exam | Topic |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | What is the default table type when you execute CREATE TABLE in Snowflake without any qualifier? | Temporary | Transient | Permanent | External | C | CREATE TABLE with no qualifier creates a permanent table. Permanent tables persist until explicitly dropped, have full Time Travel and Fail-Safe support, and are the standard production table type. | Easy | Core | Permanent Tables |
| 2 | Which combination of data protection features applies ONLY to permanent tables and NOT to transient or temporary tables? | Clustering + Result Cache | Time Travel + Fail-Safe | Auto-suspend + Auto-resume | Micro-partitioning + Pruning | B | Permanent tables have both Time Travel (0–90 days depending on edition) and Fail-Safe (7 days, Snowflake-managed). Transient tables have Time Travel (0–1 day) but NO Fail-Safe. Temporary tables have Time Travel (0–1 day) but NO Fail-Safe. | Easy | Core | Permanent Tables |
| 3 | What is the maximum DATA_RETENTION_TIME_IN_DAYS that can be set on a permanent table in Enterprise Edition? | 1 | 7 | 30 | 90 | D | Enterprise Edition supports up to 90 days of Time Travel on permanent tables. Standard Edition is limited to 1 day. The value is configurable at account, database, schema, or individual table level. | Easy | Core | Permanent Tables |
| 4 | A permanent table has DATA_RETENTION_TIME_IN_DAYS=7. It is dropped on Monday. On what day does Fail-Safe begin? | Monday (immediately on drop) | The following Monday (after 7-day Time Travel window) | 7 days after the table was last modified | Fail-Safe begins simultaneously with Time Travel | B | Time Travel and Fail-Safe are sequential. After the 7-day Time Travel window expires (the following Monday), the 7-day Fail-Safe period begins. Total protection window = 14 days from the drop event. | Medium | Core | Permanent Tables |
| 5 | Which DDL statement creates a permanent table by copying the structure and data of an existing table? | CREATE TABLE new_t LIKE old_t | CREATE TABLE new_t CLONE old_t | CREATE TABLE new_t AS SELECT * FROM old_t | CREATE TABLE new_t COPY old_t | C | CREATE TABLE ... AS SELECT (CTAS) copies both structure and data. CREATE TABLE ... LIKE copies only structure (no data). CREATE TABLE ... CLONE creates a zero-copy clone (metadata only, shares storage). COPY is not valid DDL syntax for tables. | Easy | Core | Permanent Tables |
| 6 | What storage cost does a zero-copy clone of a permanent table incur at the moment of creation? | Full duplication of storage — same cost as original | No additional storage cost — clone shares micro-partitions with source until modified | 50% of original — Snowflake compresses the clone | Clones are charged a flat metadata fee regardless of table size | B | Zero-copy cloning creates a metadata pointer to the original micro-partitions. No data is physically copied. Storage cost is only incurred as the clone or source diverges through DML operations (new micro-partitions are created for changed data). | Easy | Core | Permanent Tables |
| 7 | A permanent table row is deleted via DELETE. The transaction commits. Which statement about the deleted data is correct? | The data is immediately purged from storage | The data remains in micro-partitions and is accessible via Time Travel until retention expires | The data is moved to Fail-Safe immediately | The data is compressed and archived to cloud object storage | B | Snowflake uses immutable micro-partitions. DELETE marks rows as deleted in metadata but the original micro-partition is retained. The deleted data is accessible via Time Travel (AT/BEFORE) until the retention window expires, then enters Fail-Safe. | Medium | Core | Permanent Tables |
| 8 | Which privilege is required to create a table inside a schema? | OWNERSHIP on the database | CREATE TABLE on the schema | USAGE on the database only | MODIFY on the schema | B | CREATE TABLE privilege on the schema (plus USAGE on the database and schema) is required to create tables. OWNERSHIP grants full control but is not the minimum required privilege for table creation by a non-owner role. | Easy | Core | Permanent Tables |
| 9 | A permanent table has CLUSTER BY (region, date). When a large INSERT adds new data, what happens to clustering? | New data is immediately sorted into the correct cluster positions | New data is written as new micro-partitions which may not be optimally clustered — Automatic Clustering reclusters over time | The INSERT is rejected if data is not pre-sorted | Clustering is disabled during INSERT operations | B | New micro-partitions are created for inserted data but may not maintain optimal clustering. Automatic Clustering (Enterprise Edition) runs in the background using serverless compute to maintain cluster quality over time by reorganising micro-partitions. | Medium | Core | Permanent Tables |
| 10 | What does the SHOW TABLES command display for a permanent table's retention time if DATA_RETENTION_TIME_IN_DAYS has not been explicitly set? | 0 — retention must be explicitly set | 1 — the Standard Edition default | The account-level default (which itself defaults to 1 day) | 90 — Enterprise Edition always defaults to maximum | C | If not explicitly set on a table, the retention time inherits from the schema, then database, then account default. The account default is 1 day unless changed. SHOW TABLES displays the effective retention time after inheritance resolution. | Medium | Core | Permanent Tables |
| 11 | What is the scope and lifetime of a temporary table in Snowflake? | Session-scoped: exists only for the duration of the session that created it | Transaction-scoped: dropped after each COMMIT | Account-scoped: visible to all users but dropped after 24 hours | Database-scoped: visible within the database until manually dropped | A | Temporary tables are session-scoped — they exist only within the session that created them and are automatically dropped when that session ends. They are invisible to other sessions, even for the same user logged in via a different session. | Easy | Core | Temporary Tables |
| 12 | What is the maximum DATA_RETENTION_TIME_IN_DAYS for a temporary table? | 0 — no Time Travel on temporary tables | 1 day maximum | 7 days | 90 days in Enterprise Edition | B | Temporary tables support Time Travel with a maximum of 1 day (0 or 1). They do NOT have Fail-Safe. The retention can be set to 0 to disable Time Travel entirely for the temporary table. | Easy | Core | Temporary Tables |
| 13 | A user creates a temporary table named STAGING in session A. Another user in session B tries to SELECT from STAGING. What happens? | Session B sees the same STAGING table — temporary tables are shared within a database | Session B gets a 'table not found' error — temporary tables are invisible to other sessions | Session B can read but not write to STAGING | Session B sees an empty version of STAGING | B | Temporary tables are session-private. Session B cannot see session A's temporary table. If session B has a permanent table also named STAGING, it would see that instead. The temporary table exists only in its creating session's namespace. | Easy | Core | Temporary Tables |
| 14 | A developer creates a temporary table with the same name as an existing permanent table in the same schema. What happens? | An error is thrown — table names must be unique in a schema | The permanent table is replaced by the temporary table for the duration of the session | The temporary table takes precedence — queries in that session resolve to the temporary table; the permanent table is unaffected | Both tables coexist and must be referenced by fully qualified names | C | Temporary tables shadow permanent tables of the same name within the same session. Queries resolve to the temporary table. The permanent table is unaffected and visible to other sessions. This can cause subtle bugs if developers forget they have a shadowing temp table. | Medium | Core | Temporary Tables |
| 15 | Do temporary tables in Snowflake consume storage credits? | No — temporary tables are held in memory only | Yes — temporary tables use cloud storage (same as permanent) but with no Fail-Safe overhead | No — temporary tables use warehouse local disk only | Yes — temporary tables use 2x storage due to session isolation overhead | B | Temporary tables store data in cloud storage (S3/Azure/GCS) just like permanent tables, and storage is charged. However, they have no Fail-Safe overhead (no 7-day retention of old versions). They are cost-efficient for intermediate results because they are automatically dropped when the session ends. | Medium | Core | Temporary Tables |
| 16 | What happens to a temporary table if the Snowflake session is closed unexpectedly (e.g., browser crash)? | The table persists for 24 hours then auto-drops | The table is immediately dropped — session termination triggers automatic cleanup | The table survives until the warehouse suspends | The table is preserved in Fail-Safe for 7 days | B | Temporary tables are tied to the session lifecycle regardless of how the session ends. When a session terminates — gracefully or unexpectedly — Snowflake automatically drops all temporary tables created in that session. | Medium | Core | Temporary Tables |
| 17 | Which of the following is a valid use case for temporary tables? | Storing lookup data shared across all ETL pipelines | Intermediate transformation results within a single ETL session | Archiving historical data with 90-day recovery needs | Sharing data with external partners via Snowflake Marketplace | B | Temporary tables are ideal for intermediate ETL results within a session — they provide fast storage for staging data without the overhead of managing permanent objects, and automatically clean up when the session ends. | Easy | Core | Temporary Tables |
| 18 | Can a temporary table be cloned? | No — temporary tables cannot be cloned | Yes — but the clone is also temporary and session-scoped | Yes — the clone is a permanent table by default | Yes — but only if DATA_RETENTION_TIME_IN_DAYS=1 | B | A temporary table can be cloned, but the resulting clone is also a temporary table bound to the same session. You cannot clone a temporary table into a permanent table directly — though you can CREATE TABLE permanent_t AS SELECT * FROM temp_t to copy data. | Hard | Core | Temporary Tables |
| 19 | A temporary table and a transient table both lack Fail-Safe. What is the key difference between them? | Transient tables support clustering; temporary tables do not | Temporary tables are session-scoped and auto-drop; transient tables persist until explicitly dropped and are visible to all roles | Transient tables have 7-day Time Travel; temporary tables have 0-day | Temporary tables are faster to query due to in-memory storage | B | The critical difference is scope and persistence. Temporary tables live only for the session. Transient tables persist like permanent tables (until explicitly dropped) and are visible to all users with appropriate privileges — they just lack Fail-Safe to reduce storage overhead. | Medium | Core | Temporary Tables |
| 20 | Which SQL creates a temporary table with Time Travel disabled? | CREATE TEMPORARY TABLE t (id INT) NO TIME TRAVEL | CREATE TEMPORARY TABLE t (id INT) DATA_RETENTION_TIME_IN_DAYS=0 | CREATE VOLATILE TABLE t (id INT) | CREATE TABLE t (id INT) TEMPORARY NO_RETENTION | B | DATA_RETENTION_TIME_IN_DAYS=0 disables Time Travel. Combined with TEMPORARY keyword: CREATE TEMPORARY TABLE t (id INT) DATA_RETENTION_TIME_IN_DAYS=0. VOLATILE is a Teradata concept, not Snowflake. NO TIME TRAVEL is not valid Snowflake syntax. | Medium | Core | Temporary Tables |
| 21 | What distinguishes a transient table from a permanent table? | Transient tables store data in memory; permanent tables use cloud storage | Transient tables have no Fail-Safe period and Time Travel is limited to 0–1 day | Transient tables are automatically dropped after 24 hours | Transient tables cannot have primary or foreign keys | B | Transient tables omit Fail-Safe (saving storage cost on historical versions) and limit Time Travel to 0 or 1 day. They persist indefinitely until explicitly dropped and are visible to all sessions — unlike temporary tables. | Easy | Core | Transient Tables |
| 22 | A company stores 50TB of daily log data that only needs 1 day of recovery capability and no long-term backup. Which table type minimises storage cost? | Permanent with DATA_RETENTION=1 | Transient with DATA_RETENTION=1 | Temporary — auto-drops after session | External table — data stays in S3 | B | Transient tables with 1-day retention have: 1-day Time Travel versions + NO 7-day Fail-Safe. Permanent tables with 1-day retention still have the 7-day Fail-Safe overhead. For bulk logs needing minimal recovery, transient tables can reduce storage cost by up to 8x (no Fail-Safe micro-partition retention). | Easy | Core | Transient Tables |
| 23 | Can a transient table be part of a database created as TRANSIENT? | No — transient databases only contain permanent tables | Yes — all tables created in a transient database are automatically transient | Transient databases do not exist in Snowflake | Only schemas inside a transient database are transient, not the tables | B | CREATE TRANSIENT DATABASE creates a database where all objects (schemas, tables) default to transient type. This is useful for staging or development environments where Fail-Safe overhead is not needed for any object. | Medium | Core | Transient Tables |
| 24 | What is the maximum Time Travel retention period for a transient table? | 0 days (no Time Travel allowed) | 1 day | 7 days | 90 days | B | Transient tables support Time Travel of 0 or 1 day only — regardless of Snowflake edition. Even in Enterprise Edition with 90-day capability, transient tables are capped at 1 day. This is by design as they are intended for cost-optimised non-critical data. | Easy | Core | Transient Tables |
| 25 | A transient table named STAGING_DATA is accidentally dropped. It had DATA_RETENTION_TIME_IN_DAYS=1 and was dropped 30 minutes ago. How is it recovered? | Cannot be recovered — transient tables have no Fail-Safe | Use UNDROP TABLE STAGING_DATA — within the 1-day Time Travel window | Contact Snowflake Support — they can recover via Fail-Safe | Restore from the last Zero-Copy Clone | B | Transient tables DO have Time Travel (up to 1 day). Within that window, UNDROP TABLE works just like for permanent tables. The key difference is that once Time Travel expires, there is NO Fail-Safe fallback — the data is permanently unrecoverable without external backups. | Medium | Core | Transient Tables |
| 26 | Which CREATE TABLE syntax creates a transient table? | CREATE TABLE t (id INT) TYPE=TRANSIENT | CREATE TRANSIENT TABLE t (id INT) | CREATE TABLE t (id INT) NO_FAILSAFE | CREATE TEMP TABLE t (id INT) | B | The correct syntax is CREATE TRANSIENT TABLE. CREATE TEMP TABLE creates a temporary (session-scoped) table, not a transient table. TYPE=TRANSIENT and NO_FAILSAFE are not valid Snowflake DDL syntax. | Easy | Core | Transient Tables |
| 27 | A developer clones a permanent table using CREATE TRANSIENT TABLE clone_t CLONE perm_t. What are the properties of clone_t? | clone_t inherits permanent table properties including Fail-Safe | clone_t is transient — no Fail-Safe, Time Travel limited to 0–1 day | clone_t is permanent but shares storage with perm_t | The command fails — you cannot clone a permanent table as transient | B | You can explicitly create a transient clone of a permanent table. The clone_t table has transient properties (no Fail-Safe, max 1-day Time Travel) even though perm_t is permanent. This is a valid pattern for creating cost-optimised development copies. | Hard | Core | Transient Tables |
| 28 | What storage overhead does Fail-Safe add for a permanent table compared to a transient table? | No overhead — Fail-Safe uses compressed metadata only | Up to 7 days of historical micro-partition versions are retained for Fail-Safe — potentially significant for high-churn tables | Exactly 2x storage — one live copy and one Fail-Safe copy | Fail-Safe overhead is a flat fee, not proportional to data size | B | Fail-Safe retains modified/deleted micro-partitions for 7 days. For tables with frequent updates or deletes (high churn), this can mean storing many extra micro-partition versions. A high-churn permanent table could use 2–8x more storage than an equivalent transient table. This is the key cost motivation for using transient tables. | Medium | Core | Transient Tables |
| 29 | A schema contains a mix of permanent and transient tables. What happens when CREATE TRANSIENT SCHEMA is used instead? | All existing tables in the schema become transient | Only new tables created in the schema default to transient; existing tables are unchanged | All tables in the schema are dropped and recreated as transient | An error is thrown — schema type cannot be changed after creation | D | You cannot change a schema from permanent to transient after creation (or vice versa). CREATE TRANSIENT SCHEMA creates a new transient schema. To convert, you would need to create a new transient schema and migrate objects into it. | Hard | Core | Transient Tables |
| 30 | Which statement about transient tables and RBAC is correct? | Transient tables require special transient-specific privileges | Transient tables use the same RBAC model as permanent tables — SELECT, INSERT, UPDATE, DELETE, OWNERSHIP etc. | Only ACCOUNTADMIN can grant privileges on transient tables | Transient tables are always owned by the creating user and cannot be shared | B | Transient tables follow the exact same RBAC model as permanent tables. The transient property only affects storage behaviour (no Fail-Safe, limited Time Travel). All standard DDL and DML privilege grants apply identically. | Easy | Core | Transient Tables |
| 31 | What is a Snowflake Iceberg table? | A read-only external table that uses the Apache Iceberg open table format with metadata stored in Snowflake | A Snowflake-managed table that stores data in Apache Iceberg format in external cloud storage, combining open format with Snowflake governance | A special table type for storing semi-structured ice/geological data | A Snowflake table optimised for time-series data with built-in compression | B | Iceberg tables store data in Apache Iceberg open format in customer-owned external cloud storage (S3/Azure/GCS) while Snowflake manages the catalog/metadata and provides full SQL, governance, and performance capabilities. Data is interoperable with other Iceberg-compatible engines. | Easy | Core | Iceberg Tables |
| 32 | What is an EXTERNAL CATALOG Iceberg table in Snowflake? | An Iceberg table where Snowflake manages the catalog and writes data | An Iceberg table whose catalog is managed externally (AWS Glue, Polaris, Hive Metastore) — Snowflake has read-only access | A table stored in Snowflake's internal storage using Iceberg format | A synonym for a standard external table | B | Snowflake offers two Iceberg integration modes: (1) Snowflake as catalog — Snowflake manages metadata, full read/write; (2) External catalog (Glue, Hive, Polaris) — Snowflake queries the table read-only, respecting the external catalog's metadata. The external catalog mode enables multi-engine interoperability. | Medium | Core | Iceberg Tables |
| 33 | Which storage location does a Snowflake-managed Iceberg table use for its data files? | Snowflake's internal managed storage (same as permanent tables) | Customer-owned external cloud storage (S3, Azure Blob, GCS) specified via an external volume | A dedicated Iceberg storage tier managed by Snowflake | Either internal or external — configurable per table | B | Iceberg tables always store data in customer-owned external cloud storage defined by an EXTERNAL VOLUME. This is fundamentally different from permanent tables which use Snowflake-managed storage. The customer retains data ownership and can access the Parquet files directly. | Medium | Core | Iceberg Tables |
| 34 | What is an EXTERNAL VOLUME in the context of Iceberg tables? | A named reference to an external stage for loading files | A Snowflake object that defines the cloud storage location (S3 bucket, Azure container, GCS bucket) and access credentials for Iceberg table storage | A separate Snowflake account used for data overflow | A virtual warehouse optimised for reading external data | B | An EXTERNAL VOLUME is a Snowflake object (CREATE EXTERNAL VOLUME) that defines the cloud storage location and IAM/service principal credentials. It is the required prerequisite for creating Iceberg tables — it tells Snowflake where to write and read Iceberg data files. | Medium | Core | Iceberg Tables |
| 35 | What file format do Snowflake Iceberg tables use for data storage? | CSV with metadata headers | Apache Parquet | JSON lines (NDJSON) | ORC (Optimized Row Columnar) | B | Iceberg tables store data as Apache Parquet files in external storage. Iceberg's metadata layer (manifest files, snapshot files) sits on top of these Parquet files and provides features like schema evolution, time travel via snapshots, and partition evolution. | Easy | Core | Iceberg Tables |
| 36 | Can you perform DML (INSERT, UPDATE, DELETE) on a Snowflake-managed Iceberg table? | No — Iceberg tables are read-only in Snowflake | Yes — when Snowflake manages the catalog, full DML is supported | Only INSERT is supported — no UPDATE or DELETE | Yes, but only in Business Critical Edition | B | When Snowflake manages the Iceberg catalog (Snowflake as catalog mode), full DML operations (INSERT, UPDATE, DELETE, MERGE) are supported. In external catalog mode (e.g., Glue-managed), Snowflake is read-only. Snowflake-managed Iceberg tables are first-class writable objects. | Easy | Core | Iceberg Tables |
| 37 | What is the key governance advantage of Iceberg tables in Snowflake? | They are cheaper than permanent tables | Snowflake governance features (masking policies, row access policies, object tagging, RBAC) apply to Iceberg tables just like permanent tables | Iceberg tables bypass RBAC for maximum performance | They encrypt data with customer-managed keys automatically | B | One of Iceberg tables' key selling points is that Snowflake's full governance suite (Dynamic Data Masking, Row Access Policies, Object Tagging, RBAC, Access History) applies to Iceberg tables just like native tables — despite data living in external storage. | Medium | Core | Iceberg Tables |
| 38 | An Iceberg table is created with Snowflake as catalog. A data engineer uses Apache Spark to directly modify the Parquet files in S3 outside of Snowflake. What is the likely outcome? | Snowflake automatically detects and syncs the changes | Snowflake's metadata becomes inconsistent with the actual data files — queries may fail or return incorrect results | Snowflake locks the S3 bucket preventing external writes | The table automatically converts to external catalog mode | B | When Snowflake manages the Iceberg catalog, it owns the metadata. External modifications to Parquet files bypass Snowflake's metadata layer, causing inconsistency. The correct multi-engine pattern is to use an external catalog (Glue/Polaris) where all engines register changes through the same catalog. | Hard | Core | Iceberg Tables |
| 39 | What is the purpose of REFRESH on an Iceberg table with external catalog? | Clears the query result cache for the table | Synchronises Snowflake's view of the table metadata with the latest state of the external catalog | Reclusters the Iceberg table's micro-partitions | Updates statistics for query optimisation | B | For Iceberg tables using an external catalog, ALTER ICEBERG TABLE ... REFRESH synchronises Snowflake's metadata with the latest snapshot information from the external catalog (Glue, Hive, etc.). Without refresh, Snowflake may query stale data if external engines have added new snapshots. | Medium | Core | Iceberg Tables |
| 40 | Which Snowflake feature allows you to query an Iceberg table as of a specific point in time? | UNDROP TABLE ... AT (TIMESTAMP =>) | SELECT ... AT (TIMESTAMP => ...) or AT (SNAPSHOT => ...) using Iceberg snapshot IDs | Iceberg tables do not support historical queries in Snowflake | Only available when using Polaris as the catalog | B | For Snowflake-managed Iceberg tables, Time Travel via AT/BEFORE syntax works using Iceberg's snapshot mechanism. You can query AT a specific timestamp or AT a specific Iceberg snapshot ID. The underlying mechanism is Iceberg snapshots rather than Snowflake micro-partition history. | Hard | Core | Iceberg Tables |
| 41 | What is a Snowflake external table? | A table stored in a separate Snowflake account | A read-only table that references data files in external cloud storage (S3, Azure Blob, GCS) without loading data into Snowflake | A table in Snowflake that replicates data to an external database | A table accessible via PrivateLink from outside Snowflake | B | External tables are a Snowflake metadata layer over data files residing in external cloud storage. The data is never copied into Snowflake — queries read directly from the source files. They provide SQL access to external data without ETL. | Easy | Core | External Tables |
| 42 | What column does every Snowflake external table contain that native tables do not? | EXTERNAL_ID | VALUE (VARIANT type containing the raw file row data) | FILE_NAME | PARTITION_KEY | B | Every external table includes a system-generated VALUE column of VARIANT type that contains the full row from the file as a parsed object. Additional columns are defined using expressions that extract fields from this VALUE column. | Medium | Core | External Tables |
| 43 | Can DML operations (INSERT, UPDATE, DELETE) be performed on external tables? | Yes — all DML is supported | Only INSERT is supported — data is appended to external storage | No — external tables are read-only; data is managed externally | Yes, but only via Snowpipe integration | C | External tables are strictly read-only in Snowflake. Data management (writes) must happen directly in the external cloud storage (S3, Azure Blob, GCS). Snowflake queries the existing files but cannot write back to them. | Easy | Core | External Tables |
| 44 | What is AUTO_REFRESH on an external table? | Automatically reloads data from external storage into Snowflake on a schedule | Automatically updates the external table metadata when new files arrive in the external stage, triggered by cloud storage event notifications | Automatically drops and recreates the external table daily | Refreshes the result cache for external table queries | B | AUTO_REFRESH=TRUE uses cloud storage event notifications (SQS for S3, Event Grid for Azure, Pub/Sub for GCS) to automatically update the external table's file metadata when new files are added. Without it, ALTER EXTERNAL TABLE ... REFRESH must be called manually. | Medium | Core | External Tables |
| 45 | How does Snowflake achieve partition pruning for external tables? | Partition pruning is not possible — external tables always scan all files | By defining partition columns derived from file path patterns (e.g., year/month/day directory structure) using PARTITION BY clause | Snowflake automatically detects partitions from file metadata | Only possible with Iceberg format — not with CSV or Parquet external tables | B | External tables support partition pruning by defining PARTITION BY expressions that extract partition values from file paths. For example, TO_DATE(SPLIT_PART(metadata$filename,'/',3)) as a partition column allows Snowflake to skip entire directories of files based on query predicates. | Hard | Core | External Tables |
| 46 | An external table references CSV files in S3. A new CSV file is added to the S3 bucket. AUTO_REFRESH=TRUE is set. What must be configured in AWS for auto-refresh to work? | An AWS Lambda that calls the Snowflake REST API | An S3 event notification to an SQS queue that Snowflake monitors | An IAM role that allows Snowflake to poll S3 for changes | A Snowflake Task that runs ALTER EXTERNAL TABLE REFRESH every minute | B | AUTO_REFRESH for external tables on S3 requires an S3 event notification configured to send ObjectCreated events to an SQS queue. Snowflake subscribes to this SQS queue and processes notifications to update the external table's file metadata automatically. | Hard | Core | External Tables |
| 47 | What is the performance implication of querying an external table compared to a native Snowflake table? | External tables are faster — data is already in cloud storage without Snowflake overhead | External tables are generally slower — data is read directly from external files without Snowflake's micro-partition optimisations like clustering and columnar caching | Performance is identical — Snowflake caches external table data | External tables use the Result Cache more aggressively to compensate | B | External tables lack Snowflake's storage optimisations: no micro-partition metadata statistics, no automatic clustering, no data cache warmth. Each query reads raw files from object storage. For performance-critical workloads, loading data into native tables via COPY INTO is preferred. | Medium | Core | External Tables |
| 48 | Which file formats are supported for external tables in Snowflake? | CSV only | CSV, JSON, Avro, ORC, Parquet, and XML | Only columnar formats: Parquet and ORC | Any file format supported by the cloud provider | B | Snowflake external tables support CSV, JSON, Avro, ORC, Parquet, and XML file formats. The file format is specified as part of the external table definition or inherited from the stage's file format. | Easy | Core | External Tables |
| 49 | What is the relationship between an external table and a stage? | External tables do not require a stage — they reference cloud storage directly via a URL | An external table must be created on top of a named external stage or directly referencing a stage URL | External tables require an internal stage | An external table IS a stage with a SQL interface | B | An external table is always created over a stage (external stage pointing to S3/Azure/GCS) or a direct storage location URL. The stage defines the cloud storage location and credentials. The external table adds SQL query capability on top of the staged files. | Medium | Core | External Tables |
| 50 | What is a Snowflake Dynamic Table? | A table that automatically changes its schema based on incoming data | A declarative table whose content is defined by a SQL query and automatically refreshed incrementally as source data changes | A table with dynamic row-level security that changes per user | A table that resizes storage automatically based on data volume | B | Dynamic Tables are a declarative approach to data transformation pipelines. You define the transformation SQL, set a target lag (how fresh the data should be), and Snowflake automatically determines when and how to refresh — including incremental refresh where possible. | Easy | Core | Dynamic Tables |
| 51 | What does TARGET_LAG define in a Dynamic Table? | The maximum number of rows per refresh batch | The maximum acceptable staleness of the dynamic table's data relative to its source tables | The time limit before Snowflake drops the dynamic table | The interval between full refreshes regardless of source changes | B | TARGET_LAG specifies how stale the dynamic table's data can be. For example, TARGET_LAG='1 minute' means Snowflake ensures the table is never more than 1 minute behind its sources. Snowflake schedules refreshes to meet this SLA. Alternatively, TARGET_LAG='DOWNSTREAM' defers refresh scheduling to downstream dependencies. | Easy | Core | Dynamic Tables |
| 52 | What is the difference between INCREMENTAL and FULL refresh in a Dynamic Table? | Incremental refresh reloads all data; full refresh only adds new rows | Incremental refresh processes only changed source data (like a stream) for efficiency; full refresh re-executes the entire query — used when incremental is not feasible | Incremental refresh is manual; full refresh is automatic | Full refresh runs on a schedule; incremental runs in real time | B | When possible, Snowflake uses INCREMENTAL refresh — processing only new/changed rows from source using stream-like change tracking, which is far more efficient. When the query is too complex for incremental analysis (e.g., certain joins/aggregations), Snowflake falls back to FULL refresh, re-executing the entire SQL. | Medium | Core | Dynamic Tables |
| 53 | What does TARGET_LAG='DOWNSTREAM' mean on a Dynamic Table? | The table refreshes as fast as possible regardless of downstream dependencies | The refresh schedule is determined by downstream dynamic tables that depend on this one — this table refreshes when its consumers need it | The table never refreshes — it is a static snapshot | DOWNSTREAM means the table syncs with the source warehouse auto-suspend schedule | B | TARGET_LAG='DOWNSTREAM' is used in Dynamic Table pipelines (DAGs of dynamic tables). It means this table does not have its own refresh schedule — it refreshes only when triggered by a downstream dynamic table that has a numeric TARGET_LAG and needs fresh data from this one. | Medium | Core | Dynamic Tables |
| 54 | What compute resource is used by default for Dynamic Table refreshes? | A virtual warehouse specified in the WAREHOUSE parameter | Snowflake serverless compute | The most recently active warehouse in the account | No compute — Dynamic Tables use the Result Cache | A | Dynamic Tables require a WAREHOUSE parameter at creation — refreshes run on the specified virtual warehouse. Unlike Tasks (which can use serverless compute), Dynamic Tables always use a user-provisioned virtual warehouse for their refresh compute. Serverless Dynamic Tables are a newer option in preview. | Medium | Core | Dynamic Tables |
| 55 | How do Dynamic Tables differ from Materialized Views? | There is no difference — they are synonyms | Dynamic Tables support multi-table joins, complex transformations, and pipeline DAGs; Materialized Views are limited to single-table queries with certain restrictions | Materialized Views are more powerful — they support all SQL; Dynamic Tables are limited | Dynamic Tables are permanent; Materialized Views are temporary | B | Key differences: Dynamic Tables support multi-table joins, subqueries, complex aggregations, and can be chained into pipeline DAGs. MVs have SQL restrictions (no non-deterministic functions, limited joins). Dynamic Tables are designed to replace Streams+Tasks pipelines with a simpler declarative model. | Medium | Core | Dynamic Tables |
| 56 | What is a Dynamic Table DAG? | A diagram showing dynamic table storage growth over time | A chain of Dynamic Tables where each table's SQL references upstream Dynamic Tables, creating an automated multi-step transformation pipeline | A scheduling system for Dynamic Table refreshes using Directed Acyclic Graph logic | A Dynamic Table with multiple output streams | B | Multiple Dynamic Tables can reference each other — table B's SQL selects from table A (also a dynamic table). Snowflake treats these as a DAG, automatically ordering refreshes and propagating changes from source through the chain. This replaces complex Streams+Tasks pipeline logic. | Medium | Core | Dynamic Tables |
| 57 | What happens if a Dynamic Table refresh fails? | The table is automatically dropped | The table retains its last successfully refreshed state; the next scheduled refresh attempts again | All downstream Dynamic Tables are immediately invalidated | The table switches to FULL refresh mode permanently | B | A failed Dynamic Table refresh does not affect the currently stored data — the table retains its last successful state and remains queryable. Snowflake retries the refresh on the next scheduled cycle. Failures are visible in DYNAMIC_TABLE_REFRESH_HISTORY. | Medium | Core | Dynamic Tables |
| 58 | Which system view shows the history of Dynamic Table refresh operations? | INFORMATION_SCHEMA.TABLE_REFRESH_HISTORY | SNOWFLAKE.ACCOUNT_USAGE.DYNAMIC_TABLE_REFRESH_HISTORY | SHOW DYNAMIC TABLES HISTORY | INFORMATION_SCHEMA.DYNAMIC_TABLE_EVENTS | B | DYNAMIC_TABLE_REFRESH_HISTORY in the SNOWFLAKE.ACCOUNT_USAGE schema provides detailed refresh history including refresh type (incremental/full), status, duration, and bytes processed. This is the primary monitoring view for Dynamic Table pipeline operations. | Hard | Core | Dynamic Tables |
| 59 | Can a Dynamic Table be the source for a Snowflake Stream? | No — Streams can only be created on permanent tables | Yes — Streams can be created on Dynamic Tables to capture changes for further downstream processing | Only Append-only Streams are supported on Dynamic Tables | Only in Enterprise Edition | B | You can create a Stream on a Dynamic Table to capture its changes. This allows hybrid architectures where a Dynamic Table handles transformations and a Stream captures the resulting changes for further event-driven processing (e.g., feeding into a Task or another Dynamic Table). | Hard | Core | Dynamic Tables |
| 60 | What is a standard view in Snowflake? | A saved snapshot of query results updated on a schedule | A named SQL query stored in Snowflake that is executed each time the view is queried | A read-only copy of a table with restricted columns | A temporary table created from a SELECT statement | B | A standard view stores the SQL query definition — not data. Each time the view is queried, Snowflake executes the underlying SQL against the current data. Views add an abstraction layer and can simplify complex queries, enforce column/row restrictions, and decouple physical storage from logical presentation. | Easy | Core | Standard Views |
| 61 | Does querying a standard view consume warehouse credits? | No — views are metadata only, no compute needed | Yes — the underlying query executes on the virtual warehouse when the view is queried | Only if the view has more than 10 joins | Yes, but at a reduced rate compared to querying the base table directly | B | Querying a view executes the view's SQL definition on the virtual warehouse — just as if you ran the SQL directly. The warehouse credit consumption is determined by the complexity of the underlying query, not the view itself. | Easy | Core | Standard Views |
| 62 | What privilege is required for a user to query a view? | SELECT on the underlying base tables | SELECT on the view only — base table privileges are not required | USAGE on the view's schema and SELECT on the view | OWNERSHIP on the view | C | To query a view, a user needs USAGE on the database, USAGE on the schema, and SELECT on the view. The user does NOT need direct SELECT on the underlying base tables — this is the view's privilege isolation benefit. The view owner's access rights are used to query base tables. | Medium | Core | Standard Views |
| 63 | A view is defined as SELECT id, name FROM customers WHERE region = 'EMEA'. A user with SELECT on this view queries it. Can they see customers from the US region? | Yes — views are advisory only; users can bypass with SELECT * FROM customers | No — the view's WHERE clause restricts rows; the user sees only EMEA customers | Only if they have OWNERSHIP on the base table | Yes if they have SELECT on the base table in addition to the view | B | Views enforce their WHERE clauses as data access restrictions. A user with only SELECT on this view will always see only EMEA rows — they cannot bypass the view filter unless granted direct access to the underlying table. | Easy | Core | Standard Views |
| 64 | What is the difference between a view and a CREATE TABLE AS SELECT (CTAS)? | No difference — both store query results | A view stores the SQL definition (no data); CTAS creates a physical table containing data at creation time | CTAS is faster to query; views are faster to create | Views support DML; CTAS tables do not | B | A view stores only the SQL query — no data. Each query against the view re-executes the SQL on current data. CTAS creates a permanent physical table with data captured at the moment of creation. CTAS results become stale; view results are always current. | Easy | Core | Standard Views |
| 65 | Can a view reference objects in multiple databases? | No — views can only reference objects within the same database | Yes — Snowflake supports cross-database views using fully qualified names (db.schema.table) | Only if the databases are in the same cloud region | Yes, but only for tables, not other views | B | Snowflake views can reference objects using fully qualified three-part names (database.schema.object), enabling cross-database views. This is valid as long as the view owner has the required privileges on all referenced objects. | Medium | Core | Standard Views |
| 66 | What happens to a view if its underlying base table is dropped? | The view is automatically dropped as well | The view becomes invalid — it still exists but queries against it fail with an error | The view auto-recreates the base table from its last snapshot | The view switches to querying the Fail-Safe version of the table | B | Snowflake uses late binding for view resolution by default. If a base table is dropped, the view object persists in the schema but queries fail with a 'table not found' error. The view must be recreated or the base table restored for it to work again. Alternatively, CREATE VIEW ... WITH SCHEMA EVOLUTION handles some changes gracefully. | Medium | Core | Standard Views |
| 67 | What does CREATE OR REPLACE VIEW do if the view already exists? | Fails with an error — use ALTER VIEW instead | Atomically replaces the existing view definition with the new SQL | Creates a second view with the same name in a different namespace | Drops the existing view, leaving a gap where queries fail, then creates the new one | B | CREATE OR REPLACE VIEW is atomic — the replacement happens in a single metadata operation with no gap. Privileges granted on the original view are preserved after the replacement. This is the recommended way to update view definitions. | Easy | Core | Standard Views |
| 68 | What is the key difference between a standard view and a materialized view? | Materialized views are faster to create | Materialized views store the pre-computed query results physically — queries read the stored results instead of re-executing the SQL | Materialized views support DML; standard views do not | Standard views auto-refresh; materialized views are static snapshots | B | Materialized views physically store the result of the query. When queried, Snowflake reads the pre-computed result rather than re-executing the SQL. This makes repeated queries on complex aggregations or joins much faster, at the cost of storage and maintenance overhead. | Easy | Core | Materialized Views |
| 69 | What edition is required to create materialized views in Snowflake? | Standard Edition | Enterprise Edition | Business Critical | All editions | B | Materialized views require Enterprise Edition or higher. They are not available in Standard Edition. This is one of the key Enterprise Edition features related to query performance optimisation. | Easy | Core | Materialized Views |
| 70 | How does Snowflake keep a materialized view up to date when the base table changes? | Users must manually run ALTER MATERIALIZED VIEW ... REFRESH | Snowflake automatically maintains the materialized view using background serverless compute when the base table is modified | A scheduled Task must be configured to refresh the MV | MVs are updated synchronously as part of each DML transaction on the base table | B | Snowflake automatically maintains materialized views in the background using Snowflake-managed (serverless) compute. Users do not need to schedule or trigger refreshes. The maintenance cost is billed as serverless compute credits. | Medium | Core | Materialized Views |
| 71 | When a query is run against the base table of a materialized view, can Snowflake automatically use the MV to answer it? | No — the user must explicitly query the MV by name | Yes — Snowflake's query optimizer can transparently rewrite queries on the base table to use the MV if it can answer the query more efficiently | Only if the query is identical to the MV's defining SQL | Only in Enterprise Edition with MV_AUTO_REWRITE setting enabled | B | Snowflake's optimizer performs transparent MV query rewriting — if a query on a base table can be satisfied more efficiently by reading from a materialized view, the optimizer may choose to do so automatically without the user knowing. This is a key performance benefit of MVs. | Hard | Core | Materialized Views |
| 72 | Which SQL restriction applies to materialized views that does NOT apply to standard views? | MVs cannot reference more than one table | MVs cannot contain non-deterministic functions (CURRENT_TIMESTAMP, RANDOM), subqueries in the FROM clause, or GROUP BY on certain complex expressions | MVs cannot be queried by users without OWNERSHIP | MVs cannot use aggregate functions | B | Materialized views have significant SQL restrictions compared to standard views: no non-deterministic functions, limited subquery support, no HAVING with certain aggregations, no window functions in the defining query. These restrictions exist because Snowflake must be able to incrementally maintain the MV as data changes. | Hard | Core | Materialized Views |
| 73 | What storage cost is associated with a materialized view? | No storage cost — MVs use the result cache | Storage for the pre-computed result set — charged at standard Snowflake storage rates | A flat monthly fee per MV regardless of size | Storage is charged only when the MV is actively being queried | B | Materialized views store their results physically in Snowflake storage, incurring standard storage charges based on the size of the materialized result set. Additionally, the background maintenance compute is charged as serverless credits. | Medium | Core | Materialized Views |
| 74 | A materialized view is defined on a large fact table with aggregations. The base table receives INSERT, UPDATE, and DELETE operations continuously. What is the impact on the MV? | The MV becomes stale and is automatically dropped after 24 hours | Snowflake's background maintenance continuously updates the MV — queries always see fresh results but maintenance compute costs increase with DML frequency | The MV suspends automatic maintenance and becomes stale until manually refreshed | DML on the base table is blocked while the MV is being refreshed | B | Snowflake's background MV maintenance processes DML changes to keep the MV current. High DML frequency on the base table increases serverless maintenance costs. The MV is never stale — it is always consistent with committed data in the base table. | Hard | Core | Materialized Views |
| 75 | Can a materialized view be created on a secure view? | Yes — any view can be materialized | No — materialized views must be created directly on base tables, not on other views | Yes, but only in Business Critical Edition | No — secure views cannot be referenced by any other objects | B | Materialized views in Snowflake must be defined on base tables, not on other views (standard or secure). This restriction exists because Snowflake needs direct access to the base table's change data to maintain the MV incrementally. | Hard | Core | Materialized Views |
| 76 | What privilege is required to create a materialized view in a schema? | CREATE TABLE on the schema | CREATE MATERIALIZED VIEW on the schema | OWNERSHIP on the schema | CREATE VIEW on the schema | B | CREATE MATERIALIZED VIEW is a separate privilege from CREATE VIEW and must be explicitly granted on the schema. This allows schema owners to control whether roles can create the more resource-intensive materialized views. | Medium | Core | Materialized Views |
| 77 | A query that used to run in 30 seconds now runs in 0.5 seconds with no code changes. A new materialized view was recently created. What most likely happened? | The warehouse was upgraded to a larger size | Snowflake's query optimizer transparently rewrote the query to use the materialized view | The query hit the Result Cache after the MV warmed it | The table was clustered automatically | B | Snowflake's optimizer can transparently use a materialized view to answer queries against its base table — even if the user queries the base table directly. The 60x speedup is characteristic of a complex aggregation query now being answered from pre-computed MV results. | Medium | Core | Materialized Views |
| 78 | What does the SECURE keyword do when applied to a view? | Encrypts the view's data at rest using AES-256 | Hides the view's SQL definition from users who query it — preventing inference of underlying logic or data | Restricts the view to Business Critical accounts only | Adds row-level security based on current user | B | A secure view (CREATE SECURE VIEW) hides its SQL definition from users. Regular views expose their definition via DESCRIBE VIEW or SHOW VIEWS. Secure views prevent reverse-engineering of business logic and prevent users from inferring values they cannot see (e.g., through timing attacks on filtered data). | Easy | Core | Secure Views |
| 79 | What is the performance trade-off of using a secure view? | Secure views are always faster due to optimised execution | Secure views disable certain query optimisations to prevent data inference — they may be slower than non-secure views | Secure views consume double the warehouse credits | Secure views cannot use the Result Cache | B | To prevent data leakage through query optimisation (e.g., short-circuit evaluation revealing filter conditions), Snowflake disables certain optimisations for secure views. This can make secure views somewhat slower. If definition hiding is not needed, standard views with RBAC are preferred for performance. | Medium | Core | Secure Views |
| 80 | A developer has SELECT privilege on a secure view. They run DESCRIBE VIEW to see its definition. What do they see? | The full SQL definition — DESCRIBE is always allowed | An error — secure view definitions cannot be described by non-owners | A partial definition with sensitive columns redacted | The definition is shown only if they also have USAGE on the underlying schema | B | Non-owners of a secure view cannot see its SQL definition via DESCRIBE VIEW, SHOW VIEWS, or any metadata query. The output of DESCRIBE is blocked for the view definition. Only the view owner (or users with OWNERSHIP privilege) can see the definition. | Easy | Core | Secure Views |
| 81 | What is the difference between a secure view and row access policies for data restriction? | Secure views are more powerful — they can do everything row access policies can | Secure views restrict access by hiding view definition and applying SQL filters; row access policies dynamically filter rows at query time based on the current user's attributes — and cannot be bypassed by querying the base table | There is no difference — they achieve identical results | Row access policies require Business Critical Edition; secure views work in all editions | B | Key differences: A secure view is a fixed SQL filter. Row Access Policies (Enterprise Edition) are dynamic — the policy function evaluates per user/role, returning different row sets. Critically, Row Access Policies are attached to the TABLE, so they apply even if someone queries the base table directly. Secure views can be bypassed by anyone with direct table access. | Hard | Core | Secure Views |
| 82 | Which of the following is a valid reason to use a secure materialized view instead of a regular materialized view? | Secure MVs refresh faster | To hide the MV's definition (SQL) from users with SELECT on the MV while still providing pre-computed performance benefits | Secure MVs have no storage cost | Secure MVs bypass the Enterprise Edition requirement for MVs | B | A secure materialized view combines the performance benefits of pre-computed results with definition hiding. Users querying the secure MV get fast results without being able to see the underlying SQL logic. Both SECURE and MATERIALIZED can be combined: CREATE SECURE MATERIALIZED VIEW. | Medium | Core | Secure Views |
| 83 | Can a secure view be shared via Snowflake Data Sharing? | No — secure views cannot be shared | Yes — secure views are specifically designed for data sharing scenarios where the provider wants to share data without revealing their SQL logic | Only if the recipient is in the same cloud region | Only non-secure views can be shared | B | Secure views are a best practice for Snowflake Data Sharing. Data providers share secure views rather than base tables, allowing consumers to query data without seeing the underlying table structure, joins, or business logic. This is a core data sharing governance pattern. | Medium | Core | Secure Views |
| 84 | What happens to the view definition visibility when ALTER VIEW ... SET SECURE is applied to an existing standard view? | The existing view is dropped and recreated — all grants are lost | The view is modified in place to become secure — definition visibility is immediately restricted; existing grants are preserved | An error — views must be created as SECURE at creation time | The view becomes secure but requires a REFRESH before taking effect | B | ALTER VIEW view_name SET SECURE converts an existing standard view to a secure view in place. The view definition, privileges, and dependencies are preserved. The definition immediately becomes hidden from non-owners. This avoids dropping and recreating with potential downtime. | Medium | Core | Secure Views |
| 85 | A data provider wants to share revenue data with a partner but ensure the partner cannot determine the revenue calculation formula. They also need the partner to see only their own region's data. What is the optimal design? | Share the base table directly with a Row Access Policy | Create a secure view with WHERE region = current_user_region() and share that view | Create a standard view and grant the partner SELECT on it | Use Dynamic Data Masking on the revenue column | B | A secure view achieves both goals: (1) hides the SQL definition protecting the revenue formula, (2) includes a WHERE clause filtering by region for data isolation. Sharing the base table even with Row Access Policy would not hide the formula. A standard view exposes its definition. | Hard | Core | Secure Views |

---
*85 questions · Object Hierarchy: Tables & Views · SnowPro Core (COF-C03)*