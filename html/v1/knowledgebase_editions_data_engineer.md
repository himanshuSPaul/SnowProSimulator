# Snowflake Editions & Features — Advanced Data Engineer

> **Exam:** Advanced Data Engineer (ADE-C01)  
> **Topic:** Snowflake Editions & Features  
> **Total Questions:** 15

---

| No | Question | Option A | Option B | Option C | Option D | Answer | Explanation | Level | Exam | Topic |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | A data engineer needs to load 10TB of CSV files from S3 daily. The files arrive at irregular intervals. Which Snowflake feature automates loading without a warehouse running 24/7? | COPY INTO with a scheduled task | Snowpipe — event-driven serverless ingestion | External Tables with auto-refresh | Streams + Tasks pipeline | B | Snowpipe is serverless event-driven ingestion. It triggers automatically when new files land in the stage (via S3 event notification or REST API) without requiring a virtual warehouse to be running. | Easy | Data Engineer | Data Loading Features |
| 2 | Which Snowflake edition is required to use Snowpipe with AWS SQS automatic event notifications? | Business Critical only | Enterprise Edition minimum | All editions — Snowpipe is available in Standard and above | Virtual Private Snowflake only | C | Snowpipe (including SQS-based auto-ingestion) is available in all Snowflake editions including Standard. It is not restricted by edition. | Easy | Data Engineer | Data Loading Features |
| 3 | A data engineer uses Streams and Tasks to build a CDC pipeline. The stream becomes STALE. What caused this and what must be done? | The source table was too large — increase warehouse size and resume the stream | The stream's offset fell outside the table's Time Travel retention period — drop and recreate the stream | The task failed 3 times — restart the task and the stream auto-recovers | STALE streams are normal — they reset automatically at midnight | B | A stream becomes STALE when the Time Travel retention period of the source table expires before the stream is consumed. The stream's offset no longer exists. The stream must be dropped and recreated, but the historical CDC data captured before staleness is lost. | Medium | Data Engineer | Streams & Tasks |
| 4 | An engineer builds a Task DAG: Task A → Task B → Task C. Task B fails. What happens to Task C? | Task C runs anyway — task dependencies are advisory only | Task C is skipped — downstream tasks do not run if an upstream task fails | Task C runs after a retry of Task B | Task C runs but marks itself as SKIPPED in history | B | In a Task DAG, if a parent task fails, all downstream child tasks are skipped for that scheduled run. Task C will not execute until Task B succeeds in a future run (or is manually resumed). | Medium | Data Engineer | Streams & Tasks |
| 5 | A data engineer needs to track INSERT, UPDATE, and DELETE operations on a table for downstream processing. Which object captures this? | An Append-only Stream | A Standard Stream | A Task with MERGE logic | An External Stream | B | A Standard Stream captures all DML changes (INSERT, UPDATE, DELETE) with METADATA$ACTION and METADATA$ISUPDATE columns. An Append-only Stream only captures INSERTs — useful for append-only sources. | Easy | Data Engineer | Streams & Tasks |
| 6 | Which editions support serverless Tasks (tasks that use Snowflake-managed compute instead of a virtual warehouse)? | Enterprise Edition and above only | All editions — serverless tasks are universally available | Business Critical only | Standard Edition — serverless tasks were introduced for cost-optimised tiers | B | Serverless Tasks (using Snowflake-managed compute, billed per second) are available in all editions. They are particularly cost-effective for lightweight, frequent tasks where a full virtual warehouse would be wasteful. | Medium | Data Engineer | Streams & Tasks |
| 7 | A pipeline uses a Snowpipe + Stream + Task architecture. The engineer notices the stream is being consumed but the task is inserting duplicate rows. What is the most likely cause? | Stream consumption is not transactional — use COPY INTO instead | The task does not consume the stream in a single transaction — stream offset advances only on successful COMMIT | Two tasks are assigned to the same stream — both consuming independently | Snowpipe delivers duplicate files — enable PURGE=TRUE | B | Stream offsets only advance when a DML transaction that reads the stream commits successfully. If a task fails mid-execution (pre-COMMIT), the stream is not consumed and will be re-processed on next run. Ensure idempotent MERGE logic or transactional task design to prevent duplicates. | Hard | Data Engineer | Streams & Tasks |
| 8 | A data engineer wants to use Dynamic Tables instead of Streams + Tasks for a transformation pipeline. What edition is required? | Standard Edition | Enterprise Edition | Business Critical | All editions — Dynamic Tables are universally available | D | Dynamic Tables are available in all Snowflake editions. They simplify pipeline development by declaratively defining transformation SQL and letting Snowflake manage incremental refresh automatically. | Medium | Data Engineer | Dynamic Tables |
| 9 | What is the key difference between a Task using a virtual warehouse vs a serverless Task? | Serverless Tasks are faster — they use dedicated high-performance compute | Virtual warehouse Tasks use pre-provisioned compute (you pay per second of warehouse uptime); serverless Tasks use Snowflake-managed compute billed purely per compute-second used | Serverless Tasks cannot use Streams — only virtual warehouse Tasks can | Virtual warehouse Tasks support DAGs; serverless Tasks are standalone only | B | With a virtual warehouse, the warehouse must be running (incurring cost even during light load). Serverless Tasks use Snowflake-managed infrastructure billed only for actual compute consumed — more cost-efficient for short, frequent tasks. | Medium | Data Engineer | Streams & Tasks |
| 10 | Which feature, available in Enterprise Edition, allows a data engineer to automatically tag columns containing PII during the ingestion process? | Snowpipe with AUTO_TAG=TRUE setting | Auto-Classification with System-defined Tags (SNOWFLAKE.CORE.PRIVACY_CATEGORY) | Column-level encryption during COPY INTO | Row Access Policies applied during staging | B | Snowflake's Auto-Classification (Enterprise Edition) automatically detects and tags columns containing sensitive data (PII, financial data) using system-defined privacy categories during or after ingestion. This feeds directly into Tag-based Masking Policies. | Hard | Data Engineer | Data Governance Features |
| 11 | A data engineer loads files using COPY INTO with ON_ERROR=CONTINUE. 3 out of 100 files fail due to schema mismatch. What is true about re-running the same COPY INTO command? | All 100 files reload — COPY INTO always reloads all files | Only the 3 failed files load — successfully loaded files are skipped due to load metadata | All 100 files fail — the load metadata marks the batch as failed | The 3 failed files must be manually specified with a FILES=(...) clause | B | COPY INTO tracks a 64-day load history per stage. Files successfully loaded are skipped on re-run. Only files not previously loaded (including the 3 that failed with CONTINUE) will be attempted — avoiding duplicate ingestion. | Medium | Data Engineer | Data Loading Features |
| 12 | An engineer needs to load semi-structured JSON from S3, preserving the raw JSON in a VARIANT column while also extracting 3 specific fields into separate columns. Which approach is optimal? | Load into VARIANT first, then use INSERT INTO SELECT with lateral flatten | Use COPY INTO with a transformation clause to extract fields during load | Create an external table and use a view for extraction | Use Snowpipe with a schema transformation pipeline | B | COPY INTO supports inline SQL transformations during load — you can simultaneously load the full JSON into a VARIANT column AND extract specific paths into typed columns in a single COPY INTO statement using a SELECT clause. | Hard | Data Engineer | Data Loading Features |
| 13 | A data engineering team runs heavy ELT jobs on a Snowflake warehouse. They notice that warehouse credit consumption is high even when no jobs are running. What is the most likely cause? | Snowflake charges a flat monthly fee regardless of usage | The warehouse AUTO_SUSPEND setting is too high — it's running idle for long periods | The team is being charged for Snowpipe compute on the same warehouse | Storage costs are being misattributed to compute | B | If AUTO_SUSPEND is set too high (e.g., 3600 seconds), the warehouse stays running and consuming credits even with no active queries. Setting AUTO_SUSPEND to 60 seconds for ELT workloads minimises idle cost. | Easy | Data Engineer | Warehouse Management |
| 14 | A data engineer wants to test a transformation without affecting the production warehouse's cache. What is the recommended approach? | Run the test query with RESULT_SCAN to avoid warehouse usage | Create a separate warehouse for development — caches are isolated per warehouse | Use a clone of the warehouse | Disable caching on the production warehouse temporarily | B | Each virtual warehouse has its own local disk cache (data cache). Using a separate dev warehouse ensures test queries don't pollute or compete with production cache, and production queries aren't disrupted by test workloads. | Medium | Data Engineer | Warehouse Management |
| 15 | Which Snowflake feature allows a data engineer to limit the maximum compute cost of a single runaway query? | STATEMENT_TIMEOUT_IN_SECONDS — kills queries exceeding a time limit | RESOURCE_CONSTRAINT on the warehouse | MAX_CONCURRENCY_LEVEL limits total credits | CREDIT_QUOTA on a Resource Monitor | A | STATEMENT_TIMEOUT_IN_SECONDS terminates queries that run beyond the time limit, preventing runaway queries from consuming unbounded credits. Resource Monitors control warehouse-level credit budgets but don't stop individual long-running queries directly. | Medium | Data Engineer | Warehouse Management |

---
*15 questions · Snowflake Editions & Features · Advanced Data Engineer (ADE-C01)*